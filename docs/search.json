[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weather Shocks",
    "section": "",
    "text": "Index\nThis ebook provides some notebooks to help adapting the methodology of our paper Weather Shock (Gallic and Vermandel (2020)) to new contexts or datasets.\nThe notebooks are in construction, this page will be filled soon.\n\n\n\n\nGallic, Ewen, and Gauthier Vermandel. 2020. “Weather Shocks.” European Economic Review 124 (May): 103409. https://doi.org/10.1016/j.euroecorev.2020.103409.",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "1  Useful Functions",
    "section": "",
    "text": "1.1 Functions fro Graphs\nThroughout the analysis, we will need a few user-defined functions. While the functions specific to a part of the analysis are presented in the corresponding chapter, some functions are needed in multiple parts of this project. We define those here. These functions are contained in the ../scripts/functions/utils.R file.\nWe define theme functions for the plots.\nThe main theme for the plots:\n#' Theme for ggplot2\n#'\n#' @param ... arguments passed to the theme function\n#' @export\n#' @importFrom ggplot2 element_rect element_text element_blank element_line unit\n#'   rel\ntheme_paper &lt;- function (...) {\n  theme(\n    text = element_text(family = \"Times New Roman\"),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    panel.border = element_rect(fill = NA, colour = \"grey50\", linewidth = 1),\n    axis.text = element_text(),\n    legend.text = element_text(size = rel(1.1)),\n    legend.title = element_text(size = rel(1.1)),\n    legend.background = element_rect(fill = \"transparent\", color = NULL),\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    legend.box = \"vertical\",\n    legend.key = element_blank(),\n    panel.spacing = unit(1, \"lines\"),\n    panel.grid.major = element_line(colour = \"grey90\"),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0, size = rel(1.3), face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.margin = unit(c(1, 1, 1, 1), \"lines\"),\n    strip.background = element_rect(fill = NA, colour = NA),\n    strip.text = element_text(size = rel(1.1))\n  )\n}\nA function for maps:\n#' Theme for maps with ggplot2\n#'\n#' @param ... arguments passed to the theme function\n#' @export\n#' @importFrom ggplot2 element_rect element_text element_blank element_line unit\n#'   rel\ntheme_map_paper &lt;- function(...) {\n  theme(\n    text = element_text(family = \"Times New Roman\"),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    panel.border = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(), axis.line = element_blank(),\n    plot.title.position = \"plot\",\n    legend.text = element_text(size = rel(1.2)),\n    legend.title = element_text(size = rel(1.2)),\n    legend.background = element_rect(fill=\"transparent\", color = NULL),\n    legend.key = element_blank(),\n    legend.key.height   = unit(2, \"line\"),\n    legend.key.width    = unit(1.5, \"line\"),\n    strip.background = element_rect(fill = NA),\n    panel.spacing = unit(1, \"lines\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.margin = unit(c(1, 1, 1, 1), \"lines\"),\n    strip.text = element_text(size = rel(1.2))\n  )\n}\nTo transform the graphs into tikz pictures, we use the following function:\n#' Save a ggplot2 plot as PDF, using LaTeX tikz\n#'\n#' @param plot ggplot2 object\n#' @param path_to_latex path to LaTeX engine (Defaults to\n#'   `/Library/TeX/texbin/`)\n#' @param interpreter by default, use pdflatex (`pdflatex`)\n#' @param path path to the destination folder\n#' @param filename file name (without the extension)\n#' @param keep_tex should the tex file be kept after compilation? Defaults to\n#'   `FALSE`\n#' @param width width in inches (default to 15)\n#' @param height height in inches (default to 15)\n#' @param verbose A logical value indicating whether diagnostic messages are\n#'   printed when measuring dimensions of strings. Defaults to `FALSE`\n#' @param ignore.stdout a logical (not NA) indicating whether messages written\n#'   to ‘stdout’  should be ignored. Defaults to `TRUE`\n#'\n#' @export\n#' @importFrom tikzDevice tikz\n#' @importFrom grDevices dev.off\nggplot2_to_pdf &lt;- function(plot,\n                           path_to_latex = \"/Library/TeX/texbin/\",\n                           interpreter = \"pdflatex\",\n                           path = \"./\",\n                           filename,\n                           keep_tex = FALSE,\n                           width = 15,\n                           height = 15,\n                           verbose = FALSE,\n                           ignore.stdout = TRUE){\n    content &lt;- paste0(\n      \"\\\\documentclass{standalone}\n      \\\\usepackage{amssymb}\n      %\\\\usepackage{newtxtext,newtxmath}\n      \\\\usepackage{times,mathpazo}\n      \\\\usepackage{pgfplots}\n      \\\\usetikzlibrary{pgfplots.groupplots}\n      \\\\definecolor{mygrey2}{RGB}{127,127,127}\n      \\\\definecolor{deepblue}{RGB}{0,129,188}\n      \\\\definecolor{deepgreen}{RGB}{0,157,87}\n      \\\\definecolor{deepred}{RGB}{238,50,78}\n      \\\\begin{document}\n\n      \\\\input{\",\n      path, filename,\n      \"_content.tex}\n\n      \\\\end{document}\"\n    )\n\n    # The file which will import the graph in tex format\n    fileConn &lt;- file(paste0(path, filename, \".tex\"))\n    writeLines(content, fileConn)\n    close(fileConn)\n\n    # Export graph to tex\n    tikz(file = paste0(\n      path,\n      filename, \"_content.tex\"),\n      width = width,\n      height = height,\n      verbose = verbose\n    )\n    print(plot)\n    dev.off()\n\n    # Move the scale from ggplot, if any\n    name_scale &lt;- paste0(filename, \"_content_ras1.png\")\n    scale_exists &lt;- file.exists(name_scale)\n    if (scale_exists & ! path %in% c(\".\", \"./\", \"/\")) {\n      system(paste0(\"mv \", name_scale, \" \", path))\n    }\n\n    # Process tex file to get the PDF\n    system(\n      paste0(\n        path_to_latex,\n        interpreter, \" -shell-escape -synctex=1 -interaction=nonstopmode  \",\n        path,\n        filename, \".tex\"),\n      ignore.stdout = TRUE\n    )\n    if(!path %in%  c(\".\", \"./\", \"/\")) system(paste0(\"mv \", filename, \".pdf \", path))\n    system(paste0(\"rm \", filename, \".aux\"))\n    system(paste0(\"rm \", filename, \".log\"))\n    system(paste0(\"rm \", filename, \".synctex.gz\"))\n    if (!keep_tex) {\n      system(paste0(\"rm \", path, filename, \".tex\"))\n      system(paste0(\"rm \", path, filename, \"_content.tex\"))\n    }\n    if (scale_exists) system(paste0(\"rm \", path, \"/\", name_scale))\n  }",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "data-maps.html",
    "href": "data-maps.html",
    "title": "2  Maps",
    "section": "",
    "text": "2.1 Helper Functions\nWe need the following packages:\nWe have shown in Chapter 1 some theme functions for plots. These functions are written in an R script that we can load to have access to those functions.\nWe need to get the boundaries of New Zealand. We define a tibble with the names of the country as well as its corresponding ISO-3 classification.\nWe add a column with the name (this will be used to name the folder in which the data will be saved).\nLet us save this for later use:\nWe need three functions for the maps:\nThe first function, download_map():\n#' Download GADM shapefile (version 4.1)\n#' \n#' @param country_name name of the country\n#' @param country_code ISO-3 country code\n#' @param country_name_clear name of the country withour space or \\'\ndownload_map &lt;- function(country_name, country_code, country_name_clear) {\n  file_name &lt;- str_c(\"gadm41_\", country_code, \"_shp.zip\")\n  url &lt;- str_c(\"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/\", file_name)\n  dest_file &lt;- str_c(\n    \"../data/Maps/GADM-4.1/\", str_replace_all(country_name_clear, \" \", \"-\"), \"/\",\n    file_name\n    )\n  download.file(url = url, destfile = dest_file)\n}\nAnd the second one, import_map() which imports the maps data from the shapefile, extracts the 3 layers corresponding to administrative units (if available) and then saves them in the country’s directory.\n#' Imports map data downloaded from GADM, extracts layers and saves the result\n#' \n#' @param country_name name of the country\n#' @param country_code ISO-3 country code\n#' @param country_name_clear name of the country withour space or \\'\n#' \n#' @returns the `NULL` object\nimport_map &lt;- function(country_name, country_code, country_name_clear) {\n  out_directory &lt;- tempfile()\n  dir &lt;- \"../data/Maps/GADM-4.1/\"\n  file &lt;- str_c(dir, country_name_clear, \"/gadm41_\", country_code, \"_shp.zip\")\n  layer_0 &lt;- str_c(\"gadm41_\", country_code, \"_0\")\n  layer_1 &lt;- str_c(\"gadm41_\", country_code, \"_1\")\n  layer_2 &lt;- str_c(\"gadm41_\", country_code, \"_2\")\n\n  unzip(file, exdir = out_directory)\n  map_level_0 &lt;- st_read(dsn = out_directory, layer = layer_0, quiet = TRUE)\n  map_level_1 &lt;- st_read(dsn = out_directory, layer = layer_1, quiet = TRUE)\n  map_level_2 &lt;- try(st_read(dsn = out_directory, layer = layer_2, quiet = TRUE))\n  if (inherits(map_level_2, \"try-error\")) map_level_2 &lt;- NULL\n  res &lt;- list(\n    map_level_0 = map_level_0,\n    map_level_1 = map_level_1,\n    map_level_2 = map_level_2\n  )\n  res_name &lt;- str_to_lower(str_c(country_code, \"_maps\"))\n  assign(res_name, res)\n  eval(parse(\n    text = paste0(\n      \"save('\", res_name,\n      \"', file = '\", dir, \"/\", country_name_clear, \"/\", res_name, \".RData')\")\n  ))\n  NULL\n}\n#' Load the layers of a country's map\n#' \n#' @param country_name name of the country\n#' @param tb_countries tibble with the names of countries (`country_code`),\n#' the corresponding ISO-3 classification (`country_code`)\nload_country_map &lt;- function(country_name, tb_countries) {\n  current_tb_country &lt;- tb_countries |&gt; filter(country_name == !!country_name)\n  country_code &lt;- current_tb_country$country_code\n  country_name_clear &lt;- current_tb_country$country_name_clear\n  current_folder &lt;- str_c(\"../data/Maps/GADM-4.1/\", country_name_clear)\n  output_name &lt;- str_c(\n    current_folder, \"/\",\n    str_to_lower(str_c(country_code, \"_maps\")),\n    \".RData\"\n  )\n  load(output_name)\n  object_name &lt;- str_c(str_to_lower(country_code), \"_maps\")\n  get(object_name)\n}",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#helper-functions",
    "href": "data-maps.html#helper-functions",
    "title": "2  Maps",
    "section": "",
    "text": "one to download the shapefiles from the GADM website,\nanother one to extract relevant information (administrative units of level 1 to 3),\nand a third one to load the data in R.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#download-data",
    "href": "data-maps.html#download-data",
    "title": "2  Maps",
    "section": "2.2 Download Data",
    "text": "2.2 Download Data\nLet us loop over the countries (just New Zealand here) to download the shapefiles.\n\ncli::cli_progress_bar(\"Downloading SHP files\", total = nrow(tb_countries))\nfor (i in 1:nrow(tb_countries)) {\n  current_country_name &lt;- tb_countries$country_name[i]\n  current_country_code &lt;- tb_countries$country_code[i]\n  current_country_name_clear &lt;- tb_countries$country_name_clear[i]\n  \n  current_folder &lt;- str_c(\"../data/Maps/GADM-4.1/\", current_country_name_clear)\n  # If data not already downloaded:\n  # Create folder and download data\n  if (!dir.exists(current_folder)) {\n    dir.create(current_folder, recursive = TRUE)\n    download_map(\n      country_name = current_country_name,\n      country_code = current_country_code,\n      country_name_clear = current_country_name_clear\n    )\n  }\n  cli::cli_progress_update()\n}\ncli::cli_progress_done()",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#import-shapefiles",
    "href": "data-maps.html#import-shapefiles",
    "title": "2  Maps",
    "section": "2.3 Import Shapefiles",
    "text": "2.3 Import Shapefiles\nNow that the shapefiles were downloaded, we can extract the information from them and save the extracted information into RData files that can be loaded when needed.\nLet us loop again on the countries. We can do it in a parallel way, to fasten the process.\n\nlibrary(future)\nnb_cores &lt;- future::availableCores()-1\nplan(multisession, workers = nb_cores)\nprogressr::with_progress({\n  p &lt;- progressr::progressor(steps = nrow(tb_countries))\n  tmp &lt;- furrr::future_map(\n    .x = 1:nrow(tb_countries),#looping on the row numbers\n    .f = ~{\n      current_country_name &lt;- tb_countries$country_name[.x]\n      current_country_code &lt;- tb_countries$country_code[.x]\n      current_country_name_clear &lt;- tb_countries$country_name_clear[.x]\n      # Check if the file already exists\n      current_folder &lt;- str_c(\"../data/Maps/GADM-4.1/\", current_country_name_clear)\n      output_name &lt;- str_c(\n        current_folder, \"/\",\n        str_to_lower(str_c(current_country_code, \"_maps\")),\n        \".RData\"\n      )\n      if (!file.exists(output_name)) {\n        # If it does not\n        # Import the data and save the extracted information\n      import_map(\n        country_name = tb_countries$country_name[.x],\n        country_code = tb_countries$country_code[.x],\n        country_name_clear = tb_countries$country_name_clear[.x]\n      )\n      }\n      p()\n      NULL\n    }\n  )\n})",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#load-extracted-maps",
    "href": "data-maps.html#load-extracted-maps",
    "title": "2  Maps",
    "section": "2.4 Load Extracted Maps",
    "text": "2.4 Load Extracted Maps\nAll the maps of interest for the study have been downloaded and the information we are looking for has been extracted. We can easily load the data now. Let us create a list with all the maps from all the countries of interest.\n\nall_maps &lt;- \n  map(\n    .x = tb_countries$country_name,\n    .f = ~load_country_map(country_name = .x, tb_countries = tb_countries)\n  )\nnames(all_maps) &lt;- tb_countries$country_code\n\nThe borders are too precise for what we need to do. Let us simplify the objects (this will allow us to have lighter graph files without loosing too much detail).\n\n2.4.1 Level 0 (borders)\nLevel 0 maps corresponds to the countries boundaries.\n\n# Simplification\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\nmaps_level_0 &lt;- map(all_maps, \"map_level_0\")\nmaps_level_0 &lt;- map(\n  maps_level_0,\n  ~st_simplify(.x, dTolerance = 0.01)\n)\n\nWe save this file for later use:\n\nsave(maps_level_0, file = \"../data/Maps/GADM-4.1/maps_level_0.RData\")\n\n\nmaps_level_0_all &lt;- do.call(\"rbind\", maps_level_0)\nmaps_level_0_plot &lt;- map(\n  .x = tb_countries$country_name,\n  .f = function(x) {\n    p &lt;- ggplot(\n      data = maps_level_0_all |&gt;\n        filter(COUNTRY == x)\n    ) +\n      geom_sf()\n    if (x %in% c(\"Fiji\", \"New Zealand\"))\n      p &lt;- p + coord_sf(crs = \"+init=epsg:3460\")\n    p + theme_map_paper()\n    }\n)\n\n\ncowplot::plot_grid(\n  plotlist = c(\n    map(\n      .x = maps_level_0_plot,\n      .f = ~.x + theme(legend.position = 'none')\n    )\n  ),\n  labels = tb_countries$country_name_short,\n  label_size = 10\n)\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\n\n\nFigure 2.1: Map of New Zealand\n\n\n\n\n\n\n\n\n\n\n2.4.2 Level 1\nLet us consider level 1.\n\n# Simplification\nsf_use_s2(FALSE)\nmaps_level_1 &lt;- map(all_maps, \"map_level_1\")\nmaps_level_1 &lt;- map(\n  maps_level_1,\n  ~st_simplify(.x, dTolerance = 0.01)\n)\n\nSaving the object:\n\nsave(maps_level_1, file = \"../data/Maps/GADM-4.1/maps_level_1.RData\")\n\n\nmaps_level_1_all &lt;- do.call(\"rbind\", maps_level_1)\nmaps_level_1_plot &lt;- map(\n  .x = tb_countries$country_name,\n  .f = function(x) {\n    p &lt;- ggplot(\n      data = maps_level_1_all |&gt;\n        filter(COUNTRY == x)\n    ) +\n      geom_sf(mapping = aes(group = ISO_1))\n    if (x %in% c(\"Fiji\", \"New Zealand\")) \n      p &lt;- p + coord_sf(crs = \"+init=epsg:3460\")\n    p + theme_map_paper()\n  }\n)\n\n\ncowplot::plot_grid(\n  plotlist = c(\n    map(\n      .x = maps_level_1_plot,\n      .f = ~.x + theme(legend.position = 'none')\n    )\n  ),\n  labels = tb_countries$country_name_short,\n  label_size = 10\n)\n\n\n\n\nFigure 2.2: Map of New Zealand (Regions level 1)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Level 2\nAnd lastly, we consider level 2.\n\nmaps_level_2 &lt;- map(all_maps, \"map_level_2\")\nkeep &lt;- map_lgl(maps_level_2, ~!is.null(.x))\nmaps_level_2 &lt;- map(\n  maps_level_2[keep],\n  ~st_simplify(.x, dTolerance = 0.01)\n)\n\nSaving file:\n\nsave(maps_level_2, file = \"../data/Maps/GADM-4.1/maps_level_2.RData\")\n\n\nmaps_level_2_all &lt;- do.call(\"rbind\", maps_level_2)\nmaps_level_2_plot &lt;- map(\n  .x = tb_countries$country_name,\n  .f = function(x) {\n    p &lt;- ggplot(\n      data = maps_level_2_all |&gt;\n        filter(COUNTRY == x)\n    ) +\n      geom_sf(mapping = aes(group = GID_2))\n    if (x %in% c(\"Fiji\", \"New Zealand\"))\n      p &lt;- p + coord_sf(crs = \"+init=epsg:3460\")\n    p + theme_map_paper()\n  }\n)\n\n\ncowplot::plot_grid(\n  plotlist = c(\n    map(\n      .x = maps_level_2_plot,\n      .f = ~.x + theme(legend.position = 'none')\n    )\n  ),\n  labels = tb_countries$country_name,\n  label_size = 10\n)\n\n\n\n\nFigure 2.3: Map of New Zealand (Regions level 2)",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#bounding-boxes",
    "href": "data-maps.html#bounding-boxes",
    "title": "2  Maps",
    "section": "2.5 Bounding boxes",
    "text": "2.5 Bounding boxes\nWhen we download gridded weather data, we will focus on the parts of the grid that correspond to New Zealand. Let us get the bounding box of the country to make this task easier. We show the R code to do so here, but we will evaluate it again in [Chapter @-sec-weather-data].\n\nbboxes &lt;- map(maps_level_0, st_bbox)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you re-use this code for the Fiji Islands or New Zealand, be careful. The bounding box needs to be recreated. Otherwise, it is too large, as the coordinates of this country is not convenient to work with using this map projection.\n\n\n\n\nCode\nmaps_level_0_plot_bbox &lt;- map(\n  .x = tb_countries$country_name,\n  .f = function(x) {\n    current_country_code &lt;- tb_countries |&gt; \n      filter(country_name == x) |&gt; pull(\"country_code\")\n    bbox &lt;- bboxes[[current_country_code]]\n    p &lt;- ggplot(\n      data = maps_level_0_all |&gt;\n        filter(COUNTRY == x)\n    ) +\n      geom_sf() +\n      annotate(\n        geom = \"rect\", \n        xmin = bbox$xmin, xmax = bbox$xmax, \n        ymin = bbox$ymin, ymax = bbox$ymax,\n        fill = \"#56B4E9\", alpha = .1, linetype = \"dashed\", colour = \"black\"\n      )\n    if (x == \"Fiji\") p &lt;- p + coord_sf(crs = \"+init=epsg:3460\")\n    p + theme_map_paper()\n  }\n)\ncowplot::plot_grid(\n  plotlist = c(\n    map(\n      .x = maps_level_0_plot_bbox,\n      .f = ~.x + theme(legend.position = 'none')\n    )\n  ),\n  labels = tb_countries$country_name_short,\n  label_size = 10\n)\n\n\n\n\n\nFigure 2.4: Map of New Zealand, showing the initial extracted bounding box (blue shaded area delimited with dashed lines).\n\n\n\n\n\n\n\n\nFor simplicity, we will exclude the Chatham island and the Auckland Islands.\nWe define the new values for the bounding box:\n\nbboxes$NZL[[\"xmin\"]] &lt;- 167\nbboxes$NZL[[\"xmax\"]] &lt;- 179\nbboxes$NZL[[\"ymin\"]] &lt;- -48\nbboxes$NZL[[\"ymax\"]] &lt;- -34\n\n\n\nCode\nbbox &lt;- bboxes[[\"NZL\"]]\nbbox_sf &lt;- st_as_sfc(st_bbox(bbox), crs = 4326) |&gt; \n  st_transform(3460)\nbbox_trans &lt;- st_bbox(bbox_sf)\n\nggplot(\n  data = maps_level_0_all |&gt;\n    filter(COUNTRY == \"New Zealand\")\n) +\n  geom_sf() +\n  annotate(\n    geom = \"rect\", \n    xmin = bbox_trans$xmin, xmax = bbox_trans$xmax, \n    ymin = bbox_trans$ymin, ymax = bbox_trans$ymax,\n    fill = \"#56B4E9\", alpha = .1, linetype = \"dashed\", colour = \"black\"\n  ) +\n  coord_sf(crs = \"+init=epsg:3460\")\n\n\n\n\n\nFigure 2.5: Map of New Zealand, showing the extracted bounding box (blue shaded area delimited with dashed lines).",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-maps.html#global-look",
    "href": "data-maps.html#global-look",
    "title": "2  Maps",
    "section": "2.6 Global Look",
    "text": "2.6 Global Look\nLet us have a global look.\nWe first load a world map from the {maps} package.\n\nworld &lt;- sf::st_as_sf(maps::map('world', plot = FALSE, fill = TRUE)) |&gt; \n  mutate(ID = ifelse(ID == \"Ivory Coast\", \"Côte d'Ivoire\", ID))\n\n\n\nCode\nggplot(\n  data = world |&gt; \n    mutate(nz = ID %in% tb_countries$country_name),\n  mapping = aes(fill = nz)\n) +\n  geom_sf() +\n  scale_fill_manual(\n    NULL,\n    values = c(\"TRUE\" = \"#009E73\", \"FALSE\" = \"lightgray\"),\n    guide = \"none\"\n  ) +\n  theme_map_paper()\n\n\n\n\n\nFigure 2.6: New Zealand (in Green).",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maps</span>"
    ]
  },
  {
    "objectID": "data-weather-download.html",
    "href": "data-weather-download.html",
    "title": "3  Weather Data: Download",
    "section": "",
    "text": "3.1 Download and Extract Data\nThis chapter is compiled to be rendered as an HTML page. To avoid downloading the data everytime a compilation needs to be done, we define a variable, download_data, that is set to FALSE. Set the variable to TRUE if you want to download the data while evaluating the codes on your own.\nA few packages are needed:\nWe have shown in Chapter 1 some theme functions for plots. These functions are written in an R script that we can load to have access to those functions.\nWe extracted countries’ boundaries in Chapter 2.\nWe focus on the following countries defined in Chapter 2\nLet us import the raw grid data into R. We proceed in three steps:\nThe function from the second step is the following:\n#' From the NetCDF file, extract the daily temperatures (min or max) of the\n#' grid. Creates a tibble in which each row corresponds to a cell of the grid\n#' with the following columns:\n#'  - longitude: longitude of the centroid of the cell\n#'  - latitude: latitude of the centroid of the cell\n#'  - grid_id: country code x numerical identifier\n#'  - intersect_area: area of the intersection between the country and the\n#'    cell\n#'  - country code: ISO-3 country code\n#'  - geometry: the coordinates of the cell\n#'  - value: total daily precipitation\n#'  - date: YYYY-MM-DD date\n#' The tibble is saved in `../data/Weather/CPC/grid_daily_precip/`\n#'\n#' @param file Full path to the netCDF precipitation file.\n#' @param tb_countries Tibble with the ISO-3 country code (`tb_countries`),\n#'   the name of the country (`country_name`), the short name of the country \n#'   (`country_name_short`) and a name suitable for a filename \n#'   (`country_name_clear`).\n#' @param country_names Name of the countries for which to extract daily obs.\n#' @param maps_level_0 List of level 0 borders from shapefiles provided by GDAM.\n#' @param value_type Type of values retrieved (`\"precip\"`, `\"tmin\"` or `\"max\"`).\n#' \nextract_daily_grid &lt;- function(file,\n                               tb_countries,\n                               country_names,\n                               maps_level_0, \n                               value_type = c(\"precip\", \"tmin\", \"tmax\")) {\n  # Extract the year of the observation from the file name\n  year &lt;- str_extract(file, str_c(value_type, \"\\\\.([[:digit:]]*)\\\\.nc\")) |&gt;\n    str_remove(str_c(value_type, \"\\\\.\")) |&gt;\n    str_remove(\"\\\\.nc\")\n  \n  # Raster Brick\n  weather_data_raw &lt;- brick(file)\n  weather_data &lt;- rotate(weather_data_raw)\n  \n  cli::cli_progress_bar(total = nrow(tb_countries))\n  # for (country_name in tb_countries$country_name) {\n  for (country_name in country_names) {\n    # Corresponding ISO code\n    country_code &lt;- tb_countries |&gt;\n      filter(country_name == !!country_name) |&gt;\n      pull(\"country_code\")\n    \n    current_country_map &lt;- maps_level_0[[country_code]]\n    \n    if (country_name %in% c(\"Fiji\", \"New Zealand\")) {\n      # re-center geographical coordinates for a Pacific view\n      current_country_map &lt;- st_shift_longitude(current_country_map)\n      weather_data_current &lt;- weather_data_raw\n    } else {\n      weather_data_current &lt;- weather_data\n    }\n    \n    current_country_map_large &lt;- st_buffer(current_country_map, dist = 70000)\n    \n    # Crop accordingly to the bounding box (+ some buffer)\n    e_country &lt;- sf::st_bbox(current_country_map)\n    # Enlarge a bit so that more cells are imported for each the country\n    e_country[[\"xmin\"]] &lt;- e_country[[\"xmin\"]] - .5\n    e_country[[\"xmax\"]] &lt;- e_country[[\"xmax\"]] + .5\n    e_country[[\"ymin\"]] &lt;- e_country[[\"ymin\"]] - .5\n    e_country[[\"ymax\"]] &lt;- e_country[[\"ymax\"]] + .5\n    \n    rb_country &lt;- crop(weather_data_current, e_country)\n    \n    # From the Rasterbox to a tibble\n    sf_country &lt;- stars::st_as_stars(rb_country) |&gt;\n      sf::st_as_sf()\n    \n    # Centroids of each cell of the grid\n    centroids &lt;- sf::st_centroid(sf_country)\n    centroids_grid &lt;- as_tibble(sf::st_coordinates(centroids)) |&gt;\n      dplyr::mutate(\n        grid_id = str_c(country_code, row_number(), sep = \"_\"),\n        country_code = country_code\n      ) |&gt;\n      dplyr::rename(longitude = X, latitude = Y)\n    \n    # Compute the intersection between the country and each cell of the grid\n    intersect_area &lt;- map_dbl(\n      .x = 1:nrow(sf_country),\n      .f = function(i_cell) {\n        inters &lt;- suppressWarnings(suppressMessages(st_intersection(\n          x = sf_country |&gt; slice(i_cell),\n          y = maps_level_0[[country_code]]\n        )))\n        area &lt;- st_area(inters)\n        if (length(area) == 0) area &lt;- 0\n        area\n      }\n    )\n    \n    # Compute the intersection between the augmented country's border\n    # and each cell of the grid\n    intersect_area_larger &lt;- map_dbl(\n      .x = 1:nrow(sf_country),\n      .f = function(i_cell) {\n        inters &lt;- suppressWarnings(suppressMessages(st_intersection(\n          x = sf_country |&gt; slice(i_cell),\n          y = current_country_map_large\n        )))\n        area &lt;- st_area(inters)\n        if (length(area) == 0) area &lt;- 0\n        area\n      }\n    )\n    \n    # Create the grid\n    weather_grid_country &lt;- cbind(sf_country, centroids_grid) |&gt;\n      mutate(\n        intersect_area = intersect_area,\n        intersect_area_larger = intersect_area_larger\n      ) |&gt;\n      tidyr::pivot_longer(\n        cols = -c(\n          geometry, longitude, latitude,\n          grid_id, country_code, intersect_area, intersect_area_larger\n        ),\n        names_to = \"date_v\"\n      ) |&gt;\n      dplyr::mutate(date = str_remove(date_v, \"^X\") |&gt; lubridate::ymd()) |&gt;\n      dplyr::select(-date_v) |&gt; \n      dplyr::filter(intersect_area_larger &gt; 0) |&gt; \n      dplyr::select(-intersect_area_larger)\n    \n    file_name &lt;- str_c(country_code, \"_\", year, \".RData\")\n    \n    save(\n      weather_grid_country,\n      file = str_c(\n        \"../Data/Weather/CPC/grid_daily_\", \n        value_type, \"/\", file_name\n      )\n    )\n    cli::cli_progress_update(inc = 1)\n  }\n  NULL\n}\nThe start and end years for the data to download:\nstart_year &lt;- 1980\nend_year &lt;- 2024\nWe saw in Chapter 2 that for New Zealand, the bounding box causes issues: the minimum longitude is close to -180° and the maximum is close to +180°. Hence, the bounding box will lead to import too many tiles.\nbboxes &lt;- list(\n  \n)\nbboxes$NZL[[\"xmin\"]] &lt;- 167\nbboxes$NZL[[\"xmax\"]] &lt;- 179\nbboxes$NZL[[\"ymin\"]] &lt;- -48\nbboxes$NZL[[\"ymax\"]] &lt;- -34",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Weather Data: Download</span>"
    ]
  },
  {
    "objectID": "data-weather-download.html#sec-extract-raw-data",
    "href": "data-weather-download.html#sec-extract-raw-data",
    "title": "3  Weather Data: Download",
    "section": "",
    "text": "First, we download the daily data from the NOAA PSL’s website, covering the period from 1980 to 2024. Each file containing the raw data correspond to a year and a metric (precipitation, min temperature, max temperature).\nSecond, using a function that we define, we extract the cells of the grid that correspond to each country. For each year, country, and metric we save the extracted raw data in a RData file.\nThird, we load all the extracted daily data into R and create a tibble for each country.\n\n\n\n\n\n\n\n\n3.1.1 Precipitation\nWe create the folder that will contain the raw data and the temporary (year x country) precipitation data.\n\nif (!dir.exists(\"../data/Weather/CPC/grid_daily_precip/\")) {\n  dir.create(\"../data/Weather/CPC/grid_daily_precip/\", recursive = TRUE)\n}\n\nThe URLs of the files we want to download:\n\nurls &lt;- str_c(\n  \"https://downloads.psl.noaa.gov/\",\n  \"Datasets/cpc_global_precip/precip.\", seq(start_year, end_year), \".nc\"\n)\n\nLet us loop on these URLs to download the worldwide daily data for each year:\n\nif (download_data == TRUE) {\n  for (file in urls) {\n    file_name &lt;- str_extract(file, \"cpc_global_precip/(.*)$\") |&gt;\n      str_remove(\"cpc_global_precip/\")\n    dest_file &lt;- str_c(\"../data/Weather/CPC/grid_daily_precip/\", file_name)\n    download.file(file, destfile = dest_file)\n  }\n}\n\nThen, we can load the raw data in R and apply the extract_daily_grid() function to extract the daily precipitation for each country.\n\nN &lt;- list.files(\n  \"../data/Weather/CPC/grid_daily_precip/\",\n  pattern = \"*\\\\.nc\", full.names = TRUE\n)\n\nExtracting values for the countries of interest:\n\nfor(i in 1:length(N)) {\n  cat(str_c(N[i], \"\\nNumber \", i , \"/\", length(N)))\n  extract_daily_grid(\n    file = N[i], \n    tb_countries = tb_countries, \n    country_names = c(\"New Zealand\"), \n    maps_level_0 = maps_level_0, \n    value_type = \"precip\"\n  )\n}\n\n\n\n3.1.2 Maximum Temperatures\nWe create the folder that will contain the raw data and the temporary (year x country) maximum temperature data.\n\nif (!dir.exists(\"../data/Weather/CPC/grid_daily_tmax/\")) {\n  dir.create(\"../data/Weather/CPC/grid_daily_tmax/\")\n}\n\nThe URLs of the files we want to download:\n\nurls &lt;- str_c(\n  \"https://downloads.psl.noaa.gov/\",\n  \"Datasets/cpc_global_temp/tmax.\", seq(start_year, end_year), \".nc\"\n)\n\nLet us loop on these URLs to download the worldwide daily data for each year:\n\nif (download_data == TRUE) {\n  for (file in urls) {\n    file_name &lt;- str_extract(file, \"cpc_global_temp/(.*)$\") |&gt;\n      str_remove(\"cpc_global_temp/\")\n    download.file(\n      file,\n      destfile = str_c(\"../data/Weather/CPC/grid_daily_tmax/\", file_name)\n    )\n  }\n}\n\nThen, we can load the raw data in R and apply the extract_daily_grid() function to extract the daily maximum temperatures for each country.\n\nN &lt;- list.files(\n  \"../data/Weather/CPC/grid_daily_tmax/\",\n  pattern = \"*\\\\.nc\", full.names = TRUE\n)\n\nExtracting values for the countries of interest:\n\nfor(i in 1:length(N)) {\n  cat(N[i])\n  cat(str_c(N[i], \"\\nNumber \", i , \"/\", length(N)))\n  extract_daily_grid(\n    file = N[i], \n    tb_countries = tb_countries, \n    country_names = c(\"New Zealand\"), \n    maps_level_0 = maps_level_0, \n    value_type = \"tmax\"\n  )\n}\n\n\n\n3.1.3 Minimum Temperatures\nWe create the folder that will contain the raw data and the temporary (year x country) minimum temperature data.\n\nif (!dir.exists(\"../data/Weather/CPC/grid_daily_tmin/\")) {\n  dir.create(\"../data/Weather/CPC/grid_daily_tmin/\")\n}\n\nThe URLs of the files we want to download:\n\nurls &lt;- str_c(\n  \"https://downloads.psl.noaa.gov/\",\n  \"Datasets/cpc_global_temp/tmin.\", seq(start_year, end_year), \".nc\"\n)\n\nLet us loop on these URLs to download the worldwide daily data for each year:\n\nif (download_data == TRUE) {\n  for (file in urls) {\n    file_name &lt;- str_extract(file, \"cpc_global_temp/(.*)$\") |&gt;\n      str_remove(\"cpc_global_temp/\")\n    download.file(\n      file,\n      destfile = str_c(\"../data/Weather/CPC/grid_daily_tmin/\", file_name)\n    )\n  }\n}\n\nThen, we can load the raw data in R and apply the extract_daily_grid() function to extract the daily minimum temperatures for each country.\n\nN &lt;- list.files(\n  \"../data/Weather/CPC/grid_daily_tmin/\",\n  pattern = \"*\\\\.nc\", full.names = TRUE\n)\n\nExtracting values for the countries of interest:\n\nfor(i in 1:length(N)) {\n  cat(N[i])\n  cat(str_c(N[i], \"\\nNumber \", i , \"/\", length(N)))\n  extract_daily_grid(\n    file = N[i], \n    tb_countries = tb_countries, \n    country_names = c(\"New Zealand\"), \n    maps_level_0 = maps_level_0, \n    value_type = \"tmin\"\n  )\n}",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Weather Data: Download</span>"
    ]
  },
  {
    "objectID": "data-weather-missing.html",
    "href": "data-weather-missing.html",
    "title": "4  Weather Data: Missing Data",
    "section": "",
    "text": "4.1 Import Data\nLet us load the graphs theme functions (See Chapter 1):\nLet us also load the maps for the countries of interest (See Chapter 2).\nWe focus on the countries defined in Chapter 2:\nWe define a function, load_daily(), which loads the daily data obtained in Chapter 3.\n#' Loads a weather data from country x year\n#'\n#' @param file path to the file to load\nload_daily &lt;- function(file) {\n  load(file)\n  weather_grid_country\n}\nLet us focus on New Zealand.\ncountry &lt;- \"New Zealand\"\nWe extract the corresponding country code:\ncountry_code &lt;- tb_countries |&gt;\n  dplyr::filter(country_name == !!country) |&gt;\n  dplyr::pull(\"country_code\")\nWe list the files that contain the daily data for the current country:\nN &lt;- list(\n  precip = \"../Data/Weather/CPC/grid_daily_precip/\",\n  tmin = \"../Data/Weather/CPC/grid_daily_tmin/\",\n  tmax = \"../Data/Weather/CPC/grid_daily_tmax/\"\n) |&gt;\n  map(~list.files(.x, full.names = TRUE, pattern = str_c(country_code, \"_\")))\nLet us load these files:\n# Load precip, tmin and tmax\ncountry_precip_daily &lt;- map(N$precip, load_daily) |&gt;list_rbind()\ncountry_tmin_daily &lt;- map(N$tmin, load_daily) |&gt;list_rbind()\ncountry_tmax_daily &lt;- map(N$tmax, load_daily) |&gt;list_rbind()\nLet us merge these datasets:\ncountry_weather_daily &lt;- country_precip_daily |&gt;\n    dplyr::mutate(variable = \"precip\") |&gt;\n    bind_rows(\n      country_tmin_daily  |&gt; mutate(variable = \"tmin\")\n    ) |&gt;\n    bind_rows(\n      country_tmax_daily  |&gt; mutate(variable = \"tmax\")\n    ) |&gt;\n    dplyr::select(-grid_id) |&gt;\n  pivot_wider(values_from = value, names_from = variable) |&gt; \n  group_by(longitude, latitude) |&gt; \n  dplyr::mutate(cell_id = cur_group_id()) |&gt; \n  ungroup()\nWe extract information about longitude / latitude and geometry of each cell:\ncells &lt;- country_weather_daily |&gt; \n  dplyr::select(cell_id, longitude, latitude, geometry, intersect_area) |&gt; \n  unique()\n\ncells &lt;- cells |&gt; dplyr::mutate(country = !!country)\nLet us transform this object as an sf object:\ncells_sf &lt;- st_as_sf(cells)\nWe can have a look at the number of values per cell.\n# This code is not evaluated here.\n#| code-fold: true\nggplot() + \n  geom_sf(\n    data = st_as_sf(cells) |&gt; \n      left_join(\n        country_weather_daily |&gt; group_by(cell_id) |&gt; count() |&gt; \n          arrange(n),\n        by = \"cell_id\"\n      ),\n    mapping = aes(fill = n)\n  ) +\n  geom_sf(\n    data = maps_level_0_NZ$NGA,\n    fill = NA, col = \"black\"\n  )\nWe can identify the cells for which the number of records is lower than the max number of records:\ncells_to_remove &lt;- \n  country_weather_daily |&gt; group_by(cell_id) |&gt; count() |&gt; \n  ungroup() |&gt; \n  mutate(max = max(n)) |&gt; \n  filter(n &lt; max)\n# This code is not evaluated here.\n#| code-fold: true\nggplot() + \n  geom_sf(\n    data = st_as_sf(cells) |&gt; \n      left_join(\n        country_weather_daily |&gt; group_by(cell_id) |&gt; count() |&gt; \n          arrange(n),\n        by = \"cell_id\"\n      ) |&gt; \n      left_join(\n        cells_to_remove |&gt; select(cell_id) |&gt; \n          mutate(to_remove = TRUE)\n      ) |&gt; \n      mutate(to_remove = replace_na(to_remove, FALSE)),\n    mapping = aes(fill = to_remove)\n  ) +\n  geom_sf(\n    data = maps_level_0_NZ,\n    fill = NA, col = \"black\"\n  ) +\n  scale_fill_manual(\n    \"Remove\", \n    values = c(`TRUE` = \"red\", `FALSE` = NA)\n  )\nLet us remove those cells:\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  filter(! cell_id %in% cells_to_remove$cell_id)\nWe also remove the cells corresponding to islands for which we will have no agricultural records.\nids_islands &lt;- c(\n  1, 2, 5, 6, 24, 25, 34, 35, 45, 46, 55, 56, 256, 250, 249, 255,\n  264, 267, 263, 266, 261, 260, 259, 262, 265\n)\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  filter(\n    ! cell_id %in% ids_islands\n  )\nCode\n# This chunk is not evaluated here.\nggplot() + \n  geom_sf(\n    data = st_as_sf(cells) |&gt; \n      inner_join(\n        country_weather_daily |&gt; group_by(cell_id) |&gt; count() |&gt; \n          arrange(n),\n        by = \"cell_id\"\n      ),\n    fill = \"red\"\n  ) +\n  geom_sf(\n    data = maps_level_0_NZ,\n    fill = NA, col = \"black\"\n  )\ncells &lt;- \n  cells |&gt; filter(\n  ! cell_id %in% c(cells_to_remove$cell_id, ids_islands)\n)\n\ncells_sf &lt;- st_as_sf(cells)\nLet us save this grid:\nsave(\n  cells,\n  file = str_c(\"../data/Weather/CPC/\", country_code, \"_cells.RData\")\n)\nWe remove the geometry from the table:\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  dplyr::select(-geometry, -intersect_area)\nThe total number of observations:\nscales::number(nrow(country_weather_daily), big.mark = \",\")\n\n[1] \"2,975,097\"",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Weather Data: Missing Data</span>"
    ]
  },
  {
    "objectID": "data-weather-missing.html#missing-values",
    "href": "data-weather-missing.html#missing-values",
    "title": "4  Weather Data: Missing Data",
    "section": "4.2 Missing Values",
    "text": "4.2 Missing Values\nHow many missing values are there here in the dataset?\n\ncountry_weather_daily |&gt; \n  summarise(across(everything(), ~sum(is.na(.x))))\n\n# A tibble: 1 × 8\n  longitude latitude country_code  date precip   tmin   tmax cell_id\n      &lt;int&gt;    &lt;int&gt;        &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n1         0        0            0     0  16895 446893 445241       0\n\n\n\n4.2.1 Precipitation\nThe number of missing data for precipitation per cell:\n\ntb_nb_na_precip &lt;- country_weather_daily |&gt; \n  group_by(cell_id) |&gt; \n  select(cell_id, precip) |&gt; \n  summarise(nb_na_precip = sum(is.na(precip))) |&gt; \n  arrange(desc(nb_na_precip))\ntb_nb_na_precip\n\n# A tibble: 181 × 2\n   cell_id nb_na_precip\n     &lt;int&gt;        &lt;int&gt;\n 1     153         9875\n 2       9           39\n 3      12           39\n 4      13           39\n 5      14           39\n 6      19           39\n 7      20           39\n 8      21           39\n 9      22           39\n10      27           39\n# ℹ 171 more rows\n\n\nWhat if we look at missing values per cell and day?\n\ncountry_weather_daily |&gt; \n  group_by(date) |&gt; \n  summarise(nb_na_precip = sum(is.na(precip))) |&gt; \n  filter(nb_na_precip &gt; 0) |&gt; \n  arrange(desc(nb_na_precip)) |&gt; \n  DT::datatable()\n\n\n\n\n\nAnd by cell?\n\nggplot() +\n  geom_sf(\n    data = cells_sf |&gt; \n      left_join(tb_nb_na_precip),\n    alpha = .2,\n    mapping = aes(fill = nb_na_precip)\n  ) +\n  geom_sf(data = maps_level_0_NZ, fill = NA) +\n  scale_fill_gradient(\"NAs Min Temperature\", low = \"white\", high = \"red\") +\n  theme_map_paper()\n\nJoining with `by = join_by(cell_id)`\n\n\n\n\n\nFigure 4.1: Number of missing values for precipitation per cell\n\n\n\n\n\n\n\n\nMost missing precipitation observations are from cell 153 which is covering a tiny fraction of land.\n\n\n4.2.2 Minimum Temperatures\nNow, let us do the same for min temperatures:\n\ntb_nb_na_tmin &lt;- country_weather_daily |&gt; \n  group_by(cell_id) |&gt; \n  select(cell_id, tmin) |&gt; \n  summarise(nb_na_tmin = sum(is.na(tmin))) |&gt; \n  arrange(desc(nb_na_tmin))\ntb_nb_na_tmin\n\n# A tibble: 181 × 2\n   cell_id nb_na_tmin\n     &lt;int&gt;      &lt;int&gt;\n 1     152      16437\n 2     171      16437\n 3     196      16437\n 4     221      16437\n 5     239      16437\n 6     258      16437\n 7       9       6979\n 8      14       6979\n 9      22       6979\n10      27       6979\n# ℹ 171 more rows\n\n\n\nggplot() +\n  geom_sf(\n    data = cells_sf |&gt; \n      left_join(tb_nb_na_tmin),\n    alpha = .2,\n    mapping = aes(fill = nb_na_tmin)\n  ) +\n  geom_sf(data = maps_level_0_NZ, fill = NA) +\n  scale_fill_gradient(\"Min Temperature\", low = \"white\", high = \"red\") +\n  theme_map_paper()\n\nJoining with `by = join_by(cell_id)`\n\n\n\n\n\nFigure 4.2: Number of missing values for minimum temperatures per cell\n\n\n\n\n\n\n\n\nSpatially, missing observation are mostly on coastal areas.\nFor each cell, the number of daily observations from 1980-01-01 to 2024-12-31 is:\n\nseq(\n  min(country_weather_daily$date), \n  max(country_weather_daily$date), \n  by = \"day\"\n) |&gt; \n  length()\n\n[1] 16437\n\n\nWhat if we look at missing values per cell and day?\n\ncountry_weather_daily |&gt; \n  group_by(date) |&gt; \n  summarise(nb_na_tmin = sum(is.na(tmin))) |&gt; \n  filter(nb_na_tmin &gt; 10) |&gt; \n  arrange(desc(nb_na_tmin)) |&gt; \n  DT::datatable()\n\n\n\n\n\n\n\n4.2.3 Maximum Temperatures\nAnd for max temperatures:\n\ntb_nb_na_tmax &lt;- \n  country_weather_daily |&gt; \n  group_by(cell_id) |&gt; \n  select(cell_id, tmax) |&gt; \n  summarise(nb_na_tmax = sum(is.na(tmax))) |&gt; \n  arrange(desc(nb_na_tmax))\ntb_nb_na_tmax\n\n# A tibble: 181 × 2\n   cell_id nb_na_tmax\n     &lt;int&gt;      &lt;int&gt;\n 1     152      16437\n 2     171      16437\n 3     196      16437\n 4     221      16437\n 5     239      16437\n 6     258      16437\n 7       9       6971\n 8      14       6971\n 9      22       6971\n10      27       6971\n# ℹ 171 more rows\n\n\n\nggplot() +\n  geom_sf(\n    data = cells_sf |&gt; \n      left_join(tb_nb_na_tmax),\n    alpha = .2,\n    mapping = aes(fill = nb_na_tmax)\n  ) +\n  geom_sf(data = maps_level_0_NZ, fill = NA) +\n  scale_fill_gradient(\"Max Temperature\", low = \"white\", high = \"red\") +\n  theme_map_paper()\n\nJoining with `by = join_by(cell_id)`\n\n\n\n\n\nFigure 4.3: Number of missing values for maximum temperatures per cell\n\n\n\n\n\n\n\n\nWhat if we look at missing values per cell and day?\n\ncountry_weather_daily |&gt; \n  group_by(date) |&gt; \n  summarise(nb_na_tmax = sum(is.na(tmax))) |&gt; \n  filter(nb_na_tmax &gt; 10) |&gt; \n  arrange(desc(nb_na_tmax)) |&gt; \n  DT::datatable()",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Weather Data: Missing Data</span>"
    ]
  },
  {
    "objectID": "data-weather-missing.html#temporal-interpolation",
    "href": "data-weather-missing.html#temporal-interpolation",
    "title": "4  Weather Data: Missing Data",
    "section": "4.3 Temporal Interpolation",
    "text": "4.3 Temporal Interpolation\nLet us use splines at the cell level to interpolate missing values for which no more than 5 consecutive values are missing. We will then address the interpolation of missing values for the south-western cells.\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  arrange(date) |&gt; \n  nest(.by = cell_id) |&gt; \n  mutate(\n    precip_new = map(data, ~{\n      if (sum(is.na(.x$precip)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$precip, maxgap = 5, option = \"spline\")\n      }\n      new_val}\n    ),\n    tmin_new = map(data, ~{\n      if (sum(is.na(.x$tmin)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$tmin, maxgap = 5, option = \"spline\")\n      }\n      new_val}\n    ),\n    tmax_new = map(data, ~{\n      if (sum(is.na(.x$tmax)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$tmax, maxgap = 5, option = \"spline\")\n      }\n      new_val}\n    )\n  ) |&gt; \n  unnest(cols = c(data, precip_new, tmin_new, tmax_new))\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nWith the interpolation, there might be negative values, which is not possible for precipitation. We set to 0 precipitation for which interpolated values are negative.\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  mutate(\n    precip_new = ifelse(precip_new &lt; 0, 0, precip_new)\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Weather Data: Missing Data</span>"
    ]
  },
  {
    "objectID": "data-weather-missing.html#spatial-interpolation",
    "href": "data-weather-missing.html#spatial-interpolation",
    "title": "4  Weather Data: Missing Data",
    "section": "4.4 Spatial Interpolation",
    "text": "4.4 Spatial Interpolation\nNow, let us address the problem of the remaining cells for which we have some missing observation. First, we identify cells IDs for which observations are missing for precipitation, and for min and max temperatures.\n\ncell_id_missing_precip &lt;- \n  tb_nb_na_precip |&gt; pull(cell_id)\ncell_id_missing_tmin &lt;- \n  tb_nb_na_tmin |&gt; pull(cell_id)\ncell_id_missing_tmax &lt;- \n  tb_nb_na_tmax |&gt; pull(cell_id)\n\nWe will replace the missing observations with weighted averages of the values found in the neighbors cells. We will consider the 5 nearest cells and use the inverse distance to those as the weights in the weighted average.\nFirst, let us get the distance matrix of the cells, using the centroid of each cell as the reference point.\n\ncentroids_cells &lt;- sf::st_centroid(cells_sf)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\ndistance_matrix &lt;- sf::st_distance(centroids_cells, centroids_cells)\n\nWe can then define the find_neighbors_radius() function which returns the cells within a radius of another cell:\n\n#' Identifies the neighbors of a cell given a radius\n#' @param id cell id\n#' @param radius radius in metres to identify the neighbors\nfind_neighbors_radius &lt;- function(id, radius) {\n  ind_current_cell &lt;- which(cells_sf$cell_id == id)\n  tibble(\n    cell_id = id,\n    cell_id_neighbors = cells_sf$cell_id,\n    distance = units::drop_units(distance_matrix[, ind_current_cell])\n  ) |&gt; \n    arrange(distance) |&gt; \n    filter(cell_id_neighbors != !!id) |&gt; \n    filter(distance &lt;= !!radius)\n}\n\nFor example, for cell ID 12, the cells within 150km radius:\n\nexample_nn &lt;- find_neighbors_radius(id = 12, 150*1000)\nexample_nn\n\n# A tibble: 14 × 3\n   cell_id cell_id_neighbors distance\n     &lt;dbl&gt;             &lt;int&gt;    &lt;dbl&gt;\n 1      12                19   38446.\n 2      12                13   55597.\n 3      12                 9   67695.\n 4      12                20   67695.\n 5      12                29   76893.\n 6      12                28   94601.\n 7      12                30   95169.\n 8      12                14  111194.\n 9      12                39  115338.\n10      12                21  117767.\n11      12                38  127563.\n12      12                40  128510.\n13      12                27  134789.\n14      12                31  135586.\n\n\n\n\nCode\ncreate_circle &lt;- function(long, lat, radius){ \n  tibble(degree = 1:360) |&gt; \n    rowwise() |&gt; \n    mutate(\n      long = geosphere::destPoint(c(long, lat), degree, radius)[1],\n      lat = geosphere::destPoint(c(long, lat), degree, radius)[2]\n    ) |&gt; \n    ungroup()\n}\n\ncell_of_interest &lt;- centroids_cells |&gt; filter(cell_id == 12)\ncircle_sf &lt;- create_circle(\n  long = cell_of_interest$longitude,\n  lat = cell_of_interest$latitude,\n  radius = 150*1000\n) |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = sf::st_crs(centroids_cells)) |&gt; \n  summarise(geometry = st_combine(geometry)) %&gt;% \n  st_cast(\"POLYGON\")\n\nggplot() +\n  geom_sf(\n    data = cells_sf |&gt; \n      mutate(\n        type = case_when(\n          cell_id == 12 ~ \"Cell 12\",\n          cell_id %in% example_nn$cell_id_neighbors ~ \"Neighbors\",\n          TRUE ~ \"Other\"\n        )\n      ),\n    mapping = aes(fill = type),\n    colour = \"black\"\n  ) + \n  geom_sf(data = maps_level_0$NGA, fill = NA) +\n  geom_sf(\n    data = circle_sf, \n    color = \"black\", fill = NA, linewidth = .8, linetype = \"dashed\"\n  ) +\n  scale_fill_manual(\n    NULL, \n    values = c(\"Cell 12\" = \"#D55E00\", \"Neighbors\" = \"#009E73\", \"Other\" = \"lightgray\")\n  ) +\n  theme_map_paper() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4.4: Cells within a 150km radius from cell 12\n\n\n\n\n\n\n\n\nNow, let us get all the neighbors of the cells with many missing values for min temperatures, and for max temperatures.\n\nneighbors_precip &lt;- \n  map(cell_id_missing_precip, ~find_neighbors_radius(id = .x, radius = 150*1000)) |&gt; \n  list_rbind()\n\nneighbors_tmin &lt;- \n  map(cell_id_missing_tmin, ~find_neighbors_radius(id = .x, radius = 150*1000)) |&gt; \n  list_rbind()\n\nneighbors_tmax &lt;- \n  map(cell_id_missing_tmax, ~find_neighbors_radius(id = .x, radius = 150*1000)) |&gt; \n  list_rbind()\n\nWe then define the impute_na_neighbor_cells() function that identifies the values of a metric of interest (argument name_col) from the neighbor (found in argument neighbors) cells of a given cell (whose ID is given to the argument id).\n\n#' @param id cell ID\n#' @param neighbors tibble with distances of the neighbors (obtained with \n#'  `find_neighbors_radius()`)\n#' @param name_col name of the columns in `country_weather_daily` to use to \n#'  impute data (from neighbors)\nimpute_na_neighbor_cells &lt;- function(id, neighbors, name_col) {\n  country_weather_daily |&gt; filter(cell_id == !!id) |&gt; \n    left_join(neighbors, relationship = \"many-to-many\", by = \"cell_id\") |&gt; \n    left_join(\n      country_weather_daily |&gt;\n        select(\n          cell_id_neighbors = cell_id, \n          date,\n          new_val_neighbors = !!sym(name_col)\n        ) |&gt; \n        sf::st_drop_geometry(),\n      by = c(\"date\", \"cell_id_neighbors\")\n    ) |&gt; \n    select(date, cell_id, cell_id_neighbors, new_val_neighbors, distance) |&gt; \n    filter(!is.na(new_val_neighbors)) |&gt; \n    group_by(cell_id, date) |&gt; \n    mutate(weight_distance = distance / sum(distance)) |&gt; \n    mutate(new_val_neighbors = new_val_neighbors * weight_distance) |&gt; \n    summarise(\n      new_val_neighbors = sum(new_val_neighbors),\n      .groups = \"drop\"\n    )\n}\n\nWe apply this function to the cells for which many missing values were found, to impute values for min temperatures, and then for max temperatures.\n\nneighbors_precip_new_val &lt;- \n  map(\n    cell_id_missing_precip, \n    ~impute_na_neighbor_cells(\n      .x, neighbors = neighbors_precip, name_col = \"precip_new\")\n  ) |&gt; \n  list_rbind()\n\nneighbors_tmin_new_val &lt;- \n  map(\n    cell_id_missing_tmin, \n    ~impute_na_neighbor_cells(\n      .x, neighbors = neighbors_tmin, name_col = \"tmin_new\")\n  ) |&gt; \n  list_rbind()\n\nneighbors_tmax_new_val &lt;- \n  map(\n    cell_id_missing_tmax, \n    ~impute_na_neighbor_cells(\n      .x, neighbors = neighbors_tmax, name_col = \"tmax_new\")\n  ) |&gt; \n  list_rbind()\n\nWe replace the missing values in the table with daily observations:\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  left_join(\n    neighbors_precip_new_val |&gt; rename(precip_neighbors = new_val_neighbors)\n  ) |&gt; \n  left_join(\n    neighbors_tmin_new_val |&gt; rename(tmin_neighbors = new_val_neighbors)\n  ) |&gt; \n  left_join(\n    neighbors_tmax_new_val |&gt; rename(tmax_neighbors = new_val_neighbors)\n  ) |&gt; \n  mutate(\n    # If temporal interpolation was not enough, replace with spatially\n    # interpolated values\n    precip_new = ifelse(is.na(precip_new), precip_neighbors, precip_new),\n    tmin_new = ifelse(is.na(tmin_new), tmin_neighbors, tmin_new),\n    tmax_new = ifelse(is.na(tmax_new), tmax_neighbors, tmax_new)\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Weather Data: Missing Data</span>"
    ]
  },
  {
    "objectID": "data-weather-missing.html#second-temporal-interpolation",
    "href": "data-weather-missing.html#second-temporal-interpolation",
    "title": "4  Weather Data: Missing Data",
    "section": "4.5 Second Temporal Interpolation",
    "text": "4.5 Second Temporal Interpolation\nThere are still missing values:\n\nprecip: 2015-05-19 to 2015-05-26,\ntmin and tmax: 1983-04-25 to 1983-04-30 and 2015-05-21 to 2015-05-26.\n\n\ncountry_weather_daily |&gt; filter(is.na(precip_new)) |&gt; count(date)\ncountry_weather_daily |&gt; filter(is.na(tmin_new)) |&gt; count(date)\ncountry_weather_daily |&gt; filter(is.na(tmax_new)) |&gt; count(date)\n\nFor those observations, we use, again, a temporal interpolation. This time, we increase the windows’ width to 10 days.\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  arrange(date) |&gt; \n  nest(.by = cell_id) |&gt; \n  mutate(\n    precip_new_2 = map(data, ~{\n      if (sum(is.na(.x$precip_new)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$precip_new, maxgap = 10, option = \"spline\")\n      }\n      new_val}\n    ),\n    tmin_new_2 = map(data, ~{\n      if (sum(is.na(.x$tmin_new)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$tmin_new, maxgap = 10, option = \"spline\")\n      }\n      new_val}\n    ),\n    tmax_new_2 = map(data, ~{\n      if (sum(is.na(.x$tmax_new)) &gt;= nrow(.x)-1) {\n        new_val &lt;- NA\n      } else {\n        new_val &lt;- imputeTS::na_interpolation(.x$tmax_new, maxgap = 10, option = \"spline\")\n      }\n      new_val}\n    )\n  ) |&gt; \n  unnest(cols = c(data, precip_new_2, tmin_new_2, tmax_new_2)) |&gt; \n  mutate(\n    precip_new_2 = ifelse(precip_new_2 &lt; 0, 0, precip_new_2),\n    precip_new = ifelse(is.na(precip_new), precip_new_2, precip_new),\n    tmin_new = ifelse(is.na(tmin_new), tmin_new_2, tmin_new),\n    tmax_new = ifelse(is.na(tmax_new), tmax_new_2, tmax_new)\n  )\n\nLastly, we remove the raw values with missing data with the columns in which missing data were replaced.\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  dplyr::select(\n    -precip, -tmin, -tmax, \n    -precip_neighbors, -tmin_neighbors, -tmax_neighbors,\n    -precip_new_2, -tmin_new_2, -tmax_new_2\n  ) |&gt; \n  rename(precip = precip_new, tmin = tmin_new, tmax = tmax_new)\n\nLet us save the results for later use.\n\nsave(\n  country_weather_daily,\n  file = \"../data/Weather/CPC/NZL_daily_weather_corrected.rda\"\n)\n\nEach row gives three metrics for a grid cell identified with its ID (cell_id) whose centroid’s coordinates are given in longitude and latitude, at a given day (date). The country the data refer to (Nigeria) is stated in column country_code. The metrics are total rainfall (precip), minimum temperature (tmin) and maximum temperature (tmax).\nThe content of this dataset is summarized in Table 4.1.\n\n\n\nTable 4.1: Data dictionary for NGA_daily_weather_corrected.rda\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nlongitude\nnumeric\nLongitude of the centroid of the cell\n\n\nlatitude\nnumeric\nLatitude of the centroid of the cell\n\n\ncountry_code\nnumeric\nCountry code (ISO 3166-1 alpha-3 code)\n\n\ndate\ndate\nDate of the observation (YYYY-MM-DD)\n\n\ncell_id\ninteger\nUnique identifier of the cells of the grid\n\n\nprecip\nnumeric\nTotal rainfall, in mm\n\n\ntmin\nnumeric\nMinimum temperature, in °C\n\n\ntmax\nnumeric\nMaximum temperature, in °C",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Weather Data: Missing Data</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html",
    "href": "data-weather-metrics.html",
    "title": "5  Weather Data: Metrics",
    "section": "",
    "text": "5.1 Potential Evapotranspiration\nLet us load the graphs theme functions (See Chapter 1):\nLet us also load the maps for the countries of interest (See Chapter 2)\nWe focus on the following countries defined in Chapter 2:\nLastly, we load the weather data (see Chapter 4):\nWe rename the max and min temperature columns:\nLet us add the year, month, month name and day of year to the rows of the dataset:\nWe have daily precipitation \\(P_t\\), minimum temperatures \\(\\text{Tmin}_{t}\\) and maximum temperatures \\(\\text{Tmax}_{t}\\) at the grid cell level. We are interested in building a soil moisture index which requires to compute the evapotranspiration in a prior step. We follow the approach presented in Dingman (2015) (pp. 299–300, Box 6.8: Thornthwaite-Type Monthly Water-Balance Model) and recalled in Appendix S1 of Lutz, Wagtendonk, and Franklin (2010).",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#sec-pet",
    "href": "data-weather-metrics.html#sec-pet",
    "title": "5  Weather Data: Metrics",
    "section": "",
    "text": "5.1.1 Potential Evapotranspiration\nThe daily PET (in mm/day) writes (assumed to be from Hamon (1964), but the article cannot be found on the Internet ; Dingman (2015) Eq. 6.68 p. 294): \\[\n\\text{PET}_t =\n\\begin{cases}\n0, & \\text{if }\\mathrm{Tmean}_t \\le 0^\\circ\\mathrm{C},\\\\\n29.8 \\times \\text{DL}_t \\times \\dfrac{e^\\star(\\mathrm{Tmean}_t)}{\\mathrm{Tmean}_t + 273.2}, & \\mathrm{otherwise},\n\\end{cases}\n\\tag{5.1}\\]\nwhere \\(\\text{DL}_t\\) is the day length of day \\(t\\), expressed in hours and \\(e^\\star(\\cdot)\\) gives the saturation vapour pressure.\n\\(\\text{Tmean}_t\\) is the daily average temperature: \\[\n\\mathrm{Tmean}_t = \\frac{\\mathrm{Tmin}_t + \\mathrm{Tmax}_t}{2}.\n\\tag{5.2}\\]\n\n\n\n\n\n\nHow to compute day length \\(\\text{DL}_t\\)?\n\n\n\n\n\nThe methodology to compute day length is given in Appendix D of Dingman (2015). It writes:\n\\[\n\\text{DL}_t = \\frac{24}{\\pi}\\omega_s,\n\\tag{5.3}\\]\nwhere \\(\\omega_s\\) (in radians) is the sunrise hour angle. At sunrise and sunset, the solar zenith angle \\(\\theta_z\\) equals \\(90^\\circ + h_0\\), where \\(h_0\\) is the apparent altitude of the Sun’s center at sunrise. The sunrise hour angle \\(\\omega_s\\) satisfies: \\[\n\\cos(\\omega_s) = \\dfrac{\\sin(h_0) - \\sin(\\Lambda) \\sin(\\delta)}{\\cos(\\Lambda) \\cos(\\delta)},\n\\tag{5.4}\\] with \\(\\Lambda\\) the latitude (in radians), \\(\\delta\\) the solar declination (in radians) and \\(h_0 \\approx -0.833^\\circ\\).\nThe solar declination in radians writes (Campbell and Norman (1998)): \\[\n\\sin^{-1}(0.39795 \\times \\cos(0.2163108 + 2 \\tan^{-1}(0.9671396 \\times \\tan(0.0086 \\times (\\text{doy}_t - 186))) ))\n\\tag{5.5}\\]\n\n\n\n\n\n\n\n\n\nHow to compute saturation vapour pressure \\(e^\\star(T)\\)?\n\n\n\n\n\nThe saturation vapour pressure \\(e^\\star(\\cdot)\\) (in k pascals) is given, for a temperature \\(T\\) (in °C), by (Dingman (2015), Box 2.2 p.99): \\[\ne^\\star(T) = 0.611 \\exp\\left(\\frac{17.27\\times T}{T + 237.3}\\right)\n\\tag{5.6}\\]\n\n\n\nLet us compute the daily potential evapotranspiration in R. We define two functions:\n\ndecl_angle(), which computes the solar declination in radians for a given day-of-year (the code is from {TrenchR}),\ndaylength_hours(), which computes the day length (in hours) for a given day-of-year, at a given latitude.\n\n\n\nThe decl_angle() function.\n#' @title Solar Declination in Radians (from {TrenchR})\n#' \n#' @description The function calculates solar declination, which is the angular \n#'  distance of the sun north or south of the earth's equator, based on the day \n#'   of year (Campbell and Norman, 1998)\n#' \n#' @param doy Day of year (1-366).\n#' \n#' @returns The declination angle (in radians).\n#'\n#' @references\n#' Campbell GS, Norman JM (1998). Introduction to environmental biophysics, \n#'  2nd ed. edition. Springer, New York. ISBN 0387949372.\ndecl_angle &lt;- function (doy) {\n  \n  doy &lt;- (doy - 1) %% 365 + 1\n  \n  rev_ang &lt;- 0.2163108 + 2 * atan(0.9671396 * tan(0.0086 * (doy -186))) \n  asin(0.39795 * cos(rev_ang)) \n  \n}\n\n\n\n\nThe daylength_hours() function.\n#' Day length (in hours)\n#' \n#' @param lat_deg Latitude (in degrees)\n#' @param doy Day of year (1-366).\n#' \ndaylength_hours &lt;- function(lat_deg, doy) {\n  \n  lambda &lt;- lat_deg * pi / 180\n  delta &lt;- decl_angle(doy)\n  h0 &lt;- -0.833 * pi / 180 # apparent sunrise altitude\n  \n  # General sunrise equation with altitude:\n  # cos(ws) = (sin(h0) - sin(lambda) sin(delta)) / (cos(lambda) cos(delta))\n  num &lt;- sin(h0) - sin(lambda) * sin(delta)\n  den &lt;- cos(lambda) * cos(delta)\n  cos_ws &lt;- num / den\n  cos_ws &lt;- pmin(pmax(cos_ws, -1), 1) # clamp to [-1,1]\n  ws &lt;- acos(cos_ws)\n  \n  (24/pi) * ws\n}\n\n\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  mutate(\n    temp_mean = (temp_min + temp_max) / 2,\n    dl_hours  = daylength_hours(latitude, day_of_year)\n  ) |&gt; \n  arrange(cell_id, date) |&gt; \n  # Daily PET (Hamon)\n  mutate(\n    esat = 0.611 * exp(17.27 * temp_mean / (temp_mean + 237.3)), # kPa\n    PET_daily = 29.8 * dl_hours * (esat / (temp_mean + 273.2)), # mm/day\n    PET_daily = if_else(temp_mean &lt;= 0, 0, PET_daily) # Hamon convention\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#water-deficit-water-balance",
    "href": "data-weather-metrics.html#water-deficit-water-balance",
    "title": "5  Weather Data: Metrics",
    "section": "5.2 Water Deficit (Water Balance)",
    "text": "5.2 Water Deficit (Water Balance)\nThe equation for soil water balance depends on whether water input (\\(W_t\\)) exceeds potential evapostranspiration (Lutz, Wagtendonk, and Franklin (2010)):\n\\[\n\\text{SW}_t = \\begin{cases}\n\\min \\left\\{S_\\text{max}, (W_t - \\text{PET}_t) + \\text{SW}_{t-1}\\right\\},& \\text{if } W_t \\geq \\text{PET}_{t},\\\\\n\\text{SW}_{t-1} - \\Delta_{\\text{soil},t}, & \\text{otherwise},\n\\end{cases}\n\\tag{5.7}\\]\nwhere \\(\\Delta_{\\text{soil},t}\\) is the fraction removed from storage: \\[\n\\Delta_{\\text{soil},t} = \\text{SW}_{t-1} \\times\\left( 1 - \\exp\\left(-\\dfrac{\\text{PET}_t - W_t}{S_\\text{max}}\\right)\\right),\n\\tag{5.8}\\]\n\\(S_{\\text{max}}\\) is the soil water-holding capacity in the top 200 cm of the soil profile. Ideally, this should be given from recorded values. We do not have this here, so we will use a value of 150mm:\n\nThe current NIWA water balance model uses a fixed soil moisture capacity of 150 mm of water, based on a typical loam soil. https://niwa.co.nz/sites/default/files/NZDI_more_info.pdf (New Zealand Drought Index and Drought Monitor Framework).\n\nThe actual evapotranspiration, \\(\\text{AET}_t\\) writes: \\[\n\\text{AET}_t = \\begin{cases}\n\\text{PET}_t, & \\text{if } W_t \\geq \\text{PET}_{t},\\\\\nW_t + \\Delta_{\\text{soil},t}, & \\text{otherwise}.\n\\end{cases}\n\\]\nThe deficit writes: \\[\nD_t = \\text{PET}_t - \\text{AET}_t.\n\\]\n\n\n\n\n\n\nHow to compute water input \\(W_t\\) with daily data?\n\n\n\n\n\nDaily snowmelt is often estimated using a degree-day approach, which assumes that melt is proportional to the number of degrees by which air temperature exceeds a threshold (typically 0°C). This formulation applies only to the existing snowpack, ensuring that snow deposited on a given day cannot melt immediately.\nFollowing the standard degree-day formulation (Dingman (2015), Eq. 5.71), the daily snowmelt is written as a linear function of air temperature: \\[\n\\text{Melt}_t = \\min\\left\\{\\text{DDF} \\times \\max(\\mathrm{Tmean}_t - T_0,\\, 0),\\, \\text{Pack}_{t-1}\\right\\}\n\\tag{5.9}\\]\nwhere \\(\\text{DDF}\\) is the degree-day factor (mm day\\(^{-1}\\)°C\\(^{-1}\\)), typically in the range 2–5 mm day\\(^{-1}\\)°C\\(^{-1}\\) depending on snow properties and surface conditions. The parameter \\(T_0\\) is the threshold temperature for melting (which is always set as 0°C). \\(\\mathrm{Tmean}_t\\) is the daily mean air temperature (see Equation 5.13). \\(\\text{Pack}_{t-1}\\) is the snow water equivalent remaining from the previous day.\nThe snowpack evolves according to the mass balance: \\[\n\\text{Pack}_t = \\text{Pack}_{t-1} + \\text{Snow}_t - \\text{Melt}_t.\n\\]\nThe snow pack equation is recursive. We simply set the start value at 0 for the first date of the sequence of values within a cell.\n\\(\\text{Snow}_t\\) is the amount of snow, which is the amount of precipitation if the average daily temperature is lower or equal to \\(T_0\\), and 0 otherwise: \\[\n\\text{Snow}_t = \\begin{cases}\nP_t, & \\text{if } T_{\\text{mean}_t} \\leq T_0 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\tag{5.10}\\]\nConversely, the amount of rain is defined as: \\[\n\\text{Rain}_t = \\begin{cases}\nP_t, & \\text{if } T_{\\text{mean}_t} &gt; T_0 \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\tag{5.11}\\]\nThe total water input to the soil becomes: \\[\nW_t = \\text{Rain}_t + \\text{Melt}_t.\n\\]\n\n\n\n\n\n\n\n\n\nHow to compute water input \\(W_t\\) with monthly data?\n\n\n\n\n\nMelt Factor and Rain/Snow Partition\nThe monthly precipitation can be divided into a rain fraction \\(\\text{Rain}_t\\) and a snow fraction \\(\\text{Snow}_t\\). To do so, we define the melt factor \\(F_t\\): \\[\nF_t = \\begin{cases}\n0, & \\text{if }\\text{Tmean}_t \\leq 0^\\circ,\\\\\n0.167 \\times \\text{Tmean}_t, & \\text{if } 0^\\circ &lt;\\text{Tmean}_t \\leq 6^\\circ,\\\\\n0, & \\text{if }\\text{Tmean}_t &gt; 6^\\circ.\n\\end{cases}\n\\tag{5.12}\\]\nwhere \\(\\text{Tmean}_t\\) is the daily average temperature: \\[\n\\mathrm{Tmean}_t = \\frac{\\mathrm{Tmin}_t + \\mathrm{Tmax}_t}{2}.\n\\tag{5.13}\\]\nThe rain and snow fractions can then be computed as follows: \\[\n\\begin{align}\n\\text{Rain}_t & = F_t \\times P_t\\\\\n\\text{Snow}_t & = (1 - F_t) \\times P_t\n\\end{align}\n\\tag{5.14}\\]\nRecursive Snowpack and Melt\nThe melt factor \\(F_t\\) is also used to define snowmelt \\(\\text{Melt}_t\\): \\[\n\\text{Melt}_t = F_t \\times (\\text{Snow}_t + \\text{Pack}_{t-1}),\n\\tag{5.15}\\]\nwhere snow pack for a given day is given by: \\[\n\\text{Pack}_t = (1 - F_t)^2 \\times P_t + (1 - F_t) \\times \\text{Pack}_{t-1}\n\\tag{5.16}\\]\nThe snow pack equation is recursive. We simply set the start value at 0 for the first date of the sequence of values within a cell.\nThe monthly water input to the soil is obtained as: \\[\nW_t = \\text{Rain}_t + \\text{Melt}_t\n\\tag{5.17}\\]\n\n\n\nLet us first compute water input \\(W_t\\).\n\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  mutate(\n    T0 = 0,# freezing temperature\n    rain  = if_else(temp_mean &gt; T0, precip, 0),\n    snow  = if_else(temp_mean &lt;= T0, precip, 0),\n    # Degree-day melt from existing pack only\n    DDF = 3, # mm/day/°C\n    pot_melt = pmax(temp_mean - T0, 0) * DDF\n  ) |&gt; \n  arrange(cell_id, date) |&gt; \n  group_by(cell_id) |&gt;\n  # Daily snowpack recursion and melt\n  mutate(\n    pack = {\n      n &lt;- n()\n      out &lt;- numeric(n); prev &lt;- 0\n      for (i in seq_len(n)) {\n        melt_i &lt;- min(pot_melt[i], prev)# melt from previous pack only\n        out[i] &lt;- prev + snow[i] - melt_i\n        prev &lt;- out[i]\n      }\n      out\n    },\n    melt = pmin(lag(pack, default = 0), pot_melt),\n    water_input = rain + melt\n  ) |&gt; \n  ungroup() |&gt;\n  select(-DDF, -T0, -pot_melt)\n\n\n\nCode if monthly data and not daily\n# This code is not evaluated here since we use daily data\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt; \n  mutate(\n    # Monthly melt factor and rain/snow split\n    melt_factor = case_when(\n      temp_mean &lt;= 0 ~ 0,\n      temp_mean &gt;= 6 ~ 1,\n      TRUE           ~ 0.167 * temp_mean\n    ),\n    rain = melt_factor * precip,\n    snow = (1 - melt_factor) * precip\n  ) |&gt; \n  arrange(cell_id, date) |&gt; \n  group_by(cell_id) |&gt;\n  # Monthly snowpack recursion and melt\n  mutate(\n    a = 1 - melt_factor,\n    b = (a^2) * precip, # term added each day\n    c = a, # multiplier on previous pack\n    pack = {\n      init &lt;- 0 # initial PACK for this cell_id\n      # pack_t = b_t + c_t * pack_{t-1}\n      out &lt;- purrr:::accumulate2(\n        b, c,\n        .init = init,\n        .f = function(bi, ci, prev) bi + ci * prev\n      )\n      as.numeric(tail(out, -1)) # drop the initial value\n    },\n    pack_lag = lag(pack, default = 0),\n    melt = melt_factor * (snow + pack_lag),\n    water_input = rain + melt\n  ) |&gt;\n  ungroup() |&gt;\n  select(-a, -b, -c)\n\n\nThen, we can compute soil water balance (\\(\\text{SW}_t\\)), actual evapotranspiration (\\(\\text{AET}_t\\)) and soil water deficit \\(D_t\\).\nWe define the update_soil_water_deficit() function that computes those values for a subset of observation corresponding to a cell, using the following input variables: \\(W_t\\), \\(\\text{PET}_t\\), \\(\\text{SW}_{t-1}\\) and \\(S_{\\text{max}}\\).\n\n\nThe update_soil_water_deficit() function.\n#' Compute Soil Water Deficit\n#' \n#' @param W Water input to the system (in mm).\n#' @param PET Potential evapotranspiration (in mm).\n#' @param S_prev Soil water balance in previous period (in mm).\n#' @param S_max Soil water-holding capacity in the top 200cm of the soil \n#'  profile (in mm).\n#' \n#' @returns A list with the following elements:\n#'  - `S_new`: soil water balance,\n#'  - `AET`: evapotranspiration,\n#'  - `surplus`: water surplus,\n#'  - `deficit`: water deficit\n#'  \nupdate_soil_water_deficit &lt;- function(W, PET, S_prev, S_max) {\n  \n  if (W &gt;= PET) {\n    # Water-abundant day: recharge first, overflow = surplus\n    S_star &lt;- (W - PET) + S_prev\n    # New value for soil water balance\n    S_new &lt;- min(S_star, S_max)\n    \n    AET &lt;- PET # Evapotranspiration\n    surplus &lt;- max(0, S_star - S_max)\n    deficit &lt;- 0\n  } else {\n    # Water-limited day: exponential draw from storage (Dingman/Lutz Eq. 13)\n    D &lt;- PET - W # unmet demand by inputs\n    dSOIL &lt;- S_prev * (1 - exp(-D / S_max))  # fraction removed from storage\n    # New value for soil water balance\n    S_new &lt;- S_prev - dSOIL\n    \n    AET &lt;- W + dSOIL # Evapotranspiration\n    surplus &lt;- 0\n    deficit &lt;- PET - AET\n  }\n  \n  list(\n    S_new = S_new, # Soil water balance\n    AET = AET, # Evapotranspiration\n    surplus = surplus, # Water surplus\n    deficit = deficit # Water deficit\n  )\n}\n\n\nWe use that function on subsets of the dataset where each subset corresponds to a cell.\n\n# Note: this chunk takes about 3 minutes to run.\n# It is not evaluated here during compilation.\nif (!file.exists(\"NZL_temprary_water_deficit.rda\")) {\n  # Ideally, we would need to use a value at the cell level.\n  Smax_default &lt;- 150\n  \n  country_weather_daily &lt;- \n    country_weather_daily |&gt;\n    arrange(cell_id, date) |&gt;\n    group_by(cell_id) |&gt;\n    # Prepare new columns\n    mutate(\n      AET = NA_real_, # Evapostranspiration\n      soil_moisture = NA_real_, # Water storage\n      soil_surplus = NA_real_, # Water surplus\n      soil_deficit = NA_real_ # Water deficit\n    ) |&gt;\n    # For each cell, compute soil water deficit recursively\n    group_modify(\\(tb, key){\n      n &lt;- nrow(tb)\n      S  &lt;- 0.5 * Smax_default\n      for (i in seq_len(n)) {\n        u &lt;- update_soil_water_deficit(\n          W = tb$water_input[i],\n          PET = tb$PET_daily[i],\n          S_prev = S,\n          S_max = Smax_default\n        )\n        tb$AET[i] &lt;- u$AET\n        tb$soil_moisture[i] &lt;- u$S_new\n        tb$soil_surplus[i] &lt;- u$surplus\n        tb$soil_deficit[i] &lt;- u$deficit\n        S &lt;- u$S_new\n      }\n      tb\n    }) |&gt;\n    ungroup()\n  \n  save(\n    country_weather_daily, \n    file = \"NZL_temprary_water_deficit.rda\"\n  )\n} else {\n  load(\"NZL_temprary_water_deficit.rda\")\n}\n\ncountry_weather_daily\n\n# A tibble: 2,975,097 × 25\n   cell_id longitude latitude country_code date        precip temp_min temp_max\n     &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;date&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1       9      166.    -45.8 NZL          1980-01-01 19.5        5.38     18.1\n 2       9      166.    -45.8 NZL          1980-01-02  6.58      10.7      12.3\n 3       9      166.    -45.8 NZL          1980-01-03  2.47       9.22     13.6\n 4       9      166.    -45.8 NZL          1980-01-04  0          6.35     18.1\n 5       9      166.    -45.8 NZL          1980-01-05  0          6.42     19.2\n 6       9      166.    -45.8 NZL          1980-01-06  0          8.80     19.1\n 7       9      166.    -45.8 NZL          1980-01-07  0.0987    12.0      18.2\n 8       9      166.    -45.8 NZL          1980-01-08  0.105     11.0      14.6\n 9       9      166.    -45.8 NZL          1980-01-09  0          5.38     16.0\n10       9      166.    -45.8 NZL          1980-01-10  0          7.00     17.8\n# ℹ 2,975,087 more rows\n# ℹ 17 more variables: year &lt;dbl&gt;, month &lt;dbl&gt;, month_name &lt;ord&gt;,\n#   day_of_year &lt;dbl&gt;, temp_mean &lt;dbl&gt;, dl_hours &lt;dbl&gt;, esat &lt;dbl&gt;,\n#   PET_daily &lt;dbl&gt;, rain &lt;dbl&gt;, snow &lt;dbl&gt;, pack &lt;dbl&gt;, melt &lt;dbl&gt;,\n#   water_input &lt;dbl&gt;, AET &lt;dbl&gt;, soil_moisture &lt;dbl&gt;, soil_surplus &lt;dbl&gt;,\n#   soil_deficit &lt;dbl&gt;",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#soil-moisture-deficit-index-smdi",
    "href": "data-weather-metrics.html#soil-moisture-deficit-index-smdi",
    "title": "5  Weather Data: Metrics",
    "section": "5.3 Soil Moisture Deficit Index (SMDI)",
    "text": "5.3 Soil Moisture Deficit Index (SMDI)\nThe Soil Moisture Deficit Index (SMDI, see Narasimhan and Srinivasan (2005)), turns daily soil water storage into a weekly drought/wetness index which takes values on \\([-4,4]\\) and which is comparable across locations and seasons. Negative values indicate dry conditions whereas positive values indicate wet conditions.\nSince we have daily observation, we need to compute weekly values for soil moisture. We assign each day to one of 52 fixed 7-day blocks starting on January 1: \\[\n\\text{week} = \\min\\left(\\left\\lfloor\\frac{\\text{yday}-1}{7}\\right\\rfloor + 1,\\ 52\\right).\n\\] Note that we do not use the week() function from {lubridate} to avoid ISO weeks (which can have 53).\nThen, for each grid cell (i), year (y), and week (w), compute the weekly mean available soil water: \\[\n\\mathrm{SW}_{i,y,w} = \\frac{1}{n_{i,y,w}} \\sum_{t \\in (i,y,w)} \\text{SW}_t,\n\\tag{5.18}\\]\nwhere \\(\\text{SW}_t\\) is the soil water balance (in mm), previously computed (see Equation 5.7).\nWe define a function, find_wday() which assigns a fixed 7-day “week” indice (1 to 52) to calendar dates.\n\n#' Assign fixed 7-day \"week\" indices (1–52) to calendar dates\n#'\n#' @description\n#' Divides each year into 52 fixed 7-day blocks, starting on January 1\n#' (block 1 = days 1–7, block 2 = days 8–14, ...\n#' Any remaining day(s) beyond day 364 (e.g., Dec 31 in common years,\n#' or Dec 30–31 in leap years) are assigned to week 52.\n#'\n#' @param x A `Date` vector.\n#' @returns\n#' An integer vector of the same length as `x`, giving week indices in `1:52`.\n#' \nfind_wday &lt;- function(x) {\n  pmin(((yday(x) - 1) %/% 7) + 1, 52)\n}\n\nExample:\n\nfind_wday(c(make_date(2020,12,31), make_date(2021,01,01)))\n\n[1] 52  1\n\n\n\n# First assign each day to one of the 52 weeks\ncountry_weather_daily &lt;- \n  country_weather_daily |&gt;\n  mutate(\n    # week = lubridate::week(date) # includes 53...\n    week = find_wday(date)\n  )\n\n# Then compute the average availabe soil water at the cell level\nsw_weekly &lt;- \n  country_weather_daily |&gt; \n  group_by(cell_id, year, week) |&gt;\n  summarise(SW = mean(soil_moisture, na.rm = TRUE), .groups = \"drop\")\n\nThe long-term weekly statistics at the cell level then need to be computed:\n\n\\(\\text{MSW}_{i,w}\\): the median of \\(\\text{SW}_{i,y,w}\\) over a long period (the entire sample, here),\n\\(\\text{SW}_{\\text{min},i,w}\\): the min of \\(\\text{SW}_{i,y,w}\\) over the same long period.\n\\(\\text{SW}_{\\text{max},i,w}\\): the max of \\(\\text{SW}_{i,y,w}\\) over the same long period.\n\n\n# Long-term weekly statistics\nsw_lt &lt;- \n  sw_weekly |&gt;\n  group_by(cell_id, week) |&gt;\n  summarise(\n    MSW = median(SW, na.rm = TRUE),\n    SW_min = min(SW, na.rm = TRUE),\n    SW_max = max(SW, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nThe weekly soil-water anomaly are then computed, in percent. For each (cell, year, week), a piecewise, range-normalized anomaly is computed as follows: \\[\n\\text{SD}_{i,y,w} =\n\\begin{cases}\n100 \\dfrac{\\text{SW}_{i,y,w} - \\text{MSW}_{i,w}}{\\text{MSW}_{i,w}-\\text{SW}_{\\min,i,w}}, & \\text{if } \\text{SW}_{i,y,w} \\le \\text{MSW}_{i,w},\\\\\n100 \\dfrac{\\text{SW}_{i,y,w} - \\text{MSW}_{i,w}}{\\text{SW}_{\\max,i,w}-\\mathrm{MSW}_{i,w}}, & \\text{otherwise}\n\\end{cases}\n\\tag{5.19}\\]\nWeekly soil water anomalies \\(\\text{SD}_{i,y,w}\\) will be negative when they are drier than the median for that week, positive when wetter, and naturally scaled by the local weekly climatological range.\n\n# Soil water deficit (%)\nsw_anom &lt;- \n  sw_weekly |&gt;\n  left_join(sw_lt, by = c(\"cell_id\", \"week\")) |&gt;\n  mutate(\n    SD = if_else(\n      SW &lt;= MSW,\n      100 * (SW - MSW) / pmax(MSW - SW_min, 1e-9),\n      100 * (SW - MSW) / pmax(SW_max - MSW, 1e-9)\n    )\n  )\n\nYou may have notice that in the previous code, the denominator is ‘protected’ with a tiny value (\\(10^{-9}\\)) to avoid division by zero in flat climates.\nThe last step consists in computing the SMDI in a recursive manner: the SMDI carries persistence via a first-order recursion within each calendar year: \\[\n\\begin{align*}\n\\text{SMDI}_{i,y,1} & = \\frac{\\text{SD}_{i,y,1}}{50},\\\\\n\\text{SMDI}_{i,y,w} & = 0.5,\\text{SMDI}_{i,y,w-1} + \\frac{\\text{SD}_{i,y,w}}{50}\\quad (w\\ge 2).\n\\end{align*}\n\\tag{5.20}\\]\n\n# SMDI computed recursively, by cell.\nsmdi_weekly &lt;- \n  sw_anom |&gt;\n  arrange(cell_id, year, week) |&gt;\n  group_by(cell_id, year) |&gt;\n  group_modify(\\(tb, key) {\n    n &lt;- nrow(tb)\n    smdi &lt;- numeric(n)\n    for (i in seq_len(n)) {\n      if (i == 1 || is.na(smdi[i-1])) {\n        # Initial value\n        smdi[i] &lt;- tb$SD[i] / 50\n      } else {\n        smdi[i] &lt;- 0.5 * smdi[i-1] + tb$SD[i] / 50\n      }\n    }\n    tb$SMDI &lt;- smdi\n    tb\n  }) |&gt;\n  ungroup()\n\nLet us have a look at the values for a cell:\n\nggplot(\n data = smdi_weekly |&gt; filter(cell_id == 19) |&gt; mutate(x = year + week/52),\n mapping = aes(x = x, y = SMDI)\n) +\n  geom_line() +\n  labs(x = NULL)\n\n\n\n\nFigure 5.1: SMDI for a cell in New Zealand. Weekly values range from -4 to +4 indicating very dry to very wet conditions.\n\n\n\n\n\n\n\n\n\n5.3.1 Monthly Aggregation\nThe SMDI values are computed on a weekly basis (52 fixed 7-day blocks per year). To align with standard reporting periods of macroeconomic data, we can aggregate these values to the monthly scale. Because some 7-day weeks span two months, we need to ensure that each month receives only the appropriate share of each week.\nWe proceed as follows:\n\nWe build a monthly calendar of overlaps. For every combination of year and month present in the data, we compute how many days of each fixed 7-day week fall within that month. This gives us a set of weights (\\(w^{(m)}_{w}\\)) that indicate the fraction of the month covered by each week. The weights for a given month always sum to 1.\nWe compute weighted monthly SMDI values. We join the weights from the previous step with the weekly SMDI observations and compute, for each grid cell, year, and month, a weighted mean of weekly SMDI values: \\[\n\\mathrm{SMDI}_{m} = \\frac{\\sum_{w} \\mathrm{SMDI}_{w} \\times w^{(m)}_{w}}{\\sum_{w} w^{(m)}_{w}},\n  \\] where (\\(w^{(m)}_{w}\\)) is the proportion of the month accounted for by week (w).\n\nWe define a function, get_month_week_cal(), to get a monthly calendar of overlaps. Note that it relies on the find_wday() function previously defined.\n\n#' Calendar of fixed 7-day \"weeks\" (1--52) overlapped with a given month\n#'\n#' @description\n#' Builds the 52 fixed 7-day blocks for a given year\n#' (block 1 starts on Jan 1, block k starts on Jan 1 + 7*(k-1) days),\n#' computes each block's overlap (in days) with the specified month,\n#' and returns per-block weights equal to overlap / days-in-month.' \n#' \n#' @param year Year (numeric).\n#' @param month Month (numeric).\n#' \n#' @returns\n#' A tibble with one row per overlapping block and columns:\n#' - `year`: the requested year,\n#' - `month`: the requested month,\n#' - `week`: fixed 7-day block index in 1...52,\n#' - `weight_ndays`: overlap days over days in the month.\n#' \nget_month_week_cal &lt;- function(year, month) {\n  \n  m_start &lt;- lubridate::make_date(year, month, 1)\n  m_end &lt;- (m_start %m+% months(1)) - lubridate::days(1)\n  # days in the month\n  dates &lt;- seq(lubridate::ymd(m_start), lubridate::ymd(m_end), by = \"day\")\n  weeks &lt;- sapply(dates, find_wday)\n  ndays_weeks &lt;- tapply(weeks, weeks, length)\n  \n  dplyr::tibble(\n    week = as.integer(names(ndays_weeks)),\n    nb_days = as.integer(ndays_weeks),\n    weight_ndays = nb_days / sum(nb_days)\n  ) |&gt; \n    dplyr::mutate(\n      year = year,\n      month = month,\n      .before = 1L\n    )\n}\n\nFor example:\n\nget_month_week_cal(2020, 12)\n\n# A tibble: 5 × 5\n   year month  week nb_days weight_ndays\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n1  2020    12    48       1       0.0323\n2  2020    12    49       7       0.226 \n3  2020    12    50       7       0.226 \n4  2020    12    51       7       0.226 \n5  2020    12    52       9       0.290 \n\n\nThe years in the data:\n\nyears &lt;- sort(unique(smdi_weekly$year))\n\nWe get the calendar of months for monthly aggregations by applying the get_month_week_cal() function to all the (year, month) covering the sample period:\n\nmonths_cal &lt;- tidyr::crossing(year = years, month = 1:12) |&gt;\n  purrr::pmap(\\(year, month) get_month_week_cal(year, month)) |&gt; \n  list_rbind()\nmonths_cal\n\n# A tibble: 2,778 × 5\n    year month  week nb_days weight_ndays\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n 1  1980     1     1       7       0.226 \n 2  1980     1     2       7       0.226 \n 3  1980     1     3       7       0.226 \n 4  1980     1     4       7       0.226 \n 5  1980     1     5       3       0.0968\n 6  1980     2     5       4       0.138 \n 7  1980     2     6       7       0.241 \n 8  1980     2     7       7       0.241 \n 9  1980     2     8       7       0.241 \n10  1980     2     9       4       0.138 \n# ℹ 2,768 more rows\n\n\nWe then proceed to the second step, where we compute the monthly aggregated values for SMDI:\n\nsmdi_monthly &lt;- \n  smdi_weekly |&gt; \n  left_join(\n    months_cal, by = c(\"year\", \"week\"),\n    relationship = \"many-to-many\"\n  ) |&gt; \n  group_by(cell_id, year, month) |&gt;\n  summarise(\n    # If any missing values in SMDI will lead to NAs\n    SMDI = weighted.mean(SMDI, w = weight_ndays),\n    .groups = \"drop\"\n  )\n\nLet us have a look at the values for a cell:\n\nggplot(\n  data = smdi_monthly |&gt; mutate(date = year + month / 12) |&gt; \n    filter(cell_id == 19),\n  mapping = aes(x = date, y = SMDI)\n  ) +\n  geom_line() +\n  labs(x = NULL, y = \"SMDI\")\n\n\n\n\nFigure 5.2: Monthly SMDI values for a cell in New Zealand. Weekly values range from -4 to +4 indicating very dry to very wet conditions.\n\n\n\n\n\n\n\n\n\n\n5.3.2 Quarterly Aggregation\nWe may want to aggregate the SMDI values at the quarterly level (Q1–Q4). We follow the same logic as for monthly aggregation, adapting it to quarters:\n\nWe first build a quarterly calendar of overlaps. For each year and quarter, we determine how many days of each 7-day week fall within the quarter. This yields a set of weights (\\(w^{(q)}_w\\)) expressing the fraction of the quarter represented by each week. The weights for a given quarter always sum to 1.\nWe then compute weighted quarterly SMDI values. Using these weights, we aggregate the weekly SMDI values within each quarter by taking a weighted average, where each week’s contribution is proportional to the number of days it contributes to that quarter: \\[\n\\mathrm{SMDI}_{q} = \\frac{\\sum_{w} \\mathrm{SMDI}_{w} \\times w^{(q)}_{w}}{\\sum_{w} w^{(q)}_{w}},\n\\] where (\\(w^{(q)}_{w}\\)) denotes the proportion of the quarter accounted for by week (w).\n\nThe get_quarter_week_cal() function creates the quarterly calendar of overlaps. Again, note that it relies on the find_wday() function previously defined.\n\n#' Calendar weights for fixed 7-day \"weeks\" (1–52) within a given quarter\n#'\n#' @description\n#' For a given `year` and `quarter`, this function computes the number of days\n#' from each fixed 7-day block (weeks 1–52, defined from January 1 in 7-day\n#' increments) that fall within that quarter, along with their normalized\n#' weights. Any remaining days beyond day 364 (e.g., Dec 31 in common years,\n#' or Dec 30–31 in leap years) are assigned to week 52.\n#' \n#' @param year Year (numeric).\n#' @param quarter Quarter (numeric).\n#' \n#' @returns\n#' A tibble with one row per overlapping block and columns:\n#' - `year`: the requested year,\n#' - `quarter`: the requested quarter,\n#' - `week`: fixed 7-day block index in 1...52,\n#' - `weight_ndays`: overlap days over days in the quarter.\n#' \nget_quarter_week_cal &lt;- function(year, quarter) {\n  q_start &lt;- lubridate::make_date(year, (quarter - 1) * 3 + 1, 1)\n  q_end &lt;- (q_start %m+% months(3)) - lubridate::days(1)\n  dates &lt;- seq(q_start, q_end, by = \"day\")\n  weeks &lt;- find_wday(dates)\n  ndays_quarter &lt;- tapply(weeks, weeks, length)\n  \n  dplyr::tibble(\n    week = as.integer(names(ndays_quarter)),\n    nb_days = as.integer(ndays_quarter),\n    weight_ndays = nb_days / sum(nb_days)\n  ) |&gt; \n    dplyr::mutate(\n      year = year,\n      quarter = quarter,\n      .before = 1L\n    )\n}\n\nThe calendar of months for monthly aggregations:\n\nquarters_cal &lt;- tidyr::crossing(year = years, quarter = 1:4) |&gt;\n  purrr::pmap(\\(year, quarter) get_quarter_week_cal(year, quarter)) |&gt; \n  list_rbind()\nquarters_cal\n\n# A tibble: 2,418 × 5\n    year quarter  week nb_days weight_ndays\n   &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n 1  1980       1     1       7       0.0769\n 2  1980       1     2       7       0.0769\n 3  1980       1     3       7       0.0769\n 4  1980       1     4       7       0.0769\n 5  1980       1     5       7       0.0769\n 6  1980       1     6       7       0.0769\n 7  1980       1     7       7       0.0769\n 8  1980       1     8       7       0.0769\n 9  1980       1     9       7       0.0769\n10  1980       1    10       7       0.0769\n# ℹ 2,408 more rows\n\n\nWe then proceed to the second step, where we compute the quarterly aggregated values for SMDI:\n\nsmdi_quarterly &lt;- \n  smdi_weekly |&gt; \n  left_join(\n    quarters_cal, by = c(\"year\", \"week\"),\n    relationship = \"many-to-many\"\n  ) |&gt; \n  group_by(cell_id, year, quarter) |&gt;\n  summarise(\n    # If any missing values in SMDI will lead to NAs\n    SMDI = weighted.mean(SMDI, w = weight_ndays),\n    .groups = \"drop\"\n  )\n\nLet us have a look at the values for a cell:\n\nggplot(\n  data = smdi_quarterly |&gt; mutate(date = year + quarter / 4) |&gt; \n    filter(cell_id == 19),\n  mapping = aes(x = date, y = SMDI)\n  ) +\n  geom_line() +\n  labs(x = NULL, y = \"SMDI\")\n\n\n\n\nFigure 5.3: Quarterly SMDI values for a cell in New Zealand. Quarterly values range from -4 to +4 indicating very dry to very wet conditions.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#sec-spei",
    "href": "data-weather-metrics.html#sec-spei",
    "title": "5  Weather Data: Metrics",
    "section": "5.4 Standardized Precipitation-Evapotranspiration Index (SPEI)",
    "text": "5.4 Standardized Precipitation-Evapotranspiration Index (SPEI)\nWe will compute the Standardized Precipitation-Evapotranspiration Index (SPEI), a versatile drought index that uses climatic data to assess the onset, duration, and severity of drought conditions compared to normal conditions Vicente-Serrano, Beguería, and López-Moreno (2010). The SPEI index relies on the precipitation levels and potential evapotranspiration (estimated in Section 5.1).\nThe SPEI requires:\n\nMonthly precipitation \\(P_m\\) and monthly potential evapotranspiration \\(\\text{PET}_m\\) as inputs,\nWater balance: \\(D_m = P_m - \\text{PET}_m\\),\nA scale \\(k\\), usually in \\(\\{1,2,3,6,12,24\\}\\), which defines the width (in months) for rolling accumulations. This scale thus controls for the magnitude of the memory,\nA calibration window (we will use 1981–2010).\n\n\n\n\nTable 5.1: SPEI values and corresponding climate conditions.\n\n\n\n\n\nSPEI\nClimate condition\n\n\n\n\nSPEI \\(\\geq\\) 2.0\nExtremely wet\n\n\n1.5 \\(\\leq\\) SPEI &lt; 2.0\nSeverely wet\n\n\n1.0 \\(\\leq\\) SPEI &lt; 1.5\nModerately wet\n\n\n0.5 &lt; SPEI &lt; 1.0\nMildly wet\n\n\n−0.5 \\(\\leq\\) SPEI \\(\\leq\\) 0.5\nNormal\n\n\n−1.0 &lt; SPEI &lt; −0.5\nMildly dry\n\n\n−1.5 &lt; SPEI \\(\\leq\\) −1.0\nModerately dry\n\n\n−2.0 &lt; SPEI \\(\\leq\\) −1.5\nSeverely dry\n\n\nSPEI \\(\\leq\\) −2.0\nExtremely dry\n\n\n\n\n\n\nFirst, we need to compute monthly aggregation of required weather variables, at the grid cell level.\n\nweather_monthly &lt;- \n  country_weather_daily |&gt; \n  mutate(ym = floor_date(date, \"month\")) |&gt; \n  group_by(cell_id, ym, year, month) |&gt; \n  summarise(\n    P = sum(precip, na.rm = TRUE), # in mm/month\n    PET = sum(PET_daily, na.rm = TRUE), # in mm/month\n    .groups = \"drop\"\n  ) |&gt; \n  arrange(cell_id, year, month) |&gt; \n  mutate(balance = P - PET)\n  \nweather_monthly\n\n# A tibble: 97,740 × 7\n   cell_id ym          year month     P   PET balance\n     &lt;int&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1       9 1980-01-01  1980     1 213.   77.9   135. \n 2       9 1980-02-01  1980     2 160.   66.3    93.9\n 3       9 1980-03-01  1980     3 152.   56.7    95.8\n 4       9 1980-04-01  1980     4  69.0  41.5    27.6\n 5       9 1980-05-01  1980     5 202.   35.0   168. \n 6       9 1980-06-01  1980     6 165.   25.4   139. \n 7       9 1980-07-01  1980     7 166.   26.5   139. \n 8       9 1980-08-01  1980     8 182.   33.3   149. \n 9       9 1980-09-01  1980     9 183.   41.1   142. \n10       9 1980-10-01  1980    10 129.   55.0    73.5\n# ℹ 97,730 more rows\n\n\n\n#' Compute Standardized Precipitation-Evapotranspiration Index (SPEI)\n#' \n#' @description\n#' Computes the Standardized Precipitation–Evapotranspiration Index (SPEI)\n#' at multiple temporal scales from a monthly climatic water balance time series.\n#'\n#' @param df A data frame containing at least the following columns:\n#' - `ym`: a `Date` or `yearmon`-like variable indicating the month,\n#' - `year`: numeric year of each observation,\n#' - `balance`: the monthly climatic water balance, typically  \n#'   P - PET (precipitation minus potential evapotranspiration), in millimetres (mm).\n#' @param scales A numeric vector giving the accumulation periods (in months) \n#' for which the SPEI is computed. Default to `c(1, 3, 6, 12)`.\n#' @param ref_start Optional numeric vector of length 2 giving the start year \n#' and month (e.g., `c(1981, 1)`) of the reference calibration period.  \n#' If `NULL`, the full series is used.\n#' @param ref_end Optional numeric vector of length 2 giving the end year \n#' and month (e.g., `c(2010, 12)`) of the reference calibration period.  \n#' If `NULL`, the full series is used.\n#'\n#' @details\n#' For each accumulation period `k` in `scales`, the function:\n#' 1. Builds a monthly time series (`ts`) of the climatic water balance with \n#'    frequency 12.  \n#' 2. Fits the [SPEI::spei()] model for the given scale `k`.  \n#' 3. Extracts the standardized fitted values.  \n#'\n#' The output includes one column per computed scale, named `SPEI_k`,  \n#' where `k` is the accumulation period (e.g., `SPEI_3` for 3-month SPEI).  \n#'\n#' @returns\n#' A tibble with one row per input observation and the following columns:\n#' - `ym`: corresponding month.  \n#' - `SPEI_k`: standardized SPEI values for each scale `k`.  \n#' \ncompute_spei &lt;- function(df, \n                         scales = c(1, 3, 6, 12),\n                         ref_start = NULL, \n                         ref_end = NULL) {\n  \n  # Build a ts object for water balance with frequency = 12\n  start_year  &lt;- min(df$year, na.rm = TRUE)\n  start_month &lt;- month(min(df$ym, na.rm = TRUE))\n  bal_ts &lt;- ts(df$balance, start = c(start_year, start_month), frequency = 12)\n  \n  # Run SPEI for each scale\n  spei_list &lt;- lapply(scales, function(k) {\n    if (is.null(ref_start) || is.null(ref_end)) {\n      fit &lt;- SPEI::spei(bal_ts, scale = k, verbose = FALSE)\n    } else {\n      fit &lt;- SPEI::spei(\n        bal_ts, scale = k, \n        ref.start = ref_start, ref.end = ref_end,\n        verbose = FALSE\n      )\n    }\n    # Extract the fitted standardized values as a numeric vector\n    spei_values &lt;- as.numeric(fit$fitted)\n    tibble(\n      !!paste(\"SPEI\", k, sep = \"_\") := spei_values\n    )\n  })\n  bind_cols(spei_list) |&gt; \n    mutate(ym = df$ym, .before = 1L)\n}\n\nThe reference period will be Jan. 1981 to Dec. 200.\n\nref_start &lt;- c(1981, 1)\nref_end &lt;- c(2010, 12)\n\nFor example, the SPEI for cell 19 gives:\n\ncompute_spei(\n  df = weather_monthly |&gt; filter(cell_id == 19),\n  ref_start = ref_start,\n  ref_end = ref_end\n)\n\n# A tibble: 540 × 5\n   ym          SPEI_1  SPEI_3  SPEI_6 SPEI_12\n   &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 1980-01-01  0.457  NA      NA           NA\n 2 1980-02-01 -0.0726 NA      NA           NA\n 3 1980-03-01 -0.0889  0.0265 NA           NA\n 4 1980-04-01 -1.74   -0.917  NA           NA\n 5 1980-05-01  0.271  -0.820  NA           NA\n 6 1980-06-01 -0.145  -0.836  -0.582       NA\n 7 1980-07-01  0.351   0.142  -0.567       NA\n 8 1980-08-01  0.791   0.333  -0.351       NA\n 9 1980-09-01  0.310   0.579  -0.227       NA\n10 1980-10-01 -0.810   0.117   0.0809      NA\n# ℹ 530 more rows\n\n\n\nCheck whether there are infinite values in the resulting values. If that is the case, consider a wider reference period.\n\nThe compute_spei() function is then applied to each cell.\n\nspei_by_cell &lt;- \n  weather_monthly |&gt; \n  group_by(cell_id) |&gt; \n  group_modify(\n    ~ compute_spei(\n      .x, scales = c(1, 3, 6, 12),\n      ref_start = ref_start,\n      ref_end = ref_end\n    )\n  ) |&gt; \n  ungroup()\n\nTo finish, we recreate the year and month columns and remove the ym column:\n\nspei_by_cell &lt;- \n  spei_by_cell |&gt; \n  mutate(\n    year = year(ym),\n    month = month(ym),\n    .before = \"ym\"\n  ) |&gt; \n  select(-ym)\n\n\n\n\n\n\n\nA few infinite values\n\n\n\n\n\nThere are a few infinite values. If we widen the reference period, the number of inifnite values will decrease.\n\nspei_by_cell |&gt; \n  filter(is.infinite(SPEI_1))\n\n# A tibble: 14 × 7\n   cell_id  year month SPEI_1  SPEI_3  SPEI_6 SPEI_12\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      27  2018     7   -Inf -3.26   -2.66   -1.26  \n 2      90  2011     8   -Inf -2.39   -0.894  -0.536 \n 3      91  2011     8   -Inf -2.86   -0.534   0.104 \n 4      97  2018    11    Inf  1.25   -0.0804  0.198 \n 5      99  2011     8   -Inf -2.30   -0.613  -0.276 \n 6     100  2011     8   -Inf -3.19   -0.484   0.0936\n 7     101  2011     8   -Inf -2.14   -0.227  -0.0732\n 8     107  1993     8   -Inf -0.490   0.326  -0.358 \n 9     107  2011     8   -Inf -3.28   -0.501  -0.245 \n10     108  2011     8   -Inf -2.00   -0.290  -0.393 \n11     116  2011     8   -Inf -1.86   -0.371  -0.613 \n12     191  2021     4   -Inf -1.14   -1.55   -1.61  \n13     201  2016     5    Inf  1.25   -0.0123  0.583 \n14     237  1984    10   -Inf  0.0822  0.0930  0.275 \n\nspei_by_cell |&gt; \n  filter(is.infinite(SPEI_3))\n\n# A tibble: 30 × 7\n   cell_id  year month SPEI_1 SPEI_3 SPEI_6 SPEI_12\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1      32  2022     7 -1.68    -Inf -3.87   -1.37 \n 2      54  2024     7 -1.04    -Inf -0.785   0.145\n 3      64  2024     7 -1.16    -Inf -1.13   -0.255\n 4      73  2024     7 -1.23    -Inf -1.73   -1.30 \n 5      74  2017    12 -1.71    -Inf -1.49   -0.807\n 6      74  2024     7 -1.02    -Inf -1.36   -0.764\n 7     100  2011    10 -0.539   -Inf -1.25   -0.515\n 8     101  2011    10 -0.826   -Inf -1.25   -0.666\n 9     107  2011    10 -0.500   -Inf -1.46   -0.694\n10     108  1997    10 -1.28    -Inf -3.00   -1.66 \n# ℹ 20 more rows\n\nspei_by_cell |&gt; \n  filter(is.infinite(SPEI_6))\n\n# A tibble: 34 × 7\n   cell_id  year month SPEI_1 SPEI_3 SPEI_6 SPEI_12\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1      27  2018     8 -2.68   -6.03   -Inf  -2.12 \n 2      27  2019     1 -3.29   -4.12   -Inf  -4.01 \n 3      32  2022     6 -1.36   -3.82   -Inf  -0.802\n 4      32  2022     8 -0.613  -2.23   -Inf  -1.73 \n 5      32  2022     9 -1.81   -2.11   -Inf  -2.42 \n 6      32  2022    10 -2.30   -2.34   -Inf  -2.94 \n 7     164  2023     4  2.04    4.23    Inf Inf    \n 8     164  2023     6  1.56  Inf       Inf Inf    \n 9     165  2023     4  1.65    3.10    Inf   3.25 \n10     180  2023     4  2.18    2.68    Inf Inf    \n# ℹ 24 more rows\n\nspei_by_cell |&gt; \n  filter(is.infinite(SPEI_12))\n\n# A tibble: 117 × 7\n   cell_id  year month SPEI_1 SPEI_3 SPEI_6 SPEI_12\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1      32  2022    12 -2.49   -3.32  -3.30    -Inf\n 2      32  2023     4 -0.848  -1.29  -3.03    -Inf\n 3     164  2023     4  2.04    4.23 Inf        Inf\n 4     164  2023     5  2.61    2.22   5.07     Inf\n 5     164  2023     6  1.56  Inf    Inf        Inf\n 6     164  2023     7 -0.441   2.40   3.75     Inf\n 7     179  2023     1  3.02    3.35   5.10     Inf\n 8     179  2023     2  2.16    2.68   3.71     Inf\n 9     179  2023     3 -0.136   2.65   3.88     Inf\n10     179  2023     4  1.21    1.85   4.81     Inf\n# ℹ 107 more rows\n\n\n\n\n\nHere, we replace the few infinite values by NAs:\n\nspei_by_cell &lt;- \n  spei_by_cell |&gt; \n  mutate(\n    across(starts_with(\"SPEI_\"), ~if_else(is.infinite(.x), NA_real_, .x))\n  )\n\nLet us have a look at the values for a cell:\n\n\nCodes to create the Figure.\nspei_bands &lt;- tibble(\n  ymin = c(-Inf, -2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0),\n  ymax = c(-2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0,  Inf),\n  label = c(\n    \"Extremely dry\", \"Severely dry\", \"Moderately dry\", \"Mildly dry\",\n    \"Normal\",\n    \"Mildly wet\", \"Moderately wet\", \"Severely wet\", \"Extremely wet\"\n  ),\n  fill = c(\n    \"#67001f\", \"#b2182b\", \"#d6604d\", \"#f4a582\", # dry shades\n    \"#f7f7f7\",\n    \"#92c5de\", \"#4393c3\", \"#2166ac\", \"#053061\" # wet shades\n  )\n) |&gt; \n  mutate(label = factor(label, levels = label))\n\nggplot(\n  data = spei_by_cell |&gt; \n    filter(cell_id == 19) |&gt; \n    mutate(x = year + month/12) |&gt; \n    pivot_longer(\n      cols = starts_with(\"SPEI_\"), values_to = \"SPEI\", names_to = \"scale\"\n    ) |&gt; \n    mutate(\n      scale = str_remove(scale, \"SPEI_\"),\n      scale = factor(\n        scale, levels = c(1, 3, 6, 12),\n        labels = paste0(\"k=\", c(1, 3, 6, 12))\n      )),\n  mapping = aes(x = x, y = SPEI)\n) +\n  geom_rect(\n    data = spei_bands,\n    aes(\n      xmin = -Inf, xmax = Inf,\n      ymin = ymin, ymax = ymax,\n      fill = label\n    ),\n    inherit.aes = FALSE,\n    alpha = 0.25\n  ) +\n  geom_line() +\n  scale_fill_manual(\n    name = \"Climate condition\",\n    values = setNames(spei_bands$fill, spei_bands$label)\n  ) +\n  labs(x = NULL) +\n  facet_wrap(~ scale) +\n  theme_minimal()\n\n\n\n\n\nFigure 5.4: SPEI for a cell in New Zealand, depending on the accumulation periods (\\(k\\)). Shaded zones represent climatic conditions by SPEI range\n\n\n\n\n\n\n\n\nFor quarterly aggregation, we simply compute a mean of the SPEI values within each quater in each cell, for each year.\n\nspei_quarterly_mean &lt;-\n  spei_by_cell |&gt;\n  mutate(\n    month_date = make_date(year, month, 1),\n    q_start = floor_date(month_date, \"quarter\"),\n    quarter = quarter(q_start)\n  ) |&gt;\n  group_by(cell_id, year, quarter) |&gt;\n  summarise(\n    across(starts_with(\"SPEI_\"), ~ mean(.x, na.rm = TRUE)\n    ),\n    .groups = \"drop\"\n  )\n\n\n\nCodes to create the Figure.\nggplot(\n  data = spei_quarterly_mean |&gt; \n    filter(cell_id == 19) |&gt; \n    mutate(x = year + quarter/4) |&gt; \n    pivot_longer(\n      cols = starts_with(\"SPEI_\"), values_to = \"SPEI\", names_to = \"scale\"\n    ) |&gt; \n    mutate(\n      scale = str_remove(scale, \"SPEI_\"),\n      scale = factor(\n        scale, levels = c(1, 3, 6, 12),\n        labels = paste0(\"k=\", c(1, 3, 6, 12))\n      )),\n  mapping = aes(x = x, y = SPEI)\n) +\n  geom_rect(\n    data = spei_bands,\n    aes(\n      xmin = -Inf, xmax = Inf,\n      ymin = ymin, ymax = ymax,\n      fill = label\n    ),\n    inherit.aes = FALSE,\n    alpha = 0.25\n  ) +\n  geom_line() +\n  scale_fill_manual(\n    name = \"Climate condition\",\n    values = setNames(spei_bands$fill, spei_bands$label)\n  ) +\n  labs(x = NULL) +\n  facet_wrap(~ scale) +\n  theme_minimal()\n\n\n\n\n\nFigure 5.5: Quarterly mean of SPEI for a cell in New Zealand, depending on the accumulation periods (\\(k\\)). Shaded zones represent climatic conditions by SPEI range",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#temporal-aggregation",
    "href": "data-weather-metrics.html#temporal-aggregation",
    "title": "5  Weather Data: Metrics",
    "section": "5.5 Temporal Aggregation",
    "text": "5.5 Temporal Aggregation\nWe now need to gather the different datasets together. As for the SMDI, we will consider a monthly and a quarterly aggregation.\n\n5.5.1 Monthly Data\nFirst, we compute the total amount of precipitation, the average minimum, maximum, and mean temperatures, at the cell-level, on a monthly basis.\n\ncountry_weather_monthly &lt;- \n  country_weather_daily |&gt; \n  group_by(\n    cell_id, longitude, latitude, country_code, \n    year, month, month_name\n  ) |&gt; \n  summarise(\n    precip = sum(precip, na.rm = TRUE), # mm/month\n    temp_min = mean(temp_min, na.rm = TRUE), # deg C\n    temp_max = mean(temp_max, na.rm = TRUE), # deg C\n    temp_mean = mean(temp_mean,   na.rm = TRUE), # deg C\n    .groups = \"drop\"\n  )\n\nWe add the SMDI and the SPEI values to that dataset:\n\ncountry_weather_monthly &lt;- \n  country_weather_monthly |&gt; \n  left_join(\n    smdi_monthly,\n    by = c(\"cell_id\", \"year\", \"month\")\n  ) |&gt; \n  left_join(\n    spei_by_cell,\n    by = c(\"cell_id\", \"year\", \"month\")\n  )\n\n\n\n5.5.2 Quarterly Data\nWe compute the total amount of precipitation, the average minimum, maximum, and mean temperatures, at the cell-level, on a quarterly basis.\n\ncountry_weather_quarterly &lt;- \n  country_weather_daily |&gt; \n  mutate(\n    q_start = floor_date(date, \"quarter\"),\n    quarter = quarter(q_start),\n    .after = year\n    ) |&gt; \n  select(-q_start) |&gt; \n  group_by(\n    cell_id, longitude, latitude, country_code, \n    year, quarter\n  ) |&gt; \n  summarise(\n    precip = sum(precip, na.rm = TRUE), # mm/month\n    temp_min = mean(temp_min, na.rm = TRUE), # deg C\n    temp_max = mean(temp_max, na.rm = TRUE), # deg C\n    temp_mean = mean(temp_mean,   na.rm = TRUE), # deg C\n    .groups = \"drop\"\n  )\n\nWe add the SMDI and the SPEI values to that dataset:\n\ncountry_weather_quarterly &lt;- \n  country_weather_quarterly |&gt; \n  left_join(\n    smdi_quarterly,\n    by = c(\"cell_id\", \"year\", \"quarter\")\n  ) |&gt; \n  left_join(\n    spei_quarterly_mean,\n    by = c(\"cell_id\", \"year\", \"quarter\")\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#spatial-aggregation",
    "href": "data-weather-metrics.html#spatial-aggregation",
    "title": "5  Weather Data: Metrics",
    "section": "5.6 Spatial Aggregation",
    "text": "5.6 Spatial Aggregation\nWe now have monthly and quarterly weather observations computed at the grid cell level. Our next step is to aggregate these data spatially to match the level of available macroeconomic indicators, which are typically reported at the regional or national scale.\n\n5.6.1 Regional Level\nFor the regional aggregation, we compute a weighted average of the cell-level values, using as weights the proportion of each cell’s area lying within the corresponding region.\nWe need the maps of the country of interest. We use the st_shift_longitude() here because New Zealand’s boundaries have values straddling 180 degrees.\n\nmaps_level_0_NZ &lt;- st_shift_longitude(maps_level_0$NZL)\nmaps_level_1_NZ &lt;- st_shift_longitude(maps_level_1$NZL)\n\nThe coordinates of the cells were obtained in Chapter 4.\n\nload(\"../data/Weather/CPC/NZL_cells.RData\")\ncells_sf &lt;- st_as_sf(cells)\n\nWe compute the intersection of each cell with each region:\n\ninter &lt;- st_intersection(\n  maps_level_1_NZ |&gt; select(GID_1, region = NAME_1), \n  cells_sf |&gt; select(cell_id, longitude, latitude)\n)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nThe proportion of each cell’s area lying within a region is used as weight for the regional aggregation. Let us compute the weights.\n\ninter_tbl &lt;- inter |&gt;\n  mutate(\n    area_km2 = as.numeric(units::set_units(st_area(geometry), km^2))\n  ) |&gt;\n  st_drop_geometry() |&gt;\n  arrange(region, cell_id) |&gt; \n  as_tibble() |&gt; \n  group_by(region) |&gt; \n  mutate(w_cell_region = area_km2 / sum(area_km2)) |&gt; \n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nNote that a cell can lie in multiple regions. The weight may probably differ.\n\ninter_tbl |&gt; count(cell_id) |&gt; arrange(desc(n))\n\n# A tibble: 181 × 2\n   cell_id     n\n     &lt;int&gt; &lt;int&gt;\n 1      43     3\n 2      72     3\n 3      73     3\n 4     116     3\n 5     127     3\n 6     142     3\n 7     190     3\n 8     227     3\n 9     235     3\n10     241     3\n# ℹ 171 more rows\n\n# Example of a cell in multiple regions:\ninter_tbl |&gt; filter(\n  cell_id == inter_tbl |&gt; count(cell_id) |&gt; \n    arrange(desc(n)) |&gt; pull(\"cell_id\") |&gt; pluck(1)\n)\n\n# A tibble: 3 × 7\n  GID_1    region     cell_id longitude latitude area_km2 w_cell_region\n  &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 NZL.12_1 Otago           43      168.    -44.2     11.9      0.000410\n2 NZL.14_1 Southland       43      168.    -44.2    653.       0.0207  \n3 NZL.19_1 West Coast      43      168.    -44.2    879.       0.0377  \n\n\n\n\nWe will compute the weighted mean of the following weather variables (this needs to be updated if more weather metrics are computed):\n\n# The names of the SPEI variables\nspei_names &lt;- colnames(country_weather_monthly)[str_detect(colnames(country_weather_monthly), \"^SPEI_\")]\n\n# All the weather variables previously defined:\nweather_vars &lt;- c(\n  \"precip\", \n  \"temp_min\", \"temp_max\", \"temp_mean\",\n  \"SMDI\", \n  spei_names\n)\n\n\n5.6.1.1 Monthly Data\nWe merge the regional weights with the monthly weather data available at the cell level. During this join, cells intersecting multiple regions appear multiple times in the resulting dataset (once per overlapping region) with their contribution differentiated by the corresponding intersection weight.\n\nregion_weather_monthly &lt;- \n  country_weather_monthly |&gt; \n  left_join(\n    inter_tbl |&gt; select(GID_1, region, cell_id, w_cell_region),\n    by = c(\"cell_id\"),\n    relationship =\"many-to-many\" # a cell can be in multiple regions\n  ) |&gt; \n  group_by(GID_1, region, country_code, year, month, month_name) |&gt; \n  summarise(\n    across(\n      all_of(weather_vars),\n      ~ weighted.mean(.x, w = w_cell_region)\n    ),\n    .groups = \"drop\"\n  )\n\n\n\n5.6.1.2 Quarterly Data\nWe perform the same operation with the quarterly data.\n\nregion_weather_quarterly &lt;- \n  country_weather_quarterly |&gt; \n  left_join(\n    inter_tbl |&gt; select(GID_1, region, cell_id, w_cell_region),\n    by = c(\"cell_id\"),\n    relationship =\"many-to-many\" # a cell can be in multiple regions\n  ) |&gt; \n  group_by(GID_1, region, country_code, year, quarter) |&gt; \n  summarise(\n    across(\n      all_of(weather_vars),\n      ~ weighted.mean(.x, w = w_cell_region)\n    ),\n    .groups = \"drop\"\n  )\n\n\n\n5.6.1.3 Maps for Monthly Data\nWe visualize the aggregated weather by plotting choropleth maps for year 2013. For each month (12 panels), we produce:\n\nMaps at the regional level using area-weighted regional means,\nMaps at the cell level using cell means.\n\nTo be able to compare the values across months, we construct one map layer per month and then bind them into a single sf object for faceting.\n\n# For region layers\ndata_map &lt;- vector(mode = \"list\", length = 12)\nfor (i in 1:12) {\n  data_map[[i]] &lt;- \n    maps_level_1_NZ |&gt; \n    inner_join(\n      region_weather_monthly |&gt; filter(year == 2013, month == !!i),\n      by = c(\"GID_1\", \"NAME_1\" = \"region\")\n    )\n}\ndata_map &lt;- list_rbind(data_map) |&gt; st_as_sf()\n\n# For cell layers\ndata_map_cells &lt;- vector(mode = \"list\", length = 12)\nfor (i in 1:12) {\n  data_map_cells[[i]] &lt;- \n    cells_sf |&gt; \n    inner_join(\n      country_weather_monthly |&gt; filter(year == 2013, month == !!i),\n      by = c(\"cell_id\", \"longitude\", \"latitude\")\n    )\n}\ndata_map_cells &lt;- list_rbind(data_map_cells) |&gt; st_as_sf()\n\nTo ensure visual comparability across maps, we use a common color scale for all temperature variables. The code below computes the global minimum and maximum values across both minimum and maximum temperatures and defines a shared color palette that will be applied to every temperature map.\n\nvmin &lt;- min(data_map$temp_min, data_map$temp_max, na.rm = TRUE)\nvmax &lt;- max(data_map$temp_min, data_map$temp_max, na.rm = TRUE)\ncols &lt;- viridis::magma(6) |&gt; rev()\ncols &lt;- cols[-length(cols)]\n\nFor the Soil Moisture Deficit Index, we define a diverging palette, with warm colors indicating dry conditions and cool colors indicating wet conditions. Neutral shades around white correspond to near-normal moisture.\n\ncols_smdi &lt;- c(\n  \"#67001f\", \"#b2182b\", \"#d6604d\", \"#f4a582\", # dry shades\n  \"#f7f7f7\",\n  \"#92c5de\", \"#4393c3\", \"#2166ac\", \"#053061\" # wet shades\n)\n\nFor the maps displaying cell-level data, we ensure spatial alignment with the regional maps by applying the same bounding box.\n\nbb &lt;- sf::st_bbox(data_map)\n\n\nPrecipitationTemp MeanTemp MinTemp MaxSMDISPEI 3\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = precip)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradient2(\n    \"Precip. (mm)\", low = \"red\", high = \"#005A8B\", midpoint = 0, mid = \"white\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.6: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = precip)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradient2(\n    \"Precip. (mm)\", low = \"red\", high = \"#005A8B\", midpoint = 0, mid = \"white\"\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.7: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = temp_mean)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Mean (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.8: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = temp_mean)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Mean (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.9: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = temp_min)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Min (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.10: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = temp_min)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Min (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.11: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = temp_max)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Max (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.12: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = temp_max)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"Temperature Max (°C)\",\n    colours = cols,\n    limits = c(vmin, vmax),\n    oob = scales::squish\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.13: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = SMDI)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"SMDI\",\n    colours = cols_smdi,\n    limits = c(-4, 4),\n    oob = scales::squish\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.14: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = SMDI)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"SMDI\",\n    colours = cols_smdi,\n    limits = c(-4, 4),\n    oob = scales::squish\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.15: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map,\n    mapping = aes(fill = SPEI_3)\n  ) +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"SPEI-3\",\n    colours = cols_smdi,\n    limits = c(-4, 4),\n    oob = scales::squish\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.16: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nggplot() +\n  geom_sf(\n    data = data_map_cells,\n    mapping = aes(fill = SPEI_3)\n  ) +\n  geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n  facet_wrap(~ month_name, ncol = 6) +\n  theme_map_paper() +\n  scale_fill_gradientn(\n    name = \"SPEI-3\",\n    colours = cols_smdi,\n    limits = c(-4, 4),\n    oob = scales::squish\n  ) +\n  coord_sf(\n    xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n    ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n    expand = FALSE\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 5.17: Monthly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.4 Maps for Quarterly Data\nWe can also do the same with the quarterly aggregated values.\n\n# For region layers\ndata_map_q &lt;- vector(mode = \"list\", length = 4)\nfor (i in 1:4) {\n  data_map_q[[i]] &lt;- \n    maps_level_1_NZ |&gt; \n    inner_join(\n      region_weather_quarterly |&gt; filter(year == 2013, quarter == !!i),\n      by = c(\"GID_1\", \"NAME_1\" = \"region\")\n    )\n}\ndata_map_q &lt;- list_rbind(data_map_q) |&gt; st_as_sf()\n\n# For cell layers\ndata_map_q_cells &lt;- vector(mode = \"list\", length = 4)\nfor (i in 1:4) {\n  data_map_q_cells[[i]] &lt;- \n    cells_sf |&gt; \n    inner_join(\n      country_weather_quarterly |&gt; filter(year == 2013, quarter == !!i),\n      by = c(\"cell_id\", \"longitude\", \"latitude\")\n    )\n}\ndata_map_q_cells &lt;- list_rbind(data_map_q_cells) |&gt; st_as_sf()\n\nThis time, we define a small function to create the plots, plot_maps_q(). There is no real point to do so, we just want to provide a second alternative.\n\n\nThe plot_maps_q() function.\n#' Maps for each month for a variable showing temperatures\nplot_maps_q &lt;- function(var_temp_name, \n                        legend_title,\n                        geo = c(\"cell\", \"region\"),\n                        legend_type = c(\"precip\", \"temp\", \"drought\")) {\n  geo &lt;- match.arg(geo)\n  legend_type &lt;- match.arg(legend_type)\n  df_plot &lt;- if (geo == \"cell\") data_map_q_cells else data_map_q\n  \n  p &lt;- ggplot() +\n    geom_sf(\n      data = df_plot,\n      mapping = aes(fill = !!sym(var_temp_name))\n    ) +\n    facet_wrap(~ quarter, ncol = 4) +\n    theme_map_paper() +\n    theme(legend.position = \"bottom\", legend.key.height = unit(1, \"line\"))\n  \n  p &lt;- switch(\n    legend_type,\n    \"temp\" = p + scale_fill_gradientn(\n      name   = legend_title,\n      colours = cols,\n      limits  = c(vmin, vmax),\n      oob     = scales::squish\n    ),\n    \"precip\" = p + scale_fill_gradient2(\n      name = legend_title,\n      low = \"red\", high = \"#005A8B\", midpoint = 0, mid = \"white\"\n    ),\n    \"drought\" = p + scale_fill_gradientn(\n      name   = legend_title,\n      colours = cols_smdi,\n      limits  = c(-4, 4),\n      oob     = scales::squish\n    )\n  )\n  \n  if (geo == \"cell\") {\n    p &lt;- p +\n      geom_sf(data = maps_level_1_NZ, fill = \"NA\") +\n      coord_sf(\n        xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n        ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n        expand = FALSE\n      )\n  }\n  \n  p\n}\n\n\nHaving this function would allow to quickly loop over the different variables and plot maps. We do not do it in this notebook.\n\n# This chunk is not evaluated\n\nconfigs_maps &lt;- tribble(\n  ~variable, ~title, ~type,\n  \"precip\", \"Precip. (mm)\", \"precip\",\n  \"temp_mean\", \"Temperature Mean (°C)\", \"temp\",\n  \"temp_min\", \"Temperature Min (°C)\", \"temp\",\n  \"temp_max\", \"Temperature Max (°C)\", \"temp\",\n  \"SMDI_month\", \"SMDI\", \"drought\",\n  \"SPEI_3\", \"SPEI-3\", \"drought\",\n) |&gt; \n  expand_grid(geo = c(\"region\", \"cell\"))\n\n\nfor (i in 1:nrow(configs_maps)) {\n  plot_maps_q(\n    var_temp_name = configs_maps$variable[i], \n    legend_title = configs_maps$title[i], \n    geo = configs_maps$geo[i], \n    legend_type = configs_maps$type[i]\n  )\n}\n\n\nPrecipitationTemp MeanTemp MinTemp MaxSMDISPEI 3\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"precip\",\n    legend_title = \"Precip. (mm)\",\n    geo = \"region\", \n    legend_type = \"precip\"\n)\n\n\n\n\n\nFigure 5.18: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"precip\",\n    legend_title = \"Precip. (mm)\",\n    geo = \"cell\", \n    legend_type = \"precip\"\n)\n\n\n\n\n\nFigure 5.19: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_mean\",\n    legend_title = \"Temp Mean (°C)\",\n    geo = \"region\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.20: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_mean\",\n    legend_title = \"Temp Mean (°C)\",\n    geo = \"cell\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.21: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_min\",\n    legend_title = \"Temp Min (°C)\",\n    geo = \"region\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.22: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_min\",\n    legend_title = \"Temp Min (°C)\",\n    geo = \"cell\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.23: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_max\",\n    legend_title = \"Temp Max (°C)\",\n    geo = \"region\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.24: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"temp_max\",\n    legend_title = \"Temp Max (°C)\",\n    geo = \"cell\", \n    legend_type = \"temp\"\n)\n\n\n\n\n\nFigure 5.25: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"SMDI\",\n    legend_title = \"SMDI\",\n    geo = \"region\", \n    legend_type = \"drought\"\n)\n\n\n\n\n\nFigure 5.26: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"SMDI\",\n    legend_title = \"SMDI\",\n    geo = \"cell\", \n    legend_type = \"drought\"\n)\n\n\n\n\n\nFigure 5.27: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegionsCells\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"SPEI_3\",\n    legend_title = \"SPEI-3\",\n    geo = \"region\", \n    legend_type = \"drought\"\n)\n\n\n\n\n\nFigure 5.28: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the Figure\nplot_maps_q(\n    var_temp_name = \"SPEI_3\",\n    legend_title = \"SPEI-3\",\n    geo = \"cell\", \n    legend_type = \"drought\"\n)\n\n\n\n\n\nFigure 5.29: Quarterly averages in 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 National Level\nIn our context, we assume that weather shocks primarily affect the agricultural sector, before propagating to the broader economy. Consequently, when aggregating weather variables at the national level, we apply weights that capture the relative agricultural intensity of each region. By doing so, areas where agriculture plays a larger economic role contribute more to the national weather indices.\n\n\n\n\n\n\nImportant\n\n\n\nIf one wishes to perform the aggregation in a different context, these weights should be adapted accordingly. Setting all weights to 1 would correspond to a simple unweighted average, implicitly assuming that each grid cell contributes equally to national exposure. Such an assumption may overlook important spatial heterogeneity in sectoral relevance.\n\n\n\n5.6.2.1 Regional Agricultural Intensity\nTo aggregate grid-cell weather measures to the national level in a way that reflects sectoral exposure, we build regional agricultural weights from regional accounts.\nStatistics New Zealand’s regional GDP by industry (series RNA001AA) is downloaded from Infoshare (CSV export; the Excel export uses locale-dependent thousand separators that break on our French setup). Our extract covers 2000–2023 by region. We target a working sample of 1987–2024, so we must extrapolate outside the extract window: 1987–1999 (backward) and 2024 (forward).\n\n5.6.2.1.1 Extrapolation strategy\nLet \\(Y^{(a)}_{r,t}\\) denote agricultural GDP for region \\(r\\) and year \\(t\\).\nWe compute the inverse growth rate: \\[\n\\Delta_{r,t}^{-1} = \\frac{Y^{(a)}_{r,t}}{Y^{(a)}_{r,t+1}},\n\\tag{5.21}\\] defined wherever both years \\(t\\) and \\(t+1\\) are observed.\nOver a reference window of width equal to the number of observed years minus 1 (because the computation of \\(\\Delta_{r,t}^{-1}\\) make us lose an observation), hence corresponding to the observed period, we estimate a linear trend in \\(\\Delta_{r,t}^{-1}\\): \\[\n\\Delta_{r,t}^{-1} = \\alpha_r + \\beta_r \\cdot t + \\varepsilon_{r,t},\n\\tag{5.22}\\] and retain \\(\\hat\\beta_r\\).\nFor backward fills (years preceding the extract), we form a geometrically weighted average of the windowed inverse growth rates and blend it 50/50 with the trend-implied growth: \\[\n\\widehat{\\gamma}_{r,t}^{\\text{back}}\n  = \\tfrac{1}{2}\\Big(\\sum_{j=1}^{W} w_j  \\Delta_{r,t+j}^{-1}\\Big)\n  + \\tfrac{1}{2}\\big(1 + \\hat\\beta_r\\big),\n\\tag{5.23}\\] where \\(w_j \\propto \\text{reason}^{j-1}\\) with \\(\\text{reason}=1/1.35\\) and \\(\\sum_j w_j=1\\).\nWe then impute \\[\nY^{(a)}_{r,t} = \\widehat{\\gamma}_{r,t}^{\\text{back}} \\times \\Delta_{r,t+1}^{-1}.\n\\tag{5.24}\\]\nFor forward fills (years after the extract), we apply the trend growth: \\[\n  \\Delta_{r,t}^{-1} = \\Delta_{r,t-1}^{-1} \\times \\big(1+\\hat\\beta_r\\big).\n\\tag{5.25}\\]\nThis approach borrows strength from the local history (via the weighted window) while guarding against short-run noise (via the trend).\n\n\n5.6.2.1.2 Weight construction\nLet \\(Y^{(a)}_{t}\\) be the national agricultural GDP. For each region \\(r\\) and year \\(t\\), we define the agricultural intensity weight as follows:: \\[\nw_{r,t} = \\frac{Y^{(a)}_{r,t}}{Y^{(a)}_{t}}.\n\\tag{5.26}\\] By construction, \\(\\sum_{r} w_{r,t} = 1\\) for each year \\(t\\) (up to rounding). These weights can then be used to aggregate weather variables from regions to the national level..\n\n\n\n\n\n\nImportant\n\n\n\nOther methods to estimate the regional agricultural intensity can be used. We just present this one.\n\n\n\n\n5.6.2.1.3 Implementation in R\nLet us compute these weights in R now. We first load the dataset:\n\n# Source: https://infoshare.stats.govt.nz\n# Ref: RNA001AA\nagri &lt;- read_csv(\n  \"../data/Agriculture/RNA434201_20251025_084751_31.csv\", \n  skip = 4, \n  col_names = colnames(\n    read_csv(\"../data/Agriculture/RNA434201_20251025_084751_31.csv\", \n             skip = 2, n_max = 1, col_types = cols())),\n  n_max = 24\n) |&gt; \n  rename(year = `...1`)\n\nNew names:\nRows: 24 Columns: 19\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(19): ...1, Northland, Auckland, Waikato, Bay of Plenty, Gisborne, Hawke...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n  # filter(year &lt;= 2012) # same as in the paper\nagri\n\n# A tibble: 24 × 19\n    year Northland Auckland Waikato `Bay of Plenty` Gisborne `Hawke's Bay`\n   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1  2000       327      182     998             412       99           357\n 2  2001       459      216    1482             525      118           489\n 3  2002       510      235    1681             597      140           530\n 4  2003       334      170    1083             463      117           472\n 5  2004       372      175    1185             521      123           498\n 6  2005       356      165    1201             500      114           479\n 7  2006       276      147    1014             447      123           407\n 8  2007       336      165    1207             503      137           438\n 9  2008       512      246    1951             718      134           410\n10  2009       368      178    1360             535      138           479\n# ℹ 14 more rows\n# ℹ 12 more variables: Taranaki &lt;dbl&gt;, `Manawatu-Whanganui` &lt;dbl&gt;,\n#   Wellington &lt;dbl&gt;, `West Coast` &lt;dbl&gt;, Canterbury &lt;dbl&gt;, Otago &lt;dbl&gt;,\n#   Southland &lt;dbl&gt;, Marlborough &lt;dbl&gt;, `Tasman/Nelson` &lt;dbl&gt;,\n#   `Total North Island` &lt;dbl&gt;, `Total South Island` &lt;dbl&gt;, `New Zealand` &lt;dbl&gt;\n\n\nEach column in the table gives the agricultural GDP of a region. For convenience, we pivot the table so that the GDP values are in a single column (that we name gdp_a); another column, region, will indicate the region the values refer to. Once we have pivoted the table, we compute the inverted growth rate for each region.\n\nagri &lt;- agri |&gt; \n  pivot_longer(cols = -year, names_to = \"region\", values_to = \"gdp_a\") |&gt; \n  arrange(region, year) |&gt; \n  group_by(region) |&gt; \n  mutate(\n    inv_growth_rate = gdp_a / lead(gdp_a)\n  ) |&gt; \n  ungroup()\n\nWe set the periods with data and thos for which we want to extrapolate values.\n\nyears_to_complete_back &lt;- 1999:1987\n# years_to_complete_forw &lt;- 2013:2025 # Same as in the paper\nyears_to_complete_forw &lt;- 2024:2025\n\nThe width of the window on which the trend will be learned:\n\nwidth &lt;- length(\n  seq(max(years_to_complete_back) + 1, min(years_to_complete_forw) - 2)\n)\nwidth\n\n[1] 23\n\n\nThe years of the reference period for the linear model:\n\nyears_reference &lt;- seq(years_to_complete_back[1] + 1, length.out = width)\nyears_reference\n\n [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014\n[16] 2015 2016 2017 2018 2019 2020 2021 2022\n\n\nAnd we extend the dataset with the desired missing years:\n\nagri &lt;- crossing(\n  year = seq(min(years_to_complete_back), max(years_to_complete_forw)), \n  region = unique(agri$region)) |&gt; \n  full_join(\n    agri, by = c(\"year\", \"region\")\n  )\nagri\n\n# A tibble: 702 × 4\n    year region             gdp_a inv_growth_rate\n   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n 1  1987 Auckland              NA              NA\n 2  1987 Bay of Plenty         NA              NA\n 3  1987 Canterbury            NA              NA\n 4  1987 Gisborne              NA              NA\n 5  1987 Hawke's Bay           NA              NA\n 6  1987 Manawatu-Whanganui    NA              NA\n 7  1987 Marlborough           NA              NA\n 8  1987 New Zealand           NA              NA\n 9  1987 Northland             NA              NA\n10  1987 Otago                 NA              NA\n# ℹ 692 more rows\n\n\nFor each region \\(r\\), we regress the inverse growth rate \\(\\Delta_{r,t}^{-1}\\) on a linear trend (see Equation 5.22) and return the coefficient associated with the trend, \\(\\hat\\beta_r\\):\n\n# Regression of agricultural production on the ref. period on a linear trend\n# Extraction of the coefficient\ncoefs &lt;- agri |&gt; \n  filter(year %in% years_reference) |&gt; \n  group_by(region) |&gt; \n  mutate(t = row_number()) |&gt; \n  group_modify(~ {\n    model &lt;- lm(inv_growth_rate ~ t, data = .x)\n    tibble(coef_t = coef(model)[[\"t\"]])\n  })\n\nAs explained in Equation 5.23, we set some weights \\(w_j\\) to compute a geometrically weighted average of the inverse growth rate.\n\n# Geometric sequence for weigthing scheme\nreason &lt;- 1 / 1.35\npond &lt;- reason^(0:(width - 1))\npond &lt;- pond / sum(pond)\n\nThen, we loop on each region \\(r\\) to impute the missing values for agricultural GDP \\(Y^{(a)}_{r,t}\\).\n\nfor (region in unique(agri$region)) {\n  # Focus on curent region\n  coef_region &lt;- coefs |&gt; filter(region == !!region) |&gt; pull(coef_t)\n  \n  ## Predict backward ----\n  for (i in 1:length(years_to_complete_back)) {\n    year &lt;- years_to_complete_back[i]\n    ind_row &lt;- which(agri$year == year & agri$region == region)\n    ind_row_next &lt;- which(agri$year == year+1 & agri$region == region)\n    window &lt;- seq(year + 1, year + width)\n    agri_region_w &lt;- agri |&gt; filter(region == !!region, year %in% !!window)\n    \n    # Agricultural GDP in the next year\n    gdp_a_next &lt;- agri$gdp_a[ind_row_next]\n    \n    # Complete missing values\n    agri$gdp_a[ind_row] &lt;- \n      ((.5*(pond %*% agri_region_w$inv_growth_rate)) + (.5*(1+coef_region))) *\n      gdp_a_next\n    \n    agri$inv_growth_rate[ind_row] &lt;- \n      agri$gdp_a[ind_row] / agri$gdp_a[ind_row_next]\n  }\n  \n  ## Predict forward ----\n  for (i in 1:length(years_to_complete_forw)) {\n    year &lt;- years_to_complete_forw[i]\n    ind_row &lt;- which(agri$year == year & agri$region == region)\n    ind_row_prev &lt;- which(agri$year == year-1 & agri$region == region)\n    # Previous agricultural GDP\n    gdp_a_previous &lt;- agri$gdp_a[ind_row_prev]\n    # Prediction assuming the linear trend\n    agri$gdp_a[ind_row] &lt;- gdp_a_previous * (1+coef_region)\n  }\n}\n\nWe can compute the regional weights \\(w_{r,t}\\) (Equation 5.26).\n\n# Expressing as a share of total national value\ntb_agri_shares &lt;- agri |&gt; \n  filter(!region == \"Nez Zealand\") |&gt; \n  group_by(year) |&gt; \n  mutate(agri_share_region = gdp_a / sum(gdp_a))\ntb_agri_shares |&gt; select(year, region, gdp_a, agri_share_region)\n\n# A tibble: 702 × 4\n# Groups:   year [39]\n    year region              gdp_a agri_share_region\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n 1  1987 Auckland            180.            0.0133 \n 2  1987 Bay of Plenty       350.            0.0259 \n 3  1987 Canterbury          605.            0.0448 \n 4  1987 Gisborne             84.6           0.00626\n 5  1987 Hawke's Bay         291.            0.0215 \n 6  1987 Manawatu-Whanganui  446.            0.0330 \n 7  1987 Marlborough          73.5           0.00544\n 8  1987 New Zealand        4470.            0.331  \n 9  1987 Northland           303.            0.0224 \n10  1987 Otago               289.            0.0214 \n# ℹ 692 more rows\n\n\nLet us save this table of weights:\n\nsave(tb_agri_shares, file = \"../data/Agriculture/tb_agri_shares.rda\")\n\n\n\n\n5.6.2.2 Matching Regional Agricultural Intensity and Weather Data\n\n5.6.2.2.1 Quick Fix on Region Names\nRegion names often differ across sources. Here, we need to align agricultural intensity (from a national source) with weather data whose regions follow GADM names.\nFirst, we list weather regions that do not appear in the agricultural data:\n\nunique(region_weather_monthly$region)[! unique(region_weather_monthly$region) %in% unique(tb_agri_shares$region)] |&gt; \n  cat(sep = \"\\n\")\n\nTasman\nManawatu-Wanganui\nNelson\n\n\nAgricultural table region names:\n\nsort(unique(tb_agri_shares$region))\n\n [1] \"Auckland\"           \"Bay of Plenty\"      \"Canterbury\"        \n [4] \"Gisborne\"           \"Hawke's Bay\"        \"Manawatu-Whanganui\"\n [7] \"Marlborough\"        \"New Zealand\"        \"Northland\"         \n[10] \"Otago\"              \"Southland\"          \"Taranaki\"          \n[13] \"Tasman/Nelson\"      \"Total North Island\" \"Total South Island\"\n[16] \"Waikato\"            \"Wellington\"         \"West Coast\"        \n\n\nWe notice two types of issues here:\n\nSpelling differences (easy to correct): Manawatu-Whanganui vs. Manawatu-Wanganui\nDifferent spatial aggregation: Tasman and Nelson are merged into a single region in the agricultural data (see Figure 5.30).\n\nFor the first issue, the fix is very easy, we just need to adjust the spelling.\n\ntb_agri_shares &lt;- tb_agri_shares |&gt; \n  mutate(\n    region = if_else(\n      region == \"Manawatu-Whanganui\",\n      \"Manawatu-Wanganui\",\n      region\n    )\n  )\n\nFor the second issue, let us first visualize the Tasman/Nelson regions.\n\n\nCodes to create the Figure.\nggplot(\n  data = maps_level_1_NZ |&gt; \n    mutate(\n      issue = case_when(\n        NAME_1 == \"Tasman\" ~ \"Tasman\",\n        NAME_1 == \"Nelson\" ~ \"Nelson\",\n        TRUE ~ \"Other\"\n      ),\n      issue = factor(issue, levels = c(\"Tasman\", \"Nelson\", \"Other\"))\n    ),\n  mapping = aes(fill = issue)\n) + \n  geom_sf(colour = \"white\") +\n  scale_fill_manual(\n    \"Regions\",\n    values = c(\"Tasman\" = \"#0072B2\", \"Nelson\" = \"#D55E00\", \"Other\" = \"gray\")\n  )\n\n\n\n\n\nFigure 5.30: Regions merged in the agricultural data.\n\n\n\n\n\n\n\n\nTasman is much larger than Nelson, so we split the combined agricultural share proportionally to their land areas (not 50/50). We compute areas from the map.\n\n# Areas of each region\nnz_areas &lt;- maps_level_1_NZ |&gt;\n  filter(NAME_1 %in% c(\"Tasman\", \"Nelson\")) |&gt;\n  mutate(area_m2 = st_area(geometry)) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(area_m2 = as.numeric(area_m2)) |&gt;\n  select(region = NAME_1, area_m2)\nnz_areas\n\n  region    area_m2\n1 Nelson  419763011\n2 Tasman 9631754767\n\n\nThe corresponding weight:\n\ntas_nel_weights &lt;- nz_areas |&gt;\n  mutate(w = area_m2 / sum(area_m2)) |&gt;\n  select(region, w)\ntas_nel_weights\n\n  region          w\n1 Nelson 0.04176116\n2 Tasman 0.95823884\n\n\nNow we duplicate each \"Tasman/Nelson\" row into two rows (\"Tasman\" and Nelson) and allocate gdp_a and agri_share_region using the weights:\n\ngroup_label &lt;- \"Tasman/Nelson\"\n\ntb_agri_shares &lt;-\n  # keep all rows that are not the Tasman + Nelson aggregate\n  tb_agri_shares |&gt; filter(region != !!group_label) |&gt; \n  bind_rows(\n    # duplicate the grouped rows for Tasman and Nelson and split the share\n    tb_agri_shares |&gt;\n      filter(region == !!group_label) |&gt;\n      # create two copies per row, one for each region\n      crossing(region_2 = c(\"Tasman\", \"Nelson\")) |&gt;\n      select(-region) |&gt; \n      rename(region = region_2) |&gt; \n      # add area weights\n      left_join(tas_nel_weights, by = \"region\") |&gt;\n      # apportion only the agri_share_region (as asked)\n      mutate(\n        gdp_a = gdp_a * w,\n        agri_share_region = agri_share_region * w) |&gt;\n      select(-w)\n  ) |&gt;\n  arrange(region, year)\n\nAs a sanity check, we look whether weather regions are still missing from the agricultural table:\n\nunique(region_weather_monthly$region)[! unique(region_weather_monthly$region) %in% unique(tb_agri_shares$region)] |&gt; \n  cat(sep = \"\\n\")\n\nAn we look which agricultural regions are not present in weather data (these are country- and island-level aggregates, which is fine):\n\nunique(tb_agri_shares$region)[! unique(tb_agri_shares$region) %in% unique(region_weather_monthly$region)] |&gt; \n  cat(sep = \"\\n\")\n\nNew Zealand\nTotal North Island\nTotal South Island\n\n\n\n\n5.6.2.2.2 Merge\nTo obtain national-level indicators, we aggregate regional weather observations using agricultural intensity as weights. We merge the regional weather data with the corresponding agricultural weights (from agri_share_region) for each year and compute the weighted mean of each weather variable.\nFor monthly data, we group by year and month:\n\nnational_weather_monthly &lt;- \n  region_weather_monthly |&gt; \n  # We restricted the agricultural data to 1987--2024\n  filter(year &gt;= 1987) |&gt; \n  left_join(\n    tb_agri_shares |&gt; select(year, region, agri_share_region),\n    by = c(\"region\", \"year\")\n  ) |&gt; \n  group_by(year, month, month_name) |&gt; \n  summarise(\n    across(\n      all_of(weather_vars),\n      ~ weighted.mean(.x, w = agri_share_region)\n    ),\n    .groups = \"drop\"\n  )\n\nFor quarterly data, the same logic applies, except we aggregate by year and quarter:\n\nnational_weather_quarterly &lt;- \n  region_weather_quarterly |&gt; \n  # We restricted the agricultural data to 1987--2024\n  filter(year &gt;= 1987) |&gt; \n  left_join(\n    tb_agri_shares |&gt; select(year, region, agri_share_region),\n    by = c(\"region\", \"year\")\n  ) |&gt; \n  group_by(year, quarter) |&gt; \n  summarise(\n    across(\n      all_of(weather_vars),\n      ~ weighted.mean(.x, w = agri_share_region)\n    ),\n    .groups = \"drop\"\n  )\n\nWe can have a look at the obtained values using different type of plots.\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_monthly |&gt; \n    mutate(date = make_date(year, month)),\n  mapping = aes(x = date, y = precip)) +\n  geom_line() +\n  labs(x = NULL, y = \"Precipitation (mm)\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.31: Precipitation in New Zealand on a monthly basis.\n\n\n\n\n\n\n\n\nWith the quarterly national aggregation:\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_quarterly |&gt; \n    mutate(date = year + quarter/4),\n  mapping = aes(x = date, y = precip)) +\n  geom_line() +\n  labs(x = NULL, y = \"Precipitation (mm)\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.32: Precipitation in New Zealand on a quarterly basis.\n\n\n\n\n\n\n\n\nIf one wants to highlight a specific year, e.g., 2013:\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_monthly |&gt; \n    mutate(\n      month_abb = factor(month.abb[month], levels = month.abb),\n      highlight_year = case_when(\n        year == 2013 ~ \"2013\",\n        TRUE ~ \"Other\"\n      )\n    ),\n  mapping = aes(x = month_abb, y = precip)) +\n  geom_line(\n    mapping = aes(group = year, colour = highlight_year)\n  ) +\n  scale_colour_manual(\n    NULL, values = c(\"2013\" = \"#D55E00\", \"Other\" = \"gray\")\n  ) +\n  labs(x = NULL, y = \"Precipitation (mm)\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.33: Precipitation in New Zealand on a monthly basis (1987–2024).\n\n\n\n\n\n\n\n\nWe can use barplots also and show the 1-SD range across years with error bars.\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_monthly |&gt; \n    mutate(\n      month_abb = factor(month.abb[month], levels = month.abb)\n    ) |&gt; \n    group_by(month_abb) |&gt; \n    summarise(\n      precip_mean = mean(precip),\n      precip_sd = sd(precip)\n    )\n) +\n  geom_bar(\n    mapping = aes(x = month_abb, y = precip_mean),\n    stat=\"identity\",\n    fill=\"#5482AB\", alpha=0.7\n  ) +\n  geom_errorbar(\n    mapping = aes(\n      x = month_abb,\n      ymin = precip_mean - precip_sd,\n      ymax = precip_mean + precip_sd\n    ),\n    width = 0.4, colour = \"#005A8B\", alpha = 0.9, linewidth = 1.3\n  ) +\n  labs(x = NULL, y = \"Precipitation (mm)\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.34: Barplot of precipitation in New Zealand on a monthly basis (1987–2024).\n\n\n\n\n\n\n\n\nObviously, we are not restricted to plots showing temperatures. Let us visualize the evolution of the SMDI between 1987 and 2024, on a monthly basis. Note that the spatial aggregation has flattened the series (recall the monthly values before spatial aggregation range between -4 (very dry) to +4 (very wet)).\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_monthly |&gt; \n    mutate(date = make_date(year, month)),\n  mapping = aes(x = date, y = SMDI)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"3 years\", date_labels = \"%Y\") +\n  labs(x = NULL, y = \"SMDI\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.35: Soil Moisture Deficit Index in New Zealand, monthly basis.\n\n\n\n\n\n\n\n\nAnd with the quarterly data:\n\n\nCodes to create the Figure.\nggplot(\n  data = national_weather_quarterly |&gt; \n    mutate(date = year + quarter/4),\n  mapping = aes(x = date, y = SMDI)) +\n  geom_line() +\n  scale_x_continuous(n.breaks = 12) +\n  labs(x = NULL, y = \"SMDI\") +\n  theme_paper()\n\n\n\n\n\nFigure 5.36: Soil Moisture Deficit Index in New Zealand, quarterly basis.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-weather-metrics.html#saving-the-datasets",
    "href": "data-weather-metrics.html#saving-the-datasets",
    "title": "5  Weather Data: Metrics",
    "section": "5.7 Saving the Datasets",
    "text": "5.7 Saving the Datasets\nTo finish, let us export the two datasets. If those datasets are to be used in R later on, they can be saved as RData files to save space and importing time.\n\nsave(\n  national_weather_monthly, \n  file = \"../data/Weather/national_weather_monthly.rda\"\n)\nsave(\n  national_weather_quarterly, \n  file = \"../data/Weather/national_weather_quarterly.rda\"\n)\n\nIf one wants to share the dataset for use in another software, a CSV file is also possible:\n\nreadr::write_csv(\n  x = national_weather_monthly, \n  file = \"../data/Weather/national_weather_monthly.csv\"\n)\nreadr::write_csv(\n  x = national_weather_quarterly, \n  file = \"../data/Weather/national_weather_quarterly.csv\"\n)\n\n\n\n\nTable 5.2: Dictionary of variables for monthly precipitation (national_weather_monthly)\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nyear\nnumeric\nYear\n\n\nmonth\nnumeric\nMonth\n\n\nmonth_name\nordered factor\nName of the month\n\n\nprecip\nnumeric\nAverage of monthly precipitation (mm)\n\n\ntemp_min\nnumeric\nAverage of monthly min temperature (°C)\n\n\ntemp_max\nnumeric\nAverage of monthly max temperature (°C)\n\n\ntemp_mean\nnumeric\nAverage of monthly mean temperature (°C)\n\n\nSMDI\nnumeric\nSoil Moisture Deficit Inded (from -4, very dry to +4n very wet)\n\n\nSPEI_1\nnumeric\n1-Month Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_3\nnumeric\n3-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_6\nnumeric\n6-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_12\nnumeric\n12-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\n\n\n\n\n\n\n\nTable 5.3: Dictionary of variables for quarterly precipitation (national_weather_quarterly)\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nyear\nnumeric\nYear\n\n\nquarter\nnumeric\nQuarter\n\n\nmonth_name\nordered factor\nName of the month\n\n\nprecip\nnumeric\nAverage of monthly precipitation (mm)\n\n\ntemp_min\nnumeric\nAverage of monthly min temperature (°C)\n\n\ntemp_max\nnumeric\nAverage of monthly max temperature (°C)\n\n\ntemp_mean\nnumeric\nAverage of monthly mean temperature (°C)\n\n\nSMDI\nnumeric\nSoil Moisture Deficit Inded (from -4, very dry to +4n very wet)\n\n\nSPEI_1\nnumeric\n1-Month Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_3\nnumeric\n3-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_6\nnumeric\n6-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\nSPEI_12\nnumeric\n12-Months Standardised Precipitation-Evapotranspiration Index with (negative values for dry, positive for wet)\n\n\n\n\n\n\n\n\n\n\nCampbell, Gaylon S., and John M. Norman. 1998. “Introduction.” In An Introduction to Environmental Biophysics, 1–13. Springer New York. https://doi.org/10.1007/978-1-4612-1626-1_1.\n\n\nDingman, S. Lawrence. 2015. Physical Hydrology. Waveland press.\n\n\nHamon, W. R. 1964. “Computation of Direct Runoff Amounts from Storm Rainfall” 63: 52–62.\n\n\nLutz, James A., Jan W. van Wagtendonk, and Jerry F. Franklin. 2010. “Climatic Water Deficit, Tree Species Ranges, and Climate Change in Yosemite National Park.” Journal of Biogeography 37 (5): 936–50. https://doi.org/10.1111/j.1365-2699.2009.02268.x.\n\n\nNarasimhan, B., and R. Srinivasan. 2005. “Development and Evaluation of Soil Moisture Deficit Index (SMDI) and Evapotranspiration Deficit Index (ETDI) for Agricultural Drought Monitoring.” Agricultural and Forest Meteorology 133 (1–4): 69–88. https://doi.org/10.1016/j.agrformet.2005.07.012.\n\n\nVicente-Serrano, Sergio M., Santiago Beguería, and Juan I. López-Moreno. 2010. “A Multiscalar Drought Index Sensitive to Global Warming: The Standardized Precipitation Evapotranspiration Index.” Journal of Climate 23 (7): 1696–1718. https://doi.org/10.1175/2009jcli2909.1.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Weather Data: Metrics</span>"
    ]
  },
  {
    "objectID": "data-macro.html",
    "href": "data-macro.html",
    "title": "6  Merge With Macroeconomic Data",
    "section": "",
    "text": "6.1 Functions for Seasonal Adjustment\nLet us load the graphs theme functions (See Chapter 1):\nWe define a function, remove_seasonality() as a wrapper to the seas() function from {seasonal}. This function uses X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau, to remove seasonality of a column in a tibble.\n#' Remove seasonality (or return trend) using X-13ARIMA-SEATS\n#'\n#' @param data A data frame.\n#' @param var The column in `data` to adjust (unquoted or string).\n#' @param start Start time for the `ts()` (e.g., c(1999, 1)).\n#' @param freq Frequency of the series (default 4 = quarterly).\n#'\n#' @return A tibble with columns: year, quarter, YEARS, and the adjusted series.\nremove_seasonality &lt;- function(data, \n                               variable, \n                               start, \n                               freq = 4) {\n  \n  if (!freq %in% c(4, 12)) {\n    stop(\"`freq` must be either 4 (quarterly) or 12 (monthly).\", call. = FALSE)\n  }\n  v_sym &lt;- rlang::ensym(variable)\n  v_name &lt;- rlang::as_name(v_sym)\n  \n  # Turn in a ts object\n  data_ts &lt;- data |&gt; pull(v_sym) |&gt; \n    stats::ts(start = start, frequency = freq)\n  \n  # X-13ARIMA-SEATS\n  fit &lt;- seasonal::seas(data_ts)\n  # Extract trend (seasonaly adjuster series)\n  trend &lt;- seasonal::final(fit)\n  time_data &lt;- stats::time(data_ts)\n  \n  if (freq == 4) {\n    time_data &lt;- zoo::as.yearqtr(time_data)\n    res &lt;- tibble::tibble(\n      year = as.numeric(format(time_data, \"%Y\")),\n      quarter = as.numeric(format(time_data, \"%q\")),\n      !!v_name := trend\n    ) |&gt;\n      dplyr::mutate(\n        YEARS = year + c(0, 0.25, 0.5, 0.75)[quarter]\n      )\n  } else if (freq == 12) {\n    time_data_q &lt;- zoo::as.yearqtr(time_data)\n    time_data &lt;- zoo::as.yearmon(time_data)\n    res &lt;- tibble::tibble(\n      year = as.numeric(format(time_data, \"%Y\")),\n      month = as.numeric(format(time_data, \"%m\")),\n      quarter = as.numeric(format(time_data_q, \"%q\")),\n      !!v_name := trend\n    ) |&gt;\n      dplyr::mutate(\n        YEARS = as.numeric(time_data)\n      )\n  }\n  \n  res\n}\nWe also define another functions to remove trend: myfilter(). It calls one of the two other functions we define, hp_filter() or lin_trend() to remove seasonality in a series. With hp_filter() we use a Hodrick-Prescott filter from {mFilter} (using \\(\\lambda=1600\\), for quarterly data). With lin_trend(), we estimate a linear trend and return it.\n#' Detrends a time serie\n#' \n#' @param x Serie to be detrended (numeric).\n#' @param type Type of the method applied: Hodrick-Prescott (hp) of OLS (ols).\nmyfilter &lt;- function(x, type = c(\"hp\", \"ols\")) {\n\n  if (type == \"hp\") {\n    res &lt;- hp_filter(x)\n    res &lt;- log(x / res) * 100\n  } else if (type == \"ols\") {\n    x_log &lt;- log(x)*100\n    trend_x &lt;- lin_trend(x_log)\n    res &lt;- x_log - trend_x\n  } else {\n    stop(\"Not the right filter\")\n  }\n  res\n}\n#' Applies the HP filter on a quarterly time serie\n#' \n#' @param x Series for which to retrieve the trend.\n#' @returns The trend part of the series.\n#' \n#' @importFrom mFilter hpfilter\n#' \nhp_filter &lt;- function(x) {\n  serie &lt;- x\n  if (any(is.na(x))) {\n    serie &lt;- x[!is.na(x)]\n  }\n  \n  res &lt;- mFilter::hpfilter(serie, freq = 1600, type = \"lambda\")$trend |&gt; as.vector()\n  if (any(is.na(x))) {\n    x[!is.na(x)] &lt;- res\n    res &lt;- x\n  }\n  res\n}\n#' Returns the linear trend of `x`\n#' \nlin_trend &lt;- function(x, trend_val = df$YEARS) {\n  \n  serie &lt;- x\n  if (any(is.na(x))) {\n    serie &lt;- x[!is.na(x)]\n    trend_val &lt;- trend_val[!is.na(x)]\n  }\n  \n  df_tmp &lt;- data.frame(x = serie) |&gt; \n    mutate(cste = 1, trend = trend_val)\n  \n  reg &lt;- lm(x ~ 1 + trend, data = df_tmp)\n  resul &lt;- coef(reg)[[\"trend\"]]*df_tmp$trend + coef(reg)[[\"(Intercept)\"]]\n  \n  \n  if (any(is.na(x))) {\n    x[!is.na(x)] &lt;- resul\n    resul &lt;- x\n  }\n  \n  resul\n}",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-row",
    "href": "data-macro.html#sec-row",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.2 Trading Partners",
    "text": "6.2 Trading Partners\nIn Chapter 8, we will estimate a VAR model with three blocks:\n\nWeather block: captures climate variability indicators.\nTrading partners’ block: represents external economic shocks.\nDomestic economy block: corresponds to New Zealand.\n\nNew Zealand is a small open economy, meaning that its business cycles are influenced by shocks affecting its main trading partners.\nTo account for these external effects, we construct a synthetic rest-of-world GDP based on the top five partners: Australia, Germany, Japan, the United Kingdom, and the United States.\nThe following macroeconomic series are imported for each country:\n\n\n\nTable 6.1: Source of the different macroeconomic series\n\n\n\n\n\n\n\n\n\n\n\nVariable\nSource\nFrequency\nPurpose\n\n\n\n\nGross domestic product\nOECD QNA\nQuarterly\nEconomic conditions\n\n\nShort-term interest rate\nOECD EO100\nQuarterly\nGlobal monetary stance\n\n\nConsumer Price Index (CPI)\nOECD QNA\nQuarterly\nInflation proxy\n\n\nGDP Deflator\nOECD QNA archive\nQuarterly\nPrice base consistency\n\n\nPopulation (15–64)\nOECD historical population data\nAnnual → Quarterly\nScaling variable\n\n\n\n\n\n\nThe series are in Excel files in ../data/Economic/Rest-world. In the codes that follow, we harmonize them and, when necessary, rebase them to 2010 = 100. Annual population data are converted to quarterly frequency using the Denton–Cholette disaggregation method.\nWe set a reference year for the index:\n\nref_year &lt;- 2010\n\n\n6.2.1 Gross Domestic Product\n\nSource: OECD\nQuarterly GDP and components - expenditure approach, US Dollars\nFrequency of observation: Quarterly\nPrice base: Current prices\nCombined transaction: Gross domestic product, Total economy\n\n\ndf_gdp &lt;- read_excel(\n  \"../data/Economic/Rest-world/GDP.xlsx\", skip = 5,\n  n_max = 6\n) |&gt; \n  select(-`Time period...2`,-`Time period...3`) |&gt; \n  rename(country = `Time period...1`) |&gt; \n  filter(country != \"Reference area\") |&gt; \n  pivot_longer(cols = -country, values_to = \"gdp\") |&gt; \n  mutate(\n    year = str_sub(name, 1, 4) |&gt; as.numeric(),\n    quarter = str_sub(name, -1) |&gt; as.numeric(),\n    YEARS = year + (quarter-1)/4\n  ) |&gt; \n  select(-name)\n\n\np &lt;- ggplot(\n  data = df_gdp, \n  mapping = aes(x = YEARS, y = gdp, colour = country)\n) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_discrete(name = \"\") +\n  theme_paper()\np\n\nWarning: Removed 188 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nFigure 6.1: GDP (PPP, millions $, constant 2010 prices)\n\n\n\n\n\n\n\n\n\n\n6.2.2 Interest Rate\n\nSource: OECD\nEconomic Outlook No 100 - November 2016\nVariable: Short-term interest rate\nFrequency: Quarterly\nhttps://stats.oecd.org/Index.aspx?DataSetCode=EO100_INTERNET\n\n\ndf_r &lt;- read_excel(\n  \"../data/Economic/Rest-world/interest-rate.xlsx\",\n  skip = 4, n_max = 6\n) |&gt; \n  select(-`Time...2`) |&gt; \n  rename(country = `Time...1`) |&gt; \n  filter(country != \"Country\") |&gt; \n  pivot_longer(cols = -country, values_to = \"r\") |&gt; \n  mutate(\n    year = str_sub(name, 1, 4) |&gt; as.numeric(),\n    quarter = str_sub(name, -1) |&gt; as.numeric(),\n    YEARS = year + (quarter-1)/4\n  ) |&gt; \n  select(-name)\n\nNew names:\n• `Time` -&gt; `Time...1`\n• `Time` -&gt; `Time...2`\n\n\n\np &lt;- ggplot(\n  data = df_r, \n  mapping = aes(x = YEARS, y = r, colour = country)\n) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_discrete(name = \"\") +\n  theme_paper()\np\n\nWarning: Removed 264 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nFigure 6.2: Short-Term Interest Rate\n\n\n\n\n\n\n\n\nRemove missing observation.\n\ndf_r &lt;- df_r |&gt;  filter(!is.na(r))\n\n\n\n6.2.3 Consumer Price Index\n\nSource: OECD\nConsumer price indices (CPIs, HICPs), COICOP 1999\nFrequency of observation: Quarterly\nMeasure: Consumer price index, National\nUnit of measure: Index, 2015\n\n\ndf_cpi &lt;- read_excel(\n  \"../data/Economic/Rest-world/CPI.xlsx\",\n  skip = 5, n_max = 6\n) |&gt; \n  select(-`Time period...2`) |&gt; \n  rename(country = `Time period...1`) |&gt; \n  filter(country != \"Reference area\") |&gt; \n  pivot_longer(cols = -country, values_to = \"cpi\") |&gt; \n  mutate(\n    year = str_sub(name, 1, 4) |&gt; as.numeric(),\n    quarter = str_sub(name, -1) |&gt; as.numeric(),\n    YEARS = year + (quarter-1)/4\n  ) |&gt; \n  select(-name)\n\nChange base year:\n\nref_cpi &lt;- df_cpi |&gt;  \n  filter(YEARS == !!ref_year) |&gt; \n  select(country, cpi_ref = cpi)\n\ndf_cpi &lt;- df_cpi |&gt; \n  left_join(ref_cpi, by = c(\"country\")) |&gt; \n  mutate(\n    cpi = cpi / cpi_ref * 100\n  ) |&gt; \n  select(-cpi_ref)\n\n\np &lt;- ggplot(\n  data = df_cpi, \n  mapping = aes(x = YEARS, y = cpi, colour = country)\n) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_discrete(name = \"\") +\n  theme_paper()\np\n\nWarning: Removed 114 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nFigure 6.3: CPI\n\n\n\n\n\n\n\n\nRemove missing obs.\n\ndf_cpi &lt;- df_cpi |&gt; filter(!is.na(cpi))\n\n\n\n6.2.4 GDP Deflator\n\nSource: OECD\nQNA – Archive before 2019 benchmark revisions\nSubject: Gross domestic product - expenditure approach\nMeasure: Deflator, OECD reference year, seasonally adjusted\nFrequency: Quarterly\n\n\ndf_gdp_defl &lt;- read_excel(\n  \"../data/Economic/Rest-world/GDP_deflator.xlsx\",\n  skip = 4, n_max = 7\n) |&gt; \n  select(-`Period...2`) |&gt; \n  rename(country = `Period...1`) |&gt; \n  filter(country != \"Country\") |&gt; \n  pivot_longer(cols = -country, values_to = \"gdp_defl\") |&gt; \n  mutate(\n    year = str_sub(name, 1, 4) |&gt; as.numeric(),\n    quarter = str_sub(name, -1) |&gt; as.numeric(),\n    YEARS = year + (quarter-1)/4\n  ) |&gt; \n  select(-name)\n\n\np &lt;- ggplot(\n  data = df_gdp_defl, \n  mapping = aes(x = YEARS, y = gdp_defl, colour = country)\n) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_discrete(name = \"\") +\n  theme_paper()\np\n\nWarning: Removed 448 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\nFigure 6.4: GDP Deflator\n\n\n\n\n\n\n\n\nRemove missing values:\n\ndf_gdp_defl &lt;- df_gdp_defl |&gt;  filter(!is.na(gdp_defl))\n\n\n\n6.2.5 Population\n\nSource: OECD\nHistorical population data\nMeasure: Population\nAge: From 15 to 64 years\nTime horizon: Historical\nCombined unit of measure: Persons\n\n\npop &lt;- read_excel(\n  \"../data/Economic/Rest-world/pop.xlsx\", skip = 5,\n  n_max = 7\n) |&gt; \n  select(-`Time period...2`) |&gt; \n  rename(country = `Time period...1`) |&gt; \n  filter(country != \"Reference area\") |&gt; \n  pivot_longer(cols = -country, values_to = \"pop\") |&gt; \n  mutate(\n    year = str_sub(name, 1, 4) |&gt; as.numeric()\n  ) |&gt; \n  select(-name)\n\nThe population data is given at an annual rate. We use the Denton-Cholette method to estimate quarterly values, thanks to the td() function from {tempdisagg}. We define a function, pop_quarterly() as a wrapper function to do so, for a specific country.\n\nlibrary(tempdisagg)\n\n#' Estimate quarterly population from annual values\n#' \n#' @param country Name of the country in the dataset `pop`.\n#' \npop_quarterly &lt;- function(country) {\n  x &lt;- pop |&gt; \n    filter(country %in% !!country)\n  x_ts &lt;- ts(x$pop, start = as.numeric(x$year[1]))\n  # Forecast 3 years ahead\n  x_f &lt;- forecast::forecast(forecast::auto.arima(x_ts), h = 3)\n  # Disaggregate\n  x_ts &lt;- ts(c(x_ts, x_f$mean), start = start(x_ts))\n  res &lt;- predict(td(x_ts ~ 1, method = \"denton-cholette\", conversion = \"average\"))\n  res &lt;- tibble(\n    YEARS = as.vector(time(res)), \n    country = country, \n    pop = as.vector(res)\n  )\n  \n  # Change base year\n  ref_pop &lt;- res |&gt; filter(YEARS == ref_year) |&gt; pull(\"pop\")\n  \n  res |&gt; mutate(pop = pop / ref_pop * 100)\n}\n\nWe apply the pop_quarterly() on each of the trading partner’s population data.\n\ndf_pop &lt;- \n  map(unique(pop$country), ~pop_quarterly(.x)) |&gt; \n  list_rbind()\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\np &lt;- ggplot(\n  data = df_pop, \n  mapping = aes(x = YEARS, y = pop, colour = country)\n) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_discrete(name = \"\") +\n  theme_paper()\np\n\n\n\n\nFigure 6.5: Population\n\n\n\n\n\n\n\n\nRemove missing values:\n\ndf_pop &lt;- df_pop |&gt;  filter(!is.na(pop))\n\n\n\n6.2.6 GDP, Inflation rate, and CPI\nThe quarterly share of each country’s GDP can be easily computed:\n\nshares &lt;- \n  df_gdp |&gt; \n  group_by(year, quarter, YEARS) |&gt; \n  mutate(total_gdp = sum(gdp)) |&gt; \n  ungroup() |&gt; \n  filter(YEARS &gt;= 1987, YEARS &lt; 2017) |&gt; \n  mutate(share = gdp / total_gdp) |&gt; \n  select(YEARS, country, share)\n\nLet us merge all the previous series in a single dataset.\n\ndf_row_raw &lt;- \n  df_gdp |&gt; \n  left_join(df_r, by = c(\"country\", \"year\", \"quarter\", \"YEARS\")) |&gt; \n  left_join(df_cpi, by = c(\"country\", \"year\", \"quarter\", \"YEARS\")) |&gt; \n  left_join(df_gdp_defl, by = c(\"country\", \"year\", \"quarter\", \"YEARS\")) |&gt; \n  left_join(df_pop, by = c(\"country\", \"YEARS\")) |&gt; \n  left_join(shares, by = c(\"country\", \"YEARS\")) |&gt; \n  rename(years = YEARS) |&gt; \n  filter(years &gt;= 1987, years &lt; 2017) |&gt; \n  mutate(prices = gdp_defl)\n\nWe express the values in per-capita real terms.\n\ndf_w &lt;- \n  df_row_raw |&gt; \n  mutate(r_y = gdp / pop / prices) |&gt; \n  mutate(\n    r_y = r_y * share,\n    cpi = cpi * share,\n    gdp_defl = gdp_defl * share,\n    r = r * share\n  ) |&gt; \n  group_by(years) |&gt; \n  summarise(\n    wgdp = sum(r_y),\n    wcpi = sum(cpi),\n    wgdp_def = sum(gdp_defl),\n    wr = sum(r),\n    .groups = \"drop\"\n  ) |&gt; \n  rename(YEARS = years)\n\ndf_w\n\n# A tibble: 120 × 5\n   YEARS  wgdp  wcpi wgdp_def    wr\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 1987     NA  60.4       NA    NA\n 2 1987.    NA  61.1       NA    NA\n 3 1988.    NA  61.5       NA    NA\n 4 1988.    NA  61.9       NA    NA\n 5 1988     NA  62.2       NA    NA\n 6 1988.    NA  62.7       NA    NA\n 7 1988.    NA  63.3       NA    NA\n 8 1989.    NA  63.9       NA    NA\n 9 1989     NA  64.4       NA    NA\n10 1989.    NA  65.5       NA    NA\n# ℹ 110 more rows",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-macro-variables",
    "href": "data-macro.html#sec-macro-variables",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.3 Macroeconomic Variables",
    "text": "6.3 Macroeconomic Variables\nThe macroeconomic variables are in an Excel file:\n\nexcel_file &lt;- \"../data/Economic/data_nz.xls\"\n\n\n6.3.1 GDP (All industries & Agriculture)\n\nGDP (All industries & Agriculture)—-\nSeasonnaly adjusted\nSource: OECD\nFrequency: quarterly\n\n\ngdp &lt;- readxl::read_excel(path = excel_file, sheet = \"Y\", skip = 2)\n\ngdp &lt;- gdp |&gt; \n  dplyr::select(\n    date, \n    `NZNTAGCL Index`, # Agriculture Chain Volume\n    `NZNTPRAS Index`, # GDP Chain Volume\n    `NZNTNOM Index`   # NZ Expenditure Based GDP (Nominal, SA, NZD)\n  ) |&gt; \n  mutate(\n    agri_share = `NZNTAGCL Index` / `NZNTPRAS Index`,\n    gdp_a = agri_share * `NZNTNOM Index`\n  ) |&gt; \n  rename(gdp_tot = `NZNTNOM Index`) |&gt; \n  mutate(\n    gdp = gdp_tot - gdp_a,\n    year = as.numeric(str_sub(date, 1, 4)),\n    quarter = as.numeric(str_sub(date, 6, 7)),\n    quarter = ceiling(quarter / 3),\n    YEARS = year + quarter / 4 - 0.25\n  ) |&gt; \n  select(YEARS, gdp, gdp_a, gdp_tot)\n\n\n\n6.3.2 Consumption\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\nconsum &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"C\", skip = 3\n) |&gt; \n  rename(\n    YEARS = `...1`,\n    c = `Households...2`,\n    c_a = `Households...3`\n  )\n\nconsum &lt;- \n  consum |&gt; \n  select(YEARS, c, c_a) |&gt; \n  filter(YEARS %&gt;% str_detect(\"^[[:alnum:]]{4}Q[[:alnum:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter,\n    c = c |&gt; as.character() |&gt; as.numeric(),\n    c_a = c_a |&gt; as.character() |&gt; as.numeric()\n  ) |&gt; \n  select(-year, -quarter)\n\n\n\n6.3.3 Investment\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\ninvest &lt;- readxl::read_excel(path = excel_file, sheet = \"inv\") |&gt; \n  rename(\n    YEARS = Series,\n    I = `GDP(E)`,\n    I_private = Nominal\n  )\n\n\ninvest &lt;- \n  invest |&gt; \n  select(YEARS, I,I_private) |&gt; \n  filter(YEARS %&gt;% str_detect(\"^[[:alnum:]]{4}Q[[:alnum:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter,\n    I = I |&gt;  as.character() |&gt;  as.numeric(),\n    I_private = I_private |&gt; as.character() |&gt;  as.numeric()\n  ) |&gt; \n  select(-year, -quarter)\n\n\n\n6.3.4 Consumer price index\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\ncpi &lt;- readxl::read_excel(path = excel_file, sheet = \"CPI\", skip = 1, na = \",,\"\n) |&gt; \n  rename(\n    YEARS = `...1` ,\n    P = `All groups`\n  )\n\ncpi &lt;- \n  cpi |&gt; \n  select(YEARS, P) |&gt; \n  filter(YEARS |&gt; str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  mutate(P = P |&gt;  as.character() |&gt; as.numeric())\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `P = as.numeric(as.character(P))`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nChange base year:\n\nref_cpi &lt;- cpi |&gt;  filter(YEARS == ref_year) |&gt;  pull(\"P\")\n\ncpi &lt;- \n  cpi |&gt; \n  mutate(P = P / ref_cpi * 100)\n\n\n\n6.3.5 GDP Deflator\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\ngdp_defl &lt;- readxl::read_excel(path = excel_file, sheet = \"CPI\", skip = 1, na = \",,\"\n) |&gt; \n  select(years, gdp_defl)\n\n\ngdp_defl &lt;- \n  gdp_defl |&gt; \n  filter(years %&gt;% str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(years, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(years, -1))],\n    years = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  rename(YEARS = years, PP = gdp_defl) |&gt; \n  mutate(PP = PP |&gt; as.character() |&gt; as.numeric())\n\n# Change base year\nref_pp &lt;- gdp_defl |&gt; filter(YEARS == ref_year) |&gt; pull(\"PP\")\n\ngdp_defl &lt;- \n  gdp_defl |&gt; \n  mutate(PP = PP / ref_pp * 100)\n\n\n\n6.3.6 Exports\n\nSource: OECD\nFrequency: quarterly\n\n\ntrade &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"imports_exports\", skip = 3, na = \",,\"\n)\n\ntrade &lt;- \n  trade |&gt; \n  filter(YEARS %&gt;% str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  mutate(\n    exports_a = as.numeric(as.character(exports_a)),\n    imports = as.numeric(as.character(imports_sa_vfd)),\n    exports = as.numeric(as.character(exports_sa))\n  )\n\n\n\n6.3.7 Paid hours\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\npaid_hours &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"Hours(FTE)\", skip = 1, na = \",,\"\n) |&gt; \n  rename(\n    YEARS = `...1`,\n    weekly = `Total All Industries`\n  )\n\npaid_hours &lt;- \n  paid_hours |&gt; \n  select(YEARS, weekly) |&gt; \n  filter(YEARS |&gt; str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  rename(H = weekly) |&gt; \n  mutate(H = as.numeric(as.character(H)))\n\nIndex:\n\nref_hours &lt;- paid_hours |&gt; filter(YEARS == ref_year) |&gt; pull(\"H\")\n\npaid_hours &lt;- \n  paid_hours |&gt; \n  mutate(H = H / ref_hours * 100)\n\n\n\n6.3.8 Employment\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\nemployment &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"employment\", skip = 2, na = \",,\"\n) |&gt; \n  rename(employment = `Employment Rate`)\n\n\nemployment &lt;- \n  employment |&gt; \n  select(YEARS, employment) |&gt; \n  filter(YEARS %&gt;% str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  rename(E = employment) |&gt; \n  mutate(E = as.numeric(as.character(E)))\n\n\n\n6.3.9 Wages\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\nwages &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"Wages\", skip = 3, na = \",,\"\n) |&gt; \n  rename(\n    YEARS = `...1`,\n    Wage = `Total - Seasonally Adjusted`\n  )\n\nwages &lt;- \n  wages |&gt; \n  select(YEARS, Wage) |&gt; \n  filter(YEARS |&gt; str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  rename(W = Wage) |&gt; \n  mutate(W = as.numeric(as.character(W)))\n\nChange base year:\n\nref_wages &lt;- wages |&gt; filter(YEARS == ref_year) |&gt; pull(\"W\")\n\nwages &lt;- \n  wages |&gt; \n  mutate(W = W / ref_wages * 100)\n\n\n\n6.3.10 Population\n\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\npop &lt;- readxl::read_excel(path = excel_file, sheet = \"POP\", skip = 1)\ncolnames(pop)[c(1,2)] &lt;- c(\"YEARS\", \"pop\")\n\npop &lt;- \n  pop |&gt; \n  select(YEARS, pop) |&gt; \n  mutate(POP = pop |&gt; str_replace_all(\",\", \"\")) |&gt; \n  select(-pop) |&gt; \n  filter(YEARS |&gt; str_detect(\"^[[:digit:]]{4}Q[[:digit:]]$\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter,\n    POP = POP |&gt; as.character() |&gt; as.numeric()\n  ) |&gt; \n  select(-year, -quarter)\n\nAs an index:\n\npop_ref &lt;- pop |&gt;  filter(YEARS == ref_year) |&gt; pull(\"POP\")\n\npop &lt;- \n  pop |&gt; \n  mutate(POP = POP / pop_ref)\n\n\n\n6.3.11 Production prices\n\nProduction prices (All industrie & agriculture)—-\nSource: Statistics New Zealand\nFrequency: quarterly\n\n\nprices &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"productionPrice\", skip = 1, na = \",,\"\n) |&gt; \n  rename(\n    YEARS = `...1`,\n    P = `All Industries`,\n    P_A = `Agriculture, Forestry and Fishing`\n  )\n\nprices &lt;-\n  prices |&gt; \n  select(YEARS, P_A) |&gt; \n  filter(YEARS |&gt; str_detect(\"^[[:alnum:]]{4}Q[[:alnum:]]\")) |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, 1, 4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, -1))],\n    YEARS = year + quarter,\n    P_A = P_A |&gt; as.character() |&gt; as.numeric()\n  ) |&gt; \n  select(-year, -quarter)\n\nPrice indices:\n\np_a_ref &lt;- prices %&gt;% filter(YEARS == ref_year) |&gt; pull(\"P_A\")\n\n# Join CPI\nprices &lt;- \n  cpi |&gt; \n  left_join(prices)\n\nJoining with `by = join_by(YEARS)`\n\nprices &lt;- \n  prices |&gt; \n  mutate(P_A = P_A / p_a_ref * 100)\n\n# Join CPI\nprices &lt;- \n  cpi |&gt; \n  left_join(prices)\n\nJoining with `by = join_by(YEARS, P)`\n\n\n\n\n6.3.12 Interest rate\n\nSource: OECD stat\nFrequency: quarterly\n\n\ninterest &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"R\", na = \",,\", skip = 10\n) |&gt; \n  select(YEARS = observation_date, R = IR3TBB01NZQ156N)\n\ninterest &lt;- \n  interest |&gt; \n  mutate(YEARS = YEARS |&gt; ymd()) |&gt; \n  mutate(\n    year = year(YEARS),\n    quarter = YEARS |&gt; quarter(),\n    YEARS = year + c(0, 0.25, 0.5, 0.75)[quarter]\n  ) |&gt; \n  select(-year, -quarter) |&gt; \n  filter(!is.na(YEARS), str_length(YEARS) &gt; 0)\n\n\n\n6.3.13 Exchange Rate\n\nSource: FRED\n\n\nex_rate &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"ExchangeRate\", skip = 0\n)\n\nex_rate &lt;- ex_rate[, which(colnames(ex_rate) %in% c(\"DATE\", \"RTWI\"))]\n\nex_rate &lt;- \n  ex_rate |&gt; \n  mutate(\n    quarter = str_extract(DATE, \"M(.*?)$\"),\n    quarter = str_sub(quarter, 2),\n    quarter = match(quarter, c(1,4, 7, 10)),\n    year = str_sub(DATE, 1, 4) |&gt; as.numeric(),\n    YEARS = year + quarter/4 - 0.25\n  ) |&gt; \n  select(-quarter, -year, -DATE)\n\nAs an index:\n\nex_rate_ref &lt;- ex_rate |&gt; filter(YEARS == ref_year) |&gt;  pull(\"RTWI\")\n\nex_rate &lt;-\n  ex_rate |&gt; \n  mutate(ex_rate = RTWI / ex_rate_ref * 100) |&gt; \n  select(YEARS, ex_rate)\n\n\n\n6.3.14 Real Effective Exchange Rate\n\nSource: FRED\nReal Broad Effective Exchange Rate for New Zealand (RBNZBIS)\nSource: Bank for International Settlements\nNot seasonally adjusted, monthly\n\n\nreer &lt;- readxl::read_excel(path = excel_file, sheet = \"Reer\", skip = 10)\n\nSeasonal adjustment:\n\ndebut_reer &lt;- reer$observation_date[1]\n\nreer &lt;- \n  remove_seasonality(\n  data = reer, \n  variable = \"RBNZBIS\", \n  start = c(year(debut_reer), month(debut_reer)), \n  freq = 12\n) |&gt; \n  group_by(year, quarter) |&gt; \n  summarise(reer = mean(RBNZBIS), .groups = \"drop\")\n\nModel used in SEATS is different: (0 1 1)\n\n\nAdd years:\n\nreer &lt;-\n  reer |&gt; \n  mutate(YEARS = as.numeric(year) + as.numeric(quarter)/4 - 0.25) |&gt; \n  select(-quarter, -year)\n\nAs an index:\n\nreer_ref &lt;- reer |&gt; filter(YEARS == ref_year) |&gt; pull(\"reer\")\n\nreer &lt;-\n  reer |&gt; \n  mutate(reer = reer / reer_ref * 100) |&gt; \n  select(YEARS, reer)\n\n\n\n6.3.15 Share Price (OECD)\n\n6.3.15.1 OECD\n\nSource : OECD\n\n\nshare_price &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"Share_Prices(OECD)\", skip = 1\n)\n\nshare_price &lt;- \n  share_price |&gt; \n  mutate(\n    year = as.numeric(str_sub(YEARS, -4)),\n    quarter = c(0, 0.25, 0.5, 0.75)[as.numeric(str_sub(YEARS, 2,2))],\n    YEARS = year + quarter,\n    share_price = share_prices_index |&gt; as.character() |&gt; as.numeric()\n  ) |&gt; \n  select(-year, -quarter)\n\nAs an index:\n\nshare_price_ref &lt;- share_price |&gt; filter(YEARS == ref_year) |&gt; pull(\"share_price\")\n\nshare_price &lt;-\n  share_price |&gt; \n  mutate(share_price = share_price / share_price_ref * 100)\n\n\n\n6.3.15.2 Bloomberg\n\nSource: BLOOMBERG\n\n\nshare_price &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"NZSE_Index\", skip = 1\n)\n\nshare_price &lt;-\n  share_price |&gt; \n  rename(date = Date, share_price = PX_LAST) |&gt; \n  mutate(\n    year = year(date),\n    month = month(date),\n    quarter = match(month, c(3,6,9,12)),\n    YEARS = year + quarter/4 - 0.25\n  ) |&gt; \n  select(YEARS, share_price)\n\nAs an index:\n\nshare_price_ref &lt;- share_price |&gt; \n  filter(YEARS == ref_year) |&gt;\n  pull(\"share_price\")\n\nshare_price &lt;-\n  share_price |&gt; \n  mutate(share_price = share_price / share_price_ref * 100)\n\n\n\n\n6.3.16 Crude Oil Prices (Dubai)\n\nSource : FRED\nGlobal price of Dubai Crude,\nU.S. Dollars per Barrell, Quarterly, Not Seasonally Adjusted\n\n\ncrude_oil &lt;- readxl::read_excel(\n  path = excel_file, sheet = \"Crude Oil\", skip = 10\n)\n\ncrude_oil &lt;- \n  crude_oil |&gt; \n  rename(date = observation_date, crude_oil = POILDUBUSDQ) |&gt; \n  mutate(\n    date = as.Date(date),\n    year = year(date),\n    month = month(date),\n    quarter = match(month, c(1,4,7,10)),\n    YEARS = year + quarter/4 - 0.25\n  )\n\nAdjust seasonality:\n\ncrude_oil_desais &lt;- \n  remove_seasonality(\n    data = crude_oil, \n    variable = \"crude_oil\", \n    start = c(crude_oil$year[1], crude_oil$quarter[1]),\n    freq = 4\n  )\n\nModel used in SEATS is different: (0 1 1)\n\ncrude_oil &lt;- \n  crude_oil |&gt; \n  select(-crude_oil) |&gt; \n  left_join(crude_oil_desais, by = \"YEARS\") |&gt; \n  select(YEARS, crude_oil)\n\nAs an index:\n\ncrude_oil_ref &lt;- crude_oil |&gt; filter(YEARS == ref_year) |&gt;  pull(\"crude_oil\")\n\ncrude_oil &lt;-\n  crude_oil |&gt; \n  mutate(crude_oil = crude_oil / crude_oil_ref * 100)",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-load-weather-data",
    "href": "data-macro.html#sec-load-weather-data",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.4 Weather data",
    "text": "6.4 Weather data\nThe weather data obtained in Chapter 5 were saved in an Rdata file that can be loaded with the load() function.\n\nload(\"../data/Weather/national_weather_quarterly.rda\")\n\n\nnational_weather_quarterly &lt;- \n  national_weather_quarterly |&gt; \n  mutate(YEARS = year + quarter/4 - 0.25)\n\n\n\n\n\n\n\nWarning\n\n\n\nNegative values of the Soil Moisture Deficit index depict droughts (see Figure 6.6 ). In the impulse response functions, we will impulse positive standard deviation shocks. To depict a shock corresponding to a increase in dryness, we change the sign of the SMDI variable (see Figure 6.7).\n\n\n\n\nCodes to create the Figure.\nspei_bands &lt;- tibble(\n  ymin = c(-Inf, -2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0),\n  ymax = c(-2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0,  Inf),\n  label = c(\n    \"Extremely dry\", \"Severely dry\", \"Moderately dry\", \"Mildly dry\",\n    \"Normal\",\n    \"Mildly wet\", \"Moderately wet\", \"Severely wet\", \"Extremely wet\"\n  ),\n  fill = c(\n    \"#67001f\", \"#b2182b\", \"#d6604d\", \"#f4a582\", # dry shades\n    \"#f7f7f7\",\n    \"#92c5de\", \"#4393c3\", \"#2166ac\", \"#053061\" # wet shades\n  )\n) |&gt; \n  mutate(label = factor(label, levels = label))\n\nggplot(data = national_weather_quarterly |&gt; \n         dplyr::select(YEARS, SMDI, SPEI_1, SPEI_3, SPEI_6, SPEI_12) |&gt; \n         pivot_longer(cols = -YEARS) |&gt; \n         mutate(\n           name = factor(\n             name, \n             levels = c(\"SMDI\", \"SPEI_1\", \"SPEI_3\", \"SPEI_6\", \"SPEI_12\"),\n             labels = c(\"SMDI\", \"SPEI 1\", \"SPEI 3\", \"SPEI 6\", \"SPEI 12\")\n             )\n         ),\n       mapping = aes(x = YEARS, y = value, colour = name)\n) + \n  geom_rect(\n    data = spei_bands,\n    aes(\n      xmin = -Inf, xmax = Inf,\n      ymin = ymin, ymax = ymax,\n      fill = label\n    ),\n    inherit.aes = FALSE,\n    alpha = 0.25\n  ) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_manual(\n    name = NULL,\n    values = c(\n      \"SMDI\" = \"#882255\", \n      \"SPEI 1\" = \"#44AA99\", \n      \"SPEI 3\" = \"#DDCC77\", \n      \"SPEI 6\" = \"#88CCEE\", \n      \"SPEI 12\" = \"#332288\"\n    )\n    ) +\n  scale_fill_manual(\n    name = \"Climate condition\",\n    values = setNames(spei_bands$fill, spei_bands$label)\n  ) +\n  theme_paper() +\n  theme(legend.position = \"right\", legend.direction = \"vertical\")\n\n\n\n\n\nFigure 6.6: Drought indices. Negative values correspond to dry conditions, positive values correspond to wet conditions.\n\n\n\n\n\n\n\n\n\n# Flip sign of drought indices\nnational_weather_quarterly &lt;- \n  national_weather_quarterly |&gt; \n  mutate(\n    across(\n      .cols = c(SMDI, SPEI_1, SPEI_3, SPEI_6, SPEI_12), \n      .fns = ~ - .x\n    )\n  )\n\nThe new series are shown in Figure 6.7.\n\n\nCodes to create the Figure.\nspei_bands &lt;- tibble(\n  ymin = c(-Inf, -2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0),\n  ymax = c(-2.0, -1.5, -1.0, -0.5,  0.5,  1.0,  1.5,  2.0,  Inf),\n  label = rev(c(\n    \"Extremely dry\", \"Severely dry\", \"Moderately dry\", \"Mildly dry\",\n    \"Normal\",\n    \"Mildly wet\", \"Moderately wet\", \"Severely wet\", \"Extremely wet\"\n  )),\n  fill = rev(c(\n    \"#67001f\", \"#b2182b\", \"#d6604d\", \"#f4a582\", # dry shades\n    \"#f7f7f7\",\n    \"#92c5de\", \"#4393c3\", \"#2166ac\", \"#053061\" # wet shades\n  ))\n) |&gt; \n  mutate(label = factor(label, levels = label))\n\nggplot(data = national_weather_quarterly |&gt; \n         dplyr::select(YEARS, SMDI, SPEI_1, SPEI_3, SPEI_6, SPEI_12) |&gt; \n         pivot_longer(cols = -YEARS) |&gt; \n         mutate(\n           name = factor(\n             name, \n             levels = c(\"SMDI\", \"SPEI_1\", \"SPEI_3\", \"SPEI_6\", \"SPEI_12\"),\n             labels = c(\"SMDI\", \"SPEI 1\", \"SPEI 3\", \"SPEI 6\", \"SPEI 12\")\n             )\n         ),\n       mapping = aes(x = YEARS, y = value, colour = name)\n) + \n  geom_rect(\n    data = spei_bands,\n    aes(\n      xmin = -Inf, xmax = Inf,\n      ymin = ymin, ymax = ymax,\n      fill = label\n    ),\n    inherit.aes = FALSE,\n    alpha = 0.25\n  ) +\n  geom_line() +\n  labs(x = NULL, y = NULL) +\n  scale_colour_manual(\n    name = NULL,\n    values = c(\n      \"SMDI\" = \"#882255\", \n      \"SPEI 1\" = \"#44AA99\", \n      \"SPEI 3\" = \"#DDCC77\", \n      \"SPEI 6\" = \"#88CCEE\", \n      \"SPEI 12\" = \"#332288\"\n    )\n    ) +\n  scale_fill_manual(\n    name = \"Climate condition\",\n    values = setNames(spei_bands$fill, spei_bands$label)\n  ) +\n  theme_paper() +\n  theme(legend.position = \"right\", legend.direction = \"vertical\")\n\n\n\n\n\nFigure 6.7: Drought indices with sign flipped. Negative values correspond to wet conditions, positive values correspond to dry conditions.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-merge-data",
    "href": "data-macro.html#sec-merge-data",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.5 Merge",
    "text": "6.5 Merge\nLet us merge all the previous datasets in a single one.\n\ndonnees_brutes_nz &lt;- \n  national_weather_quarterly |&gt; \n  full_join(gdp) |&gt;  \n  full_join(consum) |&gt;  \n  full_join(trade) |&gt;  \n  full_join(invest) |&gt;   \n  full_join(prices) |&gt;   \n  full_join(gdp_defl) |&gt;  \n  full_join(interest) |&gt;  \n  full_join(ex_rate) |&gt;  \n  full_join(reer) |&gt;  \n  full_join(pop) |&gt;  \n  full_join(paid_hours) |&gt;  \n  full_join(employment) |&gt;  \n  full_join(wages) |&gt;  \n  full_join(df_w) |&gt;  \n  full_join(share_price) |&gt;  \n  full_join(crude_oil) |&gt;  \n  filter(YEARS &gt;= 1994.25, YEARS &lt; 2017)\n\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\nJoining with `by = join_by(YEARS)`\n\n\n\ndf &lt;- \n  donnees_brutes_nz |&gt; \n  rename(Y = gdp, Y_TOT = gdp_tot, Y_A = gdp_a, C_A = c_a, C = c) |&gt; \n  select(\n    YEARS, Y, Y_TOT, C, I, I_private, exports, imports, exports_a, P, PP,\n    POP, H, E, W, Y_A, P_A, C_A, R,\n    crude_oil,\n    ex_rate, reer, share_price,\n    SMDI, SPEI_3,\n    wgdp, wr, wcpi, wgdp_def) |&gt; \n  ungroup()\n\nThe correlation between all the variables:\n\nmcor &lt;- df |&gt; select(-YEARS) |&gt; cor(use = \"pairwise.complete.obs\")\nggcorrplot::ggcorrplot(mcor)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at &lt;https://github.com/kassambara/ggcorrplot/issues&gt;.\n\n\n\n\n\nFigure 6.8: Correlation Matrix",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#sec-detrending",
    "href": "data-macro.html#sec-detrending",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.6 Detrending",
    "text": "6.6 Detrending\nSelecting Hodrick-Prescott filter:\n\ntype &lt;- \"hp\"\n\nWe will use the GDP deflator as the inflation time serie rather than CPI.\n\ndf &lt;- \n  df |&gt; \n  mutate(\n    prices = PP,\n    world_prices = wgdp_def,\n    invest = I_private,\n    ratio_p = P_A / prices\n  )\n\nPer-capita values and detrending:\n\ndf_finale &lt;- \n  df |&gt; \n  # Transformting in per capita / real terms\n  mutate(\n    r_y = Y / POP / prices,\n    r_y_tot = Y_TOT / POP / prices,\n    r_y_a = Y_A / POP / P_A,\n    r_wy = wgdp,\n    r_q = share_price / prices,\n    r_c = C / POP / prices,\n    r_x = exports / POP / prices,\n    r_x_a = exports_a / POP / prices,\n    r_im = imports / POP / prices,\n    r_tb = r_x - r_im,\n    r_c_a = C_A / POP / P_A,\n    r_i = invest / POP / prices,\n    r_h = H * E,\n    r_w = W / prices\n  ) |&gt; \n  select(-wgdp) |&gt; \n  mutate(\n    y_obs = myfilter(r_y, type),\n    y_tot_obs = myfilter(r_y_tot, type),\n    wy_obs = myfilter(r_wy, type),\n    y_a_obs = myfilter(r_y_a, type),\n    c_obs = myfilter(r_c, type),\n    c_a_obs = myfilter(r_c_a, type),\n    x_obs = myfilter(r_x, type),\n    x_a_obs = myfilter(r_x_a, type),\n    im_obs = myfilter(r_im, type),\n    q_obs = c(NA, diff(log(r_q))*100),\n    q_obs = q_obs - hp_filter(q_obs),\n    i_obs = myfilter(r_i, type),\n    h_obs = log(r_h / mean(r_h, na.rm=T)) * 100,\n    h_obs2 = log(E / mean(E, na.rm=T)) * 100,\n    e_obs = E,\n    w_obs = myfilter(r_w, type),\n    p_obs = c(NA, diff(log(prices))*100),\n    ratio_p_obs = myfilter(ratio_p, type),\n    wp_obs = c(NA, diff(log(world_prices))*100),\n    p_a_obs = c(NA, diff(log(P_A))*100),\n    oil_obs = c(NA, diff(log(crude_oil))*100),\n    r_obs = R / 4,\n    wr_obs = wr / 4,\n    ex_rate_obs = myfilter(ex_rate, type),\n    reer_obs = c(NA, diff(log(reer))*100),\n    smdi_obs = SMDI,\n    spei_3_obs = SPEI_3\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#export",
    "href": "data-macro.html#export",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.7 Export",
    "text": "6.7 Export\n\nsave(df_finale, file = \"../data/df_finale_ebook.rda\")",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-macro.html#graphs",
    "href": "data-macro.html#graphs",
    "title": "6  Merge With Macroeconomic Data",
    "section": "6.8 Graphs",
    "text": "6.8 Graphs\nWe define a tibble, corresp_names with various labels for the series.\n\n\nThe corresp_names tibble.\ncorresp_names &lt;- tribble(\n  ~name_r, ~long_name, ~short_name, ~pm_name,\n  \"wy_obs\", \"Foreign Output\", \"$\\\\Delta log\\\\left(Y_t^*\\\\right)$\", \"hat(y)[t]^F\",\n  \"wp_obs\", \"Foreign CPI Inflation\", \"$\\\\pi_t^*$\", \"hat(pi)[t]^F\",\n  \"wr_obs\", \"Foreign Interest Rate\", \"r_t^*\", \"hat(r)[t]^F\",\n  \"wp_a_obs\", \"Foreign Ag. Price Infl.\", \"\\\\Delta log \\\\left(p_t^{A*}\\\\right)$\", \"p[t]^D\",\n  \"wy_a_obs\", \"Foreign Ag. Output\", \"$\\\\Delta log \\\\left(Y_t^{A*}\\\\right)$\", \"Y[t]^D\",\n  \"oil_obs\", \"Crude Oil Inflation\", \"$oil_t$\", \"hat(oil)[t]\",\n  \"y_obs\", \"Output\", \"$\\\\Delta log \\\\left(Y_t^d\\\\right)$\", \"hat(y)[t]\",\n  \"y_a_obs\", \"Ag. Output\", \"$\\\\Delta log \\\\left(X_t^A\\\\right)$\", \"hat(y)[t]^A\",\n  \"p_obs\", \"CPI Inflation\", \"$\\\\pi_t^C$\", \"hat(pi)[t]\",\n  \"ratio_p_obs\", \"Rel. Prices\", \"$\\\\pi_{x,t}^A / \\\\pi_t^C$\", \"hat(pi)[t] / hat(pi)[t]^A\",\n  \"p_a_obs\", \"Ag. Inflation\", \"$log \\\\left(\\\\pi_{x,t}^A\\\\right)$\", \"hat(pi)[t]^A\",\n  \"c_obs\", \"Consumption\", \"$log \\\\left(c_{t}\\\\right)$\", \"hat(c)[t]\",\n  \"h_obs\", \"Hours Worked\", \"\\\\Delta log \\\\left($h_t\\\\right)$\", \"hat(h)[t]\",\n  \"i_obs\", \"Investment\", \"$\\\\Delta log \\\\left(i_t\\\\right)$\", \"hat(i)[t]\",\n  \"q_obs\", \"Stock Prices\", \"q_t\", \"hat(q)[t]\",\n  \"im_obs\", \"Imports\", \"\\\\Delta log \\\\left(im_{t}\\\\right)$\", \"hat(im)[t]\",\n  \"x_obs\", \"Exports\", \"$\\\\Delta log \\\\left(x_{t}\\\\right)$\", \"hat(x)[t]\",\n  \"tb_obs\", \"Trade Balance\", \"$tb_{t}$\", \"hat(tb)[t]\",\n  \"ex_rate_obs\", \"Real Ex. Rate\", \"$rer_t$\", \"hat(e)[t]\",\n  \"reer_obs\", \"Real Eff. Ex. Rate\", \"$reer_$\", \"hat(e)[t]\",\n  \"r_obs\", \"Interest Rate\", \"$r_t$\", \"hat(r)[t]\",\n  \"smdi_obs\", \"Weather\", \"$\\\\varepsilon_{t}^{W}$\", \"hat(s)[t]\",\n  \"r_y_hp\", \"GDP Deviation From HP Filter Trend\", \"$y_t$\", \"y[t]\",\n  \"r_y_a_hp\", \"agricultural GDP Deviation From HP Filter Trend\", \"$y_t^A$\", \"y[t]^A\",\n  \"smdi\", \"Wheater\", \"$\\\\varepsilon_{t}^{W}$\", \"hat(s)[t]\",\n  \"y_w\", \"Foreign Output\", \"$\\\\Delta log\\\\left(Y_t^*\\\\right)$\", \"hat(y)[t]^F\",\n  \"y\", \"Output\", \"$\\\\Delta log \\\\left(Y_t^d\\\\right)$\", \"hat(y)[t]\"\n)\n\n\nLet us focus on the following variables:\n\ntb_var_exo &lt;- tribble(\n  ~variable, ~label,\n  \"smdi_obs\", \"Weather (SMDI)\",\n  \"wy_obs\", \"Foreign Output\",\n  \"y_obs\", \"Production\",\n  \"y_a_obs\",  \"Agriculture\",\n  \"h_obs\", \"Hours Worked\",\n  \"c_obs\", \"Consumption\",\n  \"i_obs\", \"Investment\",\n  \"reer_obs\", \"Delta Exchange Rate\"\n)\n\nLet us plot all the variables of interest.\n\nggplot(\n  data = df_finale |&gt; \n    select(YEARS, !!tb_var_exo$variable) |&gt; \n    pivot_longer(cols = -YEARS) |&gt; \n    mutate(\n      name = factor(\n        name, levels = tb_var_exo$variable, labels = tb_var_exo$label\n      )\n    ),\n  mapping = aes(x = YEARS, y = value)\n) +\n  geom_line(colour = \"dodgerblue\") +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", colour = \"gray\") +\n  geom_vline(\n    xintercept = c(1998, 2002, 2008, 2013), \n    linetype = \"dotted\", col = \"red\"\n  ) +\n  theme_paper()\n\n\n\n\nFigure 6.9: Series in the final sample\n\n\n\n\n\n\n\n\nThe SMDI only:\n\nggplot(\n  data = df_finale |&gt; \n    mutate(y_a_hp = hp_filter(r_y_a)) |&gt; \n    select(YEARS, smdi_obs),\n  mapping = aes(x = YEARS, y = smdi_obs)\n) +\n  geom_line() +\n  geom_vline(\n    xintercept = c(2007.75), \n    linetype = \"dotted\", col = \"blue\"\n  )+\n  geom_vline(\n    xintercept = c(1998, 2003, 2008, 2013), \n    linetype = \"dotted\", col = \"red\"\n  ) +\n  labs(x = NULL, y = NULL) +\n  theme_paper()\n\n\n\n\nFigure 6.10: The SMDI in the final sample",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merge With Macroeconomic Data</span>"
    ]
  },
  {
    "objectID": "data-climate-projections.html",
    "href": "data-climate-projections.html",
    "title": "7  Climate Projection",
    "section": "",
    "text": "7.1 Load Projection Data\nAccording to the IPCC:\nIn our DSGE framework (Chapter 9), climate is supposed to be stationary. Our set-up is irrelevant for analyzing changes in mean climate values. However, it allows for changes in the variance of climate.\nIn the paper, we used monthly precipitation (pr) simulated by the NCAR Community Climate System Model, version 4 (CCSM4) under four different scenarios:\nValues for soil water deficit were not provided, so we estimated the variance of precipitation instead.\nDatasets for historical values (1850–2005) and projected values up to various horizons (we use 2006–2100) can be downloaded from here: https://aims2.llnl.gov/search/cmip5/. The data we use are saved in NetCDF files, and are provided on a monthly basis.\nThe name of the file is informative. For example: pr_Amon_CCSM4_rcp45_r6i1p1_200601-210012.nc:\npr_Amon_CCSM4_historical_r1i1p1_185001-200512\nWe are going to need a few R packages:\nWe define some colours for the scenarios:\nLet us load the graphs theme functions (See Chapter 1):\nLet us also load the maps for the countries of interest (See Chapter 2).\nWe define a function, get_netcfd_ccsm() to import the data from an .nc file that contains projected precipitation data.\nThe get_netcfd_ccsm() function.\n#' Compute area-weighted regional means of precipitation from CMIP5 NetCDF files\n#' \n#' @description\n#' This function reads monthly precipitation data (in kg/m2/s) from a CMIP5\n#' NetCDF file and computes area-weighted averages for each region of a country.\n#' The results are expressed in millimeters per month.\n#' \n#' @param nc_path Path to the NetCDF file containing the precipitation data.\n#' @param map_country An `sf` object defining the regions of the country.\n#' @param region_id Name of the column in `map_country` identifying the regions.\n#' @param start_date Optional. A `Date` object indicating the first date to\n#'   include in the output. If `NULL`, all available dates are kept.\n#' \n#' @returns A tibble with the following columns:\n#' * `region_id`: Region identifier\n#' * `date`: Date (corresponding to a month, even if given as a Date object).\n#' * `pr_mean`: Area-weighted mean precipitation (in millimeters).\n#'   \nget_netcfd_ccsm &lt;- function(nc_path, \n                            map_country, \n                            region_id,\n                            start_date = NULL) {\n  pr &lt;- rast(nc_path)\n  # Extract time\n  dates &lt;- terra::time(pr)\n  \n  if (!is.null(start_date)) {\n    idx &lt;- which(dates &gt;= start_date)\n    pr &lt;- pr[[idx]]\n    dates &lt;- dates[idx]\n    time(pr) &lt;- dates\n  }\n  \n  # Focus on the country of interest\n  # Reproject shapefile to match NetCDF raster CRS if necessary\n  map_country &lt;- st_transform(map_country, crs(pr))\n  # Cropping\n  pr_country &lt;- crop(pr, vect(map_country))\n  # Compute cell areas (used as weights)\n  cell_area &lt;- cellSize(pr_country, unit = \"km\")\n  \n  # Area-weighted means for each region\n  res_mat &lt;- exact_extract(\n    pr,\n    map_country,\n    fun = \"weighted_mean\",\n    weights = cell_area,\n    progress = FALSE\n  )\n  \n  # Bind region IDs\n  res_mat &lt;- dplyr::bind_cols(\n    map_country |&gt; \n      st_drop_geometry() |&gt; \n      dplyr::select(!!region_id),\n    res_mat\n  )\n  \n  # Tidy to long format, with dates\n  names(res_mat)[-1] &lt;- as.character(dates)\n  \n  region_daily_pr &lt;- res_mat |&gt;\n    tidyr::pivot_longer(\n      cols = -!!region_id,\n      names_to = \"date\",\n      values_to = \"pr_mean\"\n    ) |&gt;\n    dplyr::mutate(date = as.Date(date)) |&gt; \n    # Express in mm\n    mutate(pr_mean = pr_mean * 86400 * 30) |&gt; \n    rename(region_id = !!region_id)\n  \n  region_daily_pr\n}\nWe use that function to import the data for the historical values and for the projected values under the four RCP scenarios.\n# Adapt this path according to where you saved the NetCDF files.\npath_to_nc_files &lt;- \"../paper/codes_weather_shocks/data/climate_data/projections/\"\n\nprecip_hist &lt;- get_netcfd_ccsm(\n  nc_path = paste0(\n    path_to_nc_files, \"pr_Amon_CCSM4_historical_r1i1p1_185001-200512.nc\"\n  ),\n  map_country = maps_level_1_NZ, \n  region_id = \"NAME_1\", \n  start_date = as.Date(\"1960-01-01\")\n)\n\nprecip_rcp26 &lt;- get_netcfd_ccsm(\n  nc_path = paste0(\n    path_to_nc_files, \n    \"pr_Amon_CCSM4_rcp26_r1i1p1_200601-210012.nc\"\n  ), \n  map_country = maps_level_1_NZ, \n  region_id = \"NAME_1\", \n  start_date = NULL\n)\nprecip_rcp45 &lt;- get_netcfd_ccsm(\n  nc_path = paste0(\n    path_to_nc_files, \n    \"pr_Amon_CCSM4_rcp45_r1i1p1_200601-210012.nc\"\n  ), \n  map_country = maps_level_1_NZ, \n  region_id = \"NAME_1\", \n  start_date = NULL\n)\nprecip_rcp60 &lt;- get_netcfd_ccsm(\n  nc_path = paste0(\n    path_to_nc_files, \n    \"pr_Amon_CCSM4_rcp60_r1i1p1_200601-210012.nc\"\n  ), \n  map_country = maps_level_1_NZ, \n  region_id = \"NAME_1\", \n  start_date = NULL\n)\nprecip_rcp85 &lt;- get_netcfd_ccsm(\n  nc_path = paste0(\n    path_to_nc_files, \n    \"pr_Amon_CCSM4_rcp85_r1i1p1_200601-210012.nc\"\n  ), \n  map_country = maps_level_1_NZ, \n  region_id = \"NAME_1\", \n  start_date = NULL\n)",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Projection</span>"
    ]
  },
  {
    "objectID": "data-climate-projections.html#load-projection-data",
    "href": "data-climate-projections.html#load-projection-data",
    "title": "7  Climate Projection",
    "section": "",
    "text": "Note\n\n\n\nIf you want to use this function to import another type of variable, you need to change the last operation in the get_netcfd_ccsm() function:\n\nthe name of the column should not be \"pr_mean\",\nthe values may not be converted in mm.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Projection</span>"
    ]
  },
  {
    "objectID": "data-climate-projections.html#regional-weights-depending-on-agricultural-production",
    "href": "data-climate-projections.html#regional-weights-depending-on-agricultural-production",
    "title": "7  Climate Projection",
    "section": "7.2 Regional Weights Depending on Agricultural Production",
    "text": "7.2 Regional Weights Depending on Agricultural Production\nIn Chapter 5, we computed regional agricultural intensity for each year between 1987 and 2025. Let us compute the average regional agricultural intensity over the period 1987–2014, as in the paper. The restriction to this period can be changed depending on the sample used.\n\nload(\"../data/Agriculture/tb_agri_shares.rda\")\n\n\ntb_agri_shares &lt;- \n  tb_agri_shares |&gt; filter(year %in% 1987:2014)\n\nWe compute the average agricultural production for each region over the whole period:\n\ncultures_weights &lt;- \n  tb_agri_shares |&gt; \n  group_by(region) |&gt; \n  summarise(gdp_a = mean(gdp_a)) |&gt; \n  mutate(weight = gdp_a / sum(gdp_a)) |&gt; \n  select(-gdp_a) |&gt; \n  rename(region_id = region)\ncultures_weights\n\n# A tibble: 18 × 2\n   region_id           weight\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Auckland           0.0101 \n 2 Bay of Plenty      0.0254 \n 3 Canterbury         0.0517 \n 4 Gisborne           0.00598\n 5 Hawke's Bay        0.0213 \n 6 Manawatu-Whanganui 0.0294 \n 7 Marlborough        0.00636\n 8 New Zealand        0.333  \n 9 Northland          0.0193 \n10 Otago              0.0206 \n11 Southland          0.0281 \n12 Taranaki           0.0283 \n13 Tasman/Nelson      0.00714\n14 Total North Island 0.214  \n15 Total South Island 0.119  \n16 Waikato            0.0662 \n17 Wellington         0.00842\n18 West Coast         0.00586",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Projection</span>"
    ]
  },
  {
    "objectID": "data-climate-projections.html#national-aggregation",
    "href": "data-climate-projections.html#national-aggregation",
    "title": "7  Climate Projection",
    "section": "7.3 National Aggregation",
    "text": "7.3 National Aggregation\nWe have precipitation values at the month-region level. For each dataset (historical values and RCP scenarios), we perform a quarterly aggregation at the national level, using a function we define, national_quaterly_aggreg(). This functions first performs the national aggregation, by means of a weighted mean, using the regional agrilcultural weights that were just computed (cultures_weights). Then, it sums the national monthly values at the quarter level for each year.\n\n\nThe national_quaterly_aggreg() function.\n#' Computes quarterly aggregation at the national level of precipitation.\n#' \n#' @param x Tibble with monthly precipitation (see details).\n#' @param regional_weights Tibble witht the regional weights to use.\n#' \n#' @details\n#' The tibble `x` must contain the following columns:\n#' * `region_id`: Region identifier.\n#' * `date`: Date (corresponding to a month, even if given as a Date object).\n#' * `pr_mean`: Area-weighted mean precipitation (in millimeters).\n#' The tibble `regional_weights` must contain the following columns:\n#' * `region_id`: Region identifier.\n#' * `weight`: The weight to use for national aggregation.\n#' \nnational_quaterly_aggreg &lt;- function(x, regional_weights) {\n  x |&gt; \n    left_join(regional_weights, by = c(\"region_id\")) |&gt; \n    group_by(date) |&gt; \n    summarise(\n      pr_mean = sum(pr_mean * weight, na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt; \n    mutate(\n      year = year(date),\n      quarter = quarter(date)\n    ) |&gt; \n    group_by(year, quarter) |&gt; \n    summarise(\n      pr_mean = sum(pr_mean, na.rm = TRUE),\n      .groups = \"drop\"\n    )\n}\n\n\nWe loop over each of the datasets to apply the national_quaterly_aggreg() function.\n\nprecip_nat_q &lt;- list(\n  `Historical` = precip_hist,\n  `RCP 2.6` = precip_rcp26,\n  `RCP 4.5` = precip_rcp45,\n  `RCP 6.0` = precip_rcp60,\n  `RCP 8.5` = precip_rcp85\n) |&gt; \n  map(\n    .f = ~national_quaterly_aggreg(x = .x, regional_weights = cultures_weights)\n  )",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Projection</span>"
    ]
  },
  {
    "objectID": "data-climate-projections.html#change-in-the-variance-of-the-weather",
    "href": "data-climate-projections.html#change-in-the-variance-of-the-weather",
    "title": "7  Climate Projection",
    "section": "7.4 Change in the Variance of the Weather",
    "text": "7.4 Change in the Variance of the Weather\nWe now turn to the estimation of the volatility of the weather shock. Estimating this volatility will be useful to assess the welfare impact of climate change under different climate scenarios.\nTo estimate the volatility of the weather shock, for each scenario, we use a rolling window of 102 quarters (25.5 years), matching the DSGE sample size (see Chapter 9). Within each window, we fit an AR(1) model, using the sd_resid_ar1() function (defined below): \\[\nP_\\tau = \\mu + \\phi\\,P_{\\tau-1} + \\varepsilon_\\tau,\n\\] and we store \\(\\widehat{\\sigma}_t = \\text{sd}(\\widehat{\\varepsilon}_\\tau)\\). The stored values give a time-varying standard deviation of the weather shock.\n\n7.4.1 Helper Functions\nWe define a few helper functions:\n\nas_quarter_end(): to create a Date object corresponding to the end of a quarter in a given year.\nbind_hist_and_rcp(): to build a continuous series for one scenario, binding together historical values and a dataset with projections under a specific scenario.\nsd_resid_ar1: to fit an AR(1) model in a window and return the standard deviation of the residuals.\nsd_resid_scenario: the core function, which computes the rolling standard deviations of AR(1) residuals for a given scenario.\n\n\n\nThe as_quarter_end() function.\n#' Make a Date at the end of each quarter\n#' @param y Year (integer).\n#' @param q Quarter (integer).\n#' @returns The date at the end of the quarter.\n#' \nas_quarter_end &lt;- function(y, q) {\n  # first day of the quarter\n  yq &lt;- yq(paste(y, q))\n  # last day of the quarter\n  (yq %m+% months(3)) - days(1)\n}\n\n\n\n\nThe bind_hist_and_rcp() function.\n#' Build a continuous series for one scenario\n#' \n#' @description\n#' concatenates Historical data (up to 2005Q4) and data from a scenario (from \n#' 2006Q1).\n#' \n#' @param hist_tb Tibble with precipitation data for the historical period.\n#' @param rcp_tb Tibble with precipitation data for a scenario.\nbind_hist_and_rcp &lt;- function(hist_tb, \n                              rcp_tb) {\n  bind_rows(\n    hist_tb,\n    rcp_tb\n  ) |&gt; \n    arrange(year, quarter) |&gt; \n    mutate(date = as_quarter_end(year, quarter))\n}\n\n\n\n\nThe sd_resid_ar1() function.\n#' Fit AR(1) in a window and return sd of residuals\n#' \n#' @param x Series with quarterly precipitation.\n#' \n#' @returns Returns the standard deviation of the residuals of the AR(1), or \n#' `NA` if the number of observation is lower than 102 (window smaller than \n#' that used in the DSGE model).\nsd_resid_ar1 &lt;- function(x) {\n  if (length(x) &lt; 102) {\n    return(NA)\n  } else {\n    fit &lt;- stats::arima(x, order = c(1, 0, 0), include.mean = TRUE)\n    sd_resid &lt;- stats::sd(stats::residuals(fit), na.rm = TRUE)\n    return(sd_resid)\n  }\n}\n\n\n\n\nThe sd_resid_scenario() function.\n#' Compute rolling standard deviations of AR(1) residuals for scenario scaling\n#' \n#' @description\n#' This function computes the time-varying volatility of precipitation by\n#' estimating, within each rolling window, an AR(1) model and extracting the \n#' standard deviation of its residuals. We will use the resulting series to\n#' derive scenario multipliers (RCP-based scaling factors) for weather shock \n#' variances.\n#'\n#' @param tb A tibble (ordered by increasing dates) with precipitation for \n#' historical values and projected values under a climate scenario. The tibble \n#' must contain at least the following column:\n#'  * `pr_mean`: precipitation values.\n#' @param reg_start\n#' @param reg_end\n#' \n#' @return The initial tibble with an addition column:\n#'  * `sd_ar1`: the rolling-window standard deviation of AR(1) residuals.\n#' \nsd_resid_scenario &lt;- function(tb) {\n  \n  # rolling window std of AR(1) residuals (window size = 102 quarters)\n  # This size corresponds to the size of our sample in the DSGE model.\n  roll_sd &lt;- slide(\n    .x = tb$pr_mean, \n    .f = ~ sd_resid_ar1(.x), \n    .before = 102, \n    .complete = FALSE # we return NAs if incomplete\n  )\n  \n  tb |&gt; \n    mutate(sd_ar1 = unlist(roll_sd)) |&gt; \n    filter(!is.na(sd_ar1))\n  \n}\n\n\n\n\n7.4.2 Rolling Windows\nWe compute the standard deviations of the AR(1) residuals on a rolling window of 25.5 years for each scenario (which corresponds to the length of the sample fed in the DSGE model (see Chapter 9). We loop over all the RCP scenarios (all the dataset except the dataset with historical values) and apply the sd_resid_scenario() function.\n\ntb_sd_scenarios &lt;- map(\n  precip_nat_q[names(precip_nat_q) != \"Historical\"], \n  ~sd_resid_scenario(\n    tb = bind_hist_and_rcp(hist_tb = precip_nat_q$Historical, rcp_tb = .x)\n  ), \n  .progress = TRUE\n)\n\n\n\nCodes to create the Figure.\np &lt;- ggplot(\n  data = list_rbind(tb_sd_scenarios, names_to = \"scenario\") |&gt; \n    filter(year &gt;= 2014),\n  mapping = aes(x = date, y = sd_ar1, colour = scenario)\n) +\n  geom_line() +\n  scale_colour_manual(NULL, values = colour_scenarios) +\n  labs(x = NULL, y = \"sd of the AR(1) residuals\") +\n  theme_paper()\n\np\n\n\n\n\n\nFigure 7.1: Visualization of the estimates sd of the residuals computed on the rolling windows\n\n\n\n\n\n\n\n\n\n\n7.4.3 Growth Rate of the Standard Deviation\nFor each scenario, we estimate a log–linear trend in the rolling standard deviations: \\[\n\\ln(\\widehat{\\sigma}_t) = \\alpha + \\beta t + u_t,\n\\] where \\(\\beta\\) gives theinstantaneous quarterly growth rate of the volatility of precipitation shocks.\nThis translates to long-run growth rates: \\[\n\\sigma_{i,\\eta^W} = e^{\\beta} - 1.\n\\]\nThe average growth over 1989–2100 can then be computed: \\[\n\\overline{\\Delta \\sigma_{i,\\eta^W}} = (1+\\sigma_{i,\\eta^W})^{q} - 1,\n\\quad q = 347\n\\]\nWe define the function compute_growth_stats() to compute the instantaneous and the long-run growth rates.\n\n#' Compute instantaneous, compound, and total growth rates of rolling residual \n#' volatility\n#'\n#' @description\n#' This function summarizes the time trend in the rolling standard deviations \n#' of AR(1) residuals from the precipitation data under a scenario. \n#' It estimates how the volatility of the underlying process evolves over time \n#' by regressing the (log of) fitted standard deviations on time.\n#'\n#' @param tb_sd A tibble containing the estimated rolling standard deviations \n#'  of residuals from AR(1) models (as produced by `sd_resid_scenario()`). It \n#'  must include the following columns:\n#'  * `year`: Calendar year.\n#'  * `sd_ar1`: Standard deviation of AR(1) residuals.\n#'  \n#' @returns A tibble with three summary statistics:\n#'   * `instant_growth`: Estimated instantaneous quarterly growth rate of \n#'      volatility (log-linear).\n#'   * `compound_growth`: Equivalent compound quarterly growth rate, computed \n#'   as \\eqn{e^{r} - 1}.\n#'   * `tot_growth`: Total cumulative growth (%) over the entire sample period \n#'   since 2014.\n#' \ncompute_growth_stats &lt;- function(tb_sd) {\n  \n  tb_sd &lt;- tb_sd |&gt; filter(year &gt;= 2014) |&gt; \n    mutate(t = row_number())\n  \n  q_total &lt;- nrow(tb_sd)\n  # regress ln(sd) on time\n  fit &lt;- lm(sd_ar1 ~ t, data = tb_sd)\n  tb_sd$fitted &lt;- fitted(fit)\n  \n  instant_growth &lt;- coef(lm(log(fitted)~ 1 + t, data = tb_sd))[2]\n  # Equivalently\n  compound_growth &lt;- exp(instant_growth) - 1\n  tot_growth &lt;- ((1 + compound_growth)^q_total - 1) * 100\n  \n  tibble(\n    instant_growth = instant_growth,\n    compound_growth = compound_growth,\n    tot_growth = tot_growth\n  )\n}\n\nWe use the compute_growth_stats() to estimate the quarterly rate of growth of the standard deviation of the weather measure and the corresponding average growth rate over the whole 1989–2100 period:\n\ngrowth_rates &lt;- map(tb_sd_scenarios, compute_growth_stats) |&gt; \n  bind_rows()\ngrowth_rates\n\n# A tibble: 4 × 3\n  instant_growth compound_growth tot_growth\n           &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1      -0.000116       -0.000116      -3.96\n2       0.000317        0.000317      11.6 \n3       0.000309        0.000309      11.3 \n4       0.000652        0.000653      25.5 \n\n\nThe values that can be used in the projection exercise with the DSGE model to represent the increase in the volatility of the weather shock:\n\n1 + growth_rates$tot_growth / 100\n\n        t         t         t         t \n0.9604073 1.1164497 1.1133924 1.2548524",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Projection</span>"
    ]
  },
  {
    "objectID": "var-estimation.html",
    "href": "var-estimation.html",
    "title": "8  VAR",
    "section": "",
    "text": "8.1 Settings\nWe will use the following variables in the VAR model.\nvariables_types &lt;-\n  matrix(\n    c(\"smdi_obs\", \"climate\",\n      \"wy_obs\", \"world\",\n      \"y_obs\", \"domestic\",\n      \"y_a_obs\", \"domestic\",\n      \"h_obs\", \"domestic\",\n      \"c_obs\", \"domestic\",\n      \"i_obs\", \"domestic\",\n      \"reer_obs\", \"domestic\"),\n    ncol = 2, byrow = T\n  )\nNote that their order is important. Also, we identified them with respect to the block they will be in:\nTo impose the restictions in the VAR, we will set to \\(0\\) some coefficients in the matrix of lag coefficients. \\[\n\\begin{equation}%\n\\begin{bmatrix}\n\\colorbox{wongPurple!17}{\\textbf{\\textcolor{wongPurple}{$X_t^W$}}}\\\\\n\\colorbox{wongOrange!17}{\\textbf{\\textcolor{wongOrange}{$X_t^\\star$}}}\\\\\n\\colorbox{wongBlue!17}{\\textbf{\\textcolor{wongBlue}{$X_{t}^{D}$}}}%\n\\end{bmatrix}\n=C+\\sum_{l=1}^{p}%\n\\begin{bmatrix}\nA_{l}^{11} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}}\\\\\n\\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}} & A_{l}^{22} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}}\\\\\nA_{l}^{31} & A_{l}^{32} & A_{l}^{33}%\n\\end{bmatrix}%\n\\begin{bmatrix}\n\\colorbox{wongPurple!17}{\\textbf{\\textcolor{wongPurple}{$X_{t-l}^W$}}}\\\\\n\\colorbox{wongOrange!17}{\\textbf{\\textcolor{wongOrange}{$X_{t-l}^{\\star}$}}}\\\\\n\\colorbox{wongBlue!17}{\\textbf{\\textcolor{wongBlue}{$X_{t-l}^{D}$}}}%\n\\end{bmatrix}\n+%\n\\begin{bmatrix}\n\\eta_{t}^{W}\\\\\n\\eta_{t}^{\\star}\\\\\n\\eta_{t}^{D}%\n\\end{bmatrix}\n,\n\\end{equation}\n\\tag{8.1}\\]\nLet us put the name of the exogenous variables in a vector of characters:\nvariables_exo &lt;- variables_types[,1]\nWe set the number of lags to 1:\nL &lt;- 1\nThe number of variables:\nN &lt;- length(variables_exo)\nWe define a function, find_block() to be able to return in which block a variable belongs to:\n#' Returns the corresponding name of the block for a variable\n#' \n#' @param name Variable name (string).\nfind_block &lt;- function(name) {\n  variables_types[variables_types[, 1] == name, 2]\n}\nFor example:\nfind_block(\"wy_obs\")\n\n[1] \"world\"\nWe create a matrix with the constraints on coefficients to ensure the exogeneity of blocks: \\[\n\\begin{bmatrix}\n    A_{l}^{11} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}}\\\\\n    \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}} & A_{l}^{22} & \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$0$}}}\\\\\n    A_{l}^{31} & A_{l}^{32} & A_{l}^{33}%\n\\end{bmatrix}\n\\tag{8.2}\\]\nThe equations will be written in columns in this matrix. Hence, at element \\([i,j]\\), the coefficient indicates whether the j\\(^\\text{th}\\) variable can affect the i\\(^\\text{th}\\) one. If so, the element \\([i,j]\\) will take the value 1; 0 otherwise.\nA_l_restrictions &lt;- \n  matrix(NA, ncol = N, nrow = N)\ncolnames(A_l_restrictions) &lt;- rownames(A_l_restrictions) &lt;- variables_exo\n\n\n# i_column &lt;- 1 ; i_row &lt;- 2\nfor (i_column in 1:nrow(A_l_restrictions)) {\n  name_eq &lt;- colnames(A_l_restrictions)[i_column]\n  \n  for (i_row in 1:ncol(A_l_restrictions)) {\n    nom_expl &lt;- rownames(A_l_restrictions)[i_row]\n    value_to_set &lt;- 1\n    # Climate block\n    if (find_block(name_eq) == \"climate\") {\n      if(find_block(nom_expl) != \"climate\") value_to_set &lt;- 0\n    }\n    \n    # Rest of the world block\n    if (find_block(name_eq) == \"world\") {\n      if(find_block(nom_expl) != \"world\") value_to_set &lt;- 0\n    }\n    \n    A_l_restrictions[i_row, i_column] &lt;- value_to_set\n  }\n}\n\n# The equations by rows:\nA_l_restrictions &lt;- t(A_l_restrictions)\nA_l_restrictions\n\n         smdi_obs wy_obs y_obs y_a_obs h_obs c_obs i_obs reer_obs\nsmdi_obs        1      0     0       0     0     0     0        0\nwy_obs          0      1     0       0     0     0     0        0\ny_obs           1      1     1       1     1     1     1        1\ny_a_obs         1      1     1       1     1     1     1        1\nh_obs           1      1     1       1     1     1     1        1\nc_obs           1      1     1       1     1     1     1        1\ni_obs           1      1     1       1     1     1     1        1\nreer_obs        1      1     1       1     1     1     1        1\nDepending on the number of desired lags, this matrix needs to be replicated so that all the constraints imposed apply to each lag. Again, equations are written in rows.\nA_l_restrict &lt;- parse(\n  text = str_c(\n    \"cbind(\",\n    str_c(rep(\"A_l_restrictions\", L), collapse = \", \"),\n    \")\"\n  )\n) |&gt; \n  eval()\n\nA_l_restrict &lt;- do.call(cbind, rep(list(A_l_restrictions), L))\nbase_nms &lt;- colnames(A_l_restrictions)\ncolnames(A_l_restrict) &lt;- paste0(\n  base_nms, \"_\", rep(seq_len(L), each = length(base_nms))\n)\n\n# Let us add an intercept (const)\nA_l_restrict &lt;- cbind(A_l_restrict, const = rep(1, nrow(A_l_restrict)))\nA_l_restrict\n\n         smdi_obs_1 wy_obs_1 y_obs_1 y_a_obs_1 h_obs_1 c_obs_1 i_obs_1\nsmdi_obs          1        0       0         0       0       0       0\nwy_obs            0        1       0         0       0       0       0\ny_obs             1        1       1         1       1       1       1\ny_a_obs           1        1       1         1       1       1       1\nh_obs             1        1       1         1       1       1       1\nc_obs             1        1       1         1       1       1       1\ni_obs             1        1       1         1       1       1       1\nreer_obs          1        1       1         1       1       1       1\n         reer_obs_1 const\nsmdi_obs          0     1\nwy_obs            0     1\ny_obs             1     1\ny_a_obs           1     1\nh_obs             1     1\nc_obs             1     1\ni_obs             1     1\nreer_obs          1     1",
    "crumbs": [
      "VAR",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>VAR</span>"
    ]
  },
  {
    "objectID": "var-estimation.html#sec-var-settions",
    "href": "var-estimation.html#sec-var-settions",
    "title": "8  VAR",
    "section": "",
    "text": "Weather block: drought index \\((\\hat{\\omega}_t)\\). The climate block is going to be exogenous.\nForeign block: rest-of-world GDP \\((\\hat{y}_t^\\star)\\). The variable that represents the rest of the world (here, the top trading partners). This block will not be influenced by the variations of the national weather, to reflect the idea that the country of interest is a small open economy.\nDomestic block: \\(\\{\\hat{y}_t, \\hat{y}_t^A, \\hat{\\imath}_t, \\hat{h}_t, \\hat{q}_t, \\widehat{rer}_t\\}\\).",
    "crumbs": [
      "VAR",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>VAR</span>"
    ]
  },
  {
    "objectID": "var-estimation.html#load-data",
    "href": "var-estimation.html#load-data",
    "title": "8  VAR",
    "section": "8.2 Load Data",
    "text": "8.2 Load Data\nLet us load the data obtained gathered in Chapter 6.\n\nload('../data/df_finale_ebook.rda')\n\nTotal production is denoted y_tot_obs in the base we just loaded, let us change this to simply y_obs. Then, let us keep only the variables of interest.\n\ndf_finale &lt;-\n  df_finale |&gt; \n  mutate(\n    y_obs = y_tot_obs) |&gt; \n  dplyr::select(YEARS, !!variables_exo)\n\n\n\n\n\n\n\nWarning\n\n\n\nIn Chapter 6, we made sure that positive values of the Soil Moisture Deficit index depict droughts. In the impulse response functions, we will impulse positive standard deviation shocks. This ensures that a positive shock depicts an increase in dryness.\n\n\nWe know there is no missing values between different dates so, we can use na.omit() without risk. There is only one missing value for the reer.\n\ndf_finale &lt;- na.omit(df_finale)\n\nIt will be more convenient to cast the series into a time series object (of class ts).\n\nstart_date_raw_data &lt;- c(\n  df_finale$YEARS[1] %/% 1,\n  df_finale$YEARS[1] %% 1 * 4 + 1\n)\nend_date_raw_data &lt;- c(\n  df_finale$YEARS[nrow(df_finale)] %/% 1,\n  df_finale$YEARS[nrow(df_finale)] %% 1 * 4 + 1\n)\nfrequency_raw_data &lt;- 4 # quarterly\n\nstart_date_sample &lt;- c(\n  df_finale$YEARS[1] %/% 1,\n  df_finale$YEARS[1] %% 1 * 4 + 1\n)\nend_date_sample &lt;- end_date_raw_data\n\n# Turning it to a ts object\nraw_data &lt;- \n  df_finale |&gt; \n  dplyr::select(!!variables_exo) |&gt; \n  ts(frequency = frequency_raw_data, start = start_date_raw_data)\n\nLet us call the data used in the estimation \"data\":\n\ndata &lt;- raw_data\n\n\ncor(data)\n\n            smdi_obs      wy_obs       y_obs     y_a_obs       h_obs\nsmdi_obs  1.00000000  0.19606076 -0.19660938 -0.11263581 -0.14052625\nwy_obs    0.19606076  1.00000000  0.25998532  0.07661121  0.11983123\ny_obs    -0.19660938  0.25998532  1.00000000  0.08190103  0.24652057\ny_a_obs  -0.11263581  0.07661121  0.08190103  1.00000000 -0.03260390\nh_obs    -0.14052625  0.11983123  0.24652057 -0.03260390  1.00000000\nc_obs     0.07587367  0.28990378  0.28920644  0.49468434 -0.07365197\ni_obs    -0.12784838  0.54658438  0.70161881  0.17183983  0.29588680\nreer_obs -0.12222779 -0.33922581  0.09042631 -0.12259028  0.07331378\n               c_obs      i_obs    reer_obs\nsmdi_obs  0.07587367 -0.1278484 -0.12222779\nwy_obs    0.28990378  0.5465844 -0.33922581\ny_obs     0.28920644  0.7016188  0.09042631\ny_a_obs   0.49468434  0.1718398 -0.12259028\nh_obs    -0.07365197  0.2958868  0.07331378\nc_obs     1.00000000  0.2634593 -0.07931696\ni_obs     0.26345930  1.0000000 -0.14470007\nreer_obs -0.07931696 -0.1447001  1.00000000",
    "crumbs": [
      "VAR",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>VAR</span>"
    ]
  },
  {
    "objectID": "var-estimation.html#estimation",
    "href": "var-estimation.html#estimation",
    "title": "8  VAR",
    "section": "8.3 Estimation",
    "text": "8.3 Estimation\nWe will use the functions from {vars} to estimate the VAR (note that it masks the function select() from {dplyr}, which leads us to use dplyr::select() afterwards, if needed).\n\nlibrary(vars)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: strucchange\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\n\nAttaching package: 'strucchange'\n\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\n\nLoading required package: urca\n\n\nLoading required package: lmtest\n\n\nThe function VARselect() estimates infomation criteria and final prediction error for sequential increasing the lag order up to a VAR(p)-proccess.\n\nVARselect(data, lag.max = 4, type = \"const\")\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     2      1      1      2 \n\n$criteria\n                1         2          3          4\nAIC(n)   4.730414  4.486327   4.559102   4.731252\nHQ(n)    5.557377  6.048368   6.856220   7.763449\nSC(n)    6.785217  8.367621  10.266886  12.265528\nFPE(n) 114.042079 92.621039 109.638606 157.964462\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the number of lags that produces the best fit with respect to the criterion you want to use is different from 1, set the value to L to the desired value in Section 8.1, and run the above codes again.\n\n\n\n8.3.1 Restricted VAR\nWe first need to estimate the VAR without constraints, i.e., without imposing \\(0\\) coefficients in the matrix shown in Equation 8.2:\n\nvar_1 &lt;- VAR(data, p = L, type = \"const\")\n\nThe restrictions can be added to estimate the model shown in Equation 8.1:\n\nvar_res &lt;- restrict(var_1, method = \"manual\", resmat = A_l_restrict)\n\n\n\n8.3.2 Structural VAR\nWe further need to impose structure on the contemporaneous relationships amont variables. We estimate a Structural VAR (SVAR) model to do so: \\[\n\\begin{equation*}\n    \\colorbox{wongGreen!17}{\\textbf{\\textcolor{wongGreen}{$A_{0}$}}} X_{t} = C + \\sum_{l=1}^{p} A_{l} X_{t-l} + \\colorbox{wongGold!17}{\\textbf{\\textcolor{wongGold}{$\\eta_{t}$}}},\n\\end{equation*}\n\\tag{8.3}\\]\nThe matrix \\(\\textcolor{wongGreen}{A_0}\\) is a lower triangular matrix which encodes the contemporaneous restrictions, and \\(\\textcolor{wongGold}{\\eta_{t}}\\) are the orthogonal structural shocks. It ensures the contemporaneous exogeneity of the weather and of foreign variables. Those variables do not react within the quarter to domestic shocks, while domestic variables may react instantly to them. The order of the variables matters. Variables ordered earlier can contemporaneously affect variables ordered later, but not the other way around. In addition, we put 1s on the diagonal to fix the scale of each equation so that the shocks \\(\\eta_t\\) are structural shocks, i.e., orthogonal and scaled, which is required to compute the IRFs.\nThe SVAR model can be estimate, by specifying the \\(A_0\\) matrix. Each row of this matrix sets the constraints for an equation of the previously estimated VAR. \\[\n\\begin{equation*}\n  {\\color{wongGreen}A_{0}}=\n  \\begin{bmatrix}\n  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  b_{31} & b_{32} & 1 & 0 & 0 & 0 & 0 & 0\\\\\n  b_{41} & b_{42} & b_{43} & 1 & 0 & 0 & 0 & 0\\\\\n  b_{51} & b_{52} & b_{53} & b_{54} & 1 & 0 & 0 & 0\\\\\n  b_{61} & b_{62} & b_{63} & b_{64} & b_{65} & 1 & 0 & 0\\\\\n  b_{71} & b_{72} & b_{73} & b_{74} & b_{75} & b_{76} & 1 & 0\\\\\n  b_{81} & b_{82} & b_{83} & b_{84} & b_{85} & b_{86} & b_{87} & 1\n  \\end{bmatrix}\n\\end{equation*}\n\\tag{8.4}\\]\nFor example, the 4th row (row for y_a_obs) states that the agricultural output can contemporaneously depend only on the weather (first column), foreign GDP (second column) and domestic output (third column).\n\namat &lt;- A_l_restrictions\namat[amat == 1] &lt;- NA\namat[upper.tri(amat)] &lt;- 0\ndiag(amat) &lt;- 1\namat\n\n         smdi_obs wy_obs y_obs y_a_obs h_obs c_obs i_obs reer_obs\nsmdi_obs        1      0     0       0     0     0     0        0\nwy_obs          0      1     0       0     0     0     0        0\ny_obs          NA     NA     1       0     0     0     0        0\ny_a_obs        NA     NA    NA       1     0     0     0        0\nh_obs          NA     NA    NA      NA     1     0     0        0\nc_obs          NA     NA    NA      NA    NA     1     0        0\ni_obs          NA     NA    NA      NA    NA    NA     1        0\nreer_obs       NA     NA    NA      NA    NA    NA    NA        1\n\n\n: we enforce weather/foreign as exogenous blocks in the lag structure.\n:\n: \\begin{itemize}\nto have 1 s.d. structural shock in the IRFs\nThe estimation of the SVAR:\n\nsvar_est &lt;- SVAR(x = var_res, Amat = amat, Bmat = NULL, estmethod = \"direct\")\n\n\n\n\n\n\n\nEstimation Results\n\n\n\n\n\nThe estimation results:\n\nsummary(svar_est)\n\n\nSVAR Estimation Results:\n======================== \n\nCall:\nSVAR(x = var_res, estmethod = \"direct\", Amat = amat, Bmat = NULL)\n\nType: A-model \nSample size: 89 \nLog Likelihood: -1010.284 \nMethod: direct \nNumber of iterations: 502 \nConvergence code: 1 \n\nLR overidentification test:\n\n    LR overidentification\n\ndata:  data\nChi^2 = -316, df = 9, p-value = 1\n\n\nEstimated A matrix:\n         smdi_obs  wy_obs   y_obs  y_a_obs  h_obs  c_obs i_obs reer_obs\nsmdi_obs  1.00000 0.00000 0.00000  0.00000 0.0000 0.0000 0.000        0\nwy_obs    0.00000 1.00000 0.00000  0.00000 0.0000 0.0000 0.000        0\ny_obs     0.06329 0.20378 1.00000  0.00000 0.0000 0.0000 0.000        0\ny_a_obs   0.10282 0.14097 0.11542  1.00000 0.0000 0.0000 0.000        0\nh_obs     0.17090 0.06810 0.10482  0.09727 1.0000 0.0000 0.000        0\nc_obs     0.12516 0.12901 0.11598  0.09116 0.1129 1.0000 0.000        0\ni_obs     0.13012 0.11064 0.01358 -0.13792 0.1071 0.1097 1.000        0\nreer_obs  0.12177 0.08929 0.09664  0.11193 0.1280 0.0870 0.102        1\n\nEstimated B matrix:\n         smdi_obs wy_obs y_obs y_a_obs h_obs c_obs i_obs reer_obs\nsmdi_obs        1      0     0       0     0     0     0        0\nwy_obs          0      1     0       0     0     0     0        0\ny_obs           0      0     1       0     0     0     0        0\ny_a_obs         0      0     0       1     0     0     0        0\nh_obs           0      0     0       0     1     0     0        0\nc_obs           0      0     0       0     0     1     0        0\ni_obs           0      0     0       0     0     0     1        0\nreer_obs        0      0     0       0     0     0     0        1\n\nCovariance matrix of reduced form residuals (*100):\n         smdi_obs  wy_obs   y_obs y_a_obs   h_obs   c_obs   i_obs reer_obs\nsmdi_obs  100.000   0.000  -6.329  -9.551 -15.498  -9.161 -11.578   -6.535\nwy_obs      0.000 100.000 -20.378 -11.745  -3.532  -9.068 -11.034   -3.279\ny_obs      -6.329 -20.378 104.553  -8.544  -7.659  -7.061   2.075   -5.174\ny_a_obs    -9.551 -11.745  -8.544 103.624  -6.752  -4.982  18.220   -9.122\nh_obs     -15.498  -3.532  -7.659  -6.752 104.349  -7.886  -8.732   -8.078\nc_obs      -9.161  -9.068  -7.061  -4.982  -7.886 104.480  -9.013   -3.996\ni_obs     -11.578 -11.034   2.075  18.220  -8.732  -9.013 107.136   -8.871\nreer_obs   -6.535  -3.279  -5.174  -9.122  -8.078  -3.996  -8.871  104.896\n\n\n\n\n\nThe data can be exported for use in the DSGE\n\nwrite_delim(df_finale, file = \"../data/df_finale_var_ebook.csv\", delim = \";\")\n\nWe can have a look at the observed weather series and the fitted one:\n\nggplot(\n  data = tibble(\n    time = zoo::as.Date(time(var_res$y))[-(1: var_res$p)],\n    obs =  var_res$datamat$smdi_obs,\n    fitted = fitted(var_res$varresult$smdi_obs)\n  ) |&gt; \n    pivot_longer(cols = -time, names_to = \"type\") |&gt; \n    mutate(type = factor(type, levels = c(\"obs\", \"fitted\"))),\n  mapping = aes(x = time, y = value, colour = type)\n) +\n  geom_line() +\n  scale_colour_manual(\n    name = NULL, values = c(\"obs\" = \"black\", \"fitted\" = \"blue\")\n  )\n\n\n\n\n\n\n\n\n\n\n8.3.3 Fisher Test\nWe use a nested-model F-test to assess whether the weather variable (smdi_obs) improves fit for each target variable (output, agri output, hours, consumption, investment, REER).\nTo do so, we define a function, fisher_test(), that builds two linear regressions:\n\nA complete model, which includes the target’s admissible contemporaneous regressors (from the SVAR \\(\\color{wongGreen}A_0\\) pattern in amat, if contemp = TRUE) and admissible lagged regressors (from the VAR mask A_l_restrict).\n\n\nA restricted model, with the same specification, but with smdi_obs removed (both its contemporaneous and/or lag terms, depending on the setting).\n\nIt then compares the residual sums of squares (RSS) to form \\[\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_C)/k}{\\text{RSS}_C/(n - p - k)},\n\\tag{8.5}\\] where \\(k\\) is the number of coefficients dropped and \\(p\\) the number in the restricted model. A small p-value indicates that weather adds information.\n\n\nThe fisher_test() function.\n#' Fisher Test for the Inclusion of the Weather Variable\n#' \n#' @description\n#' Performs a nested-model Fisher test to evaluate whether including the weather\n#' variable (`smdi_obs`) improves the fit of an equation in the restricted \n#' VAR/SVAR system.\n#' The function compares a complete model (including all admissible \n#' contemporaneous and lagged regressors) to a restricted model where the \n#' weather variable (and optionally its lags) is removed.\n#' \n#' @param variables_rest Character vector of variables included in the baseline\n#'  (restricted) model. Include the weather variable `\"smdi_obs\"` and possibly \n#'  other variables.\n#' @param variables_interest Character string giving the dependent variable \n#'  to test.\n#' @param print_res If `TRUE` (default), the equations of the models and the\n#'  summary of the results are printed in the console.\n#' @param contemp If `TRUE` (default), for the target equation (row in `amat`), \n#'  any entry that is `NA` (i.e., free to estimate) is included; zeros are\n#'  excluded. If `FALSE`, only lagged effects are considered.\n#'  \n#' @details\n#' The admissible regressors are determined from the restriction matrices:\n#' * The contemporaneous matrix `amat` (from the SVAR): if a coefficient\n#'   is `NA` in the row corresponding to the dependent variable, the associated\n#'   variable is included.\n#' * The lag restriction matrix _A_l_restrict_ (from the restricted VAR):\n#'   if the entry equals `1` in that row, the corresponding lagged variable\n#'   is included.\n#' For each dependent variable, two linear models are estimated:\n#' * Complete model: includes all admissible contemporaneous and lagged \n#'   regressors.\n#' * Restricted model: same as the complete model but with the weather variable\n#'   (and its lags, if any) removed.\n#'   \n#' The Fisher test statistic is computed as:\n#' \\deqn{\n#' F = \\frac{(RSS_R - RSS_C)/k}{RSS_C / (n - p - k)},\n#' }\n#' where \\eqn{RSS_R} and \\eqn{RSS_C} are the residual sums of squares of the\n#' restricted and complete models, respectively; \\eqn{k} is the number of\n#' restrictions, and \\eqn{p} is the number of estimated parameters in the\n#' restricted model.\n#' \n#' @returns  A tibble with the following columns:\n#' * `var`: tested dependent variable,\n#' * `F`: computed F-statistic,\n#' * `Pr(&gt;F)`: associated p-value.\n#' \nfisher_test &lt;- function(variables_rest, \n                        variables_interest, \n                        print_res = TRUE, \n                        contemp = TRUE) {\n  \n  variables &lt;- c(variables_rest, variables_interest)\n  \n  # Complete model\n  data_tmp &lt;- as_tibble(data)\n  data_tmp &lt;- \n    data_tmp |&gt; \n    dplyr::select(!!variables)\n  \n  form &lt;- str_c(variables_interest, \" ~ 1\")\n  \n  # i &lt;- 1\n  for (i in 1:length(variables)) {\n    var_name &lt;- variables[i]\n    var_name_new &lt;- str_c(variables[i], \"_1\")\n    \n    # Contemporaneous effect\n    if (contemp) {\n      ind &lt;- which(rownames(amat) == variables_interest)\n      if (is.na(amat[ind, which(colnames(amat) == var_name)])) {\n        form &lt;- str_c(form, var_name, sep = \" + \")\n      }\n    }\n    \n    # Lagged effect\n    ind &lt;- which(rownames(A_l_restrict) == variables_interest)\n    \n    if (A_l_restrict[ind, which(colnames(A_l_restrict) == var_name_new)] == 1) {\n      data_tmp &lt;- \n        data_tmp |&gt; \n        mutate(!!var_name_new := dplyr::lag(!!sym(var_name),1))\n      \n      form &lt;- str_c(form, var_name_new, sep = \" + \")\n    }\n  }\n  \n  mod_complete &lt;- lm(as.formula(form), data = data_tmp)\n  \n  # Restricted model\n  variables_rest_r &lt;- variables_rest[-which(variables_rest == \"smdi_obs\")]\n  variables_r &lt;- c(variables_rest_r, variables_interest)\n  \n  data_tmp_r &lt;- as_tibble(data) |&gt; \n    dplyr::select(c(!!variables_rest_r, !!variables_interest))\n  \n  form_r &lt;- str_c(variables_interest, \" ~ 1\")\n  \n  # i &lt;- 1\n  for (i in 1:length(variables_r)) {\n    var_name &lt;- variables_r[i]\n    var_name_new &lt;- str_c(variables_r[i], \"_1\")\n    \n    # Contemporaneous effect\n    if (contemp) {\n      ind &lt;- which(rownames(amat) == variables_interest)\n      if (is.na(amat[ind, which(colnames(amat) == var_name)])) {\n        form_r &lt;- str_c(form_r, var_name, sep = \" + \")\n      }\n    }\n    \n    # Lagged effect\n    if (A_l_restrict[ind, which(colnames(A_l_restrict) == var_name_new)] == 1) {\n      data_tmp_r &lt;- \n        data_tmp_r |&gt; \n        mutate(!!var_name_new := dplyr::lag(!!sym(var_name),1))\n      \n      form_r &lt;- str_c(form_r, var_name_new, sep = \" + \")\n    }\n  }\n  \n  mod_restricted &lt;- lm(as.formula(form_r), data = data_tmp_r)\n  \n  rss_r &lt;- sum(mod_restricted$residuals^2)\n  rss_c &lt;- sum(mod_complete$residuals^2)\n  const &lt;- 0\n  p &lt;- mod_restricted$rank-const\n  k &lt;- mod_complete$rank - const - p\n  n &lt;- length(mod_restricted$residuals)\n  F_obs &lt;- ((rss_r - rss_c) / k) / (rss_c / (n - (k+p+const)))\n  (F_tab &lt;- qf(p = 1-0.05, df1 = p, df2 = n - (k+p+1)))\n  p_value &lt;- (1 - pf(q = F_obs, df1 = p, df2 = n - (k+p+const)))\n  \n  cat(\"Complete model:\\n---------------\\n\")\n  cat(form)\n  cat(\"\\n\\nRestricted model:\\n---------------\\n\")\n  cat(form_r)\n  cat(\"\\n\")\n  if(print_res)\n    texreg::screenreg(\n      l = list(mod_complete, mod_restricted),\n      custom.model.names = c(\"Complete\", \"Restricted\")\n    ) |&gt; \n    print()\n  \n  tibble(var = !!variables_interest, F = F_obs, `Pr(&gt;F)` = p_value)\n}\n\n\nWe consider three testing setups:\n\nBivariate, past + current: we test whether weather (contemporaneous and its lags, as allowed) improves a bivariate regression (target on itself and weather).\nMultivariate, past only: we test whether lagged weather adds predictive power beyond the full set of other lagged variables.\nMultivariate, past + current: we test whether both contemporaneous and lagged weather terms improve fit in the SVAR-consistent equation.\n\n\nvariables_rest &lt;- c(\"smdi_obs\")\n\n\n\n\n\n\n\nBivariate, past + current\n\n\n\n\n\n\nf_test &lt;- NULL\nfor (v in c(\"y_obs\", \"y_a_obs\", \"h_obs\", \"c_obs\", \"i_obs\", \"reer_obs\")) {\n  f_test_tmp &lt;- fisher_test(\n    variables_rest = variables_rest,\n    variables_interest = v, print_res = F\n  )\n  f_test &lt;- f_test |&gt; \n    bind_rows(f_test_tmp)\n}\n\nComplete model:\n---------------\ny_obs ~ 1 + smdi_obs + smdi_obs_1 + y_obs_1\n\nRestricted model:\n---------------\ny_obs ~ 1 + y_obs_1\nComplete model:\n---------------\ny_a_obs ~ 1 + smdi_obs + smdi_obs_1 + y_a_obs_1\n\nRestricted model:\n---------------\ny_a_obs ~ 1 + y_a_obs_1\nComplete model:\n---------------\nh_obs ~ 1 + smdi_obs + smdi_obs_1 + h_obs_1\n\nRestricted model:\n---------------\nh_obs ~ 1 + h_obs_1\nComplete model:\n---------------\nc_obs ~ 1 + smdi_obs + smdi_obs_1 + c_obs_1\n\nRestricted model:\n---------------\nc_obs ~ 1 + c_obs_1\nComplete model:\n---------------\ni_obs ~ 1 + smdi_obs + smdi_obs_1 + i_obs_1\n\nRestricted model:\n---------------\ni_obs ~ 1 + i_obs_1\nComplete model:\n---------------\nreer_obs ~ 1 + smdi_obs + smdi_obs_1 + reer_obs_1\n\nRestricted model:\n---------------\nreer_obs ~ 1 + reer_obs_1\n\n\n\n\n\n\n\n\n\n\n\nMultivariate, past only\n\n\n\n\n\n\nvariables_rest &lt;- c(\n  \"smdi_obs\", \"wy_obs\", \"y_obs\", \"y_a_obs\", \"h_obs\", \"c_obs\", \"i_obs\", \"reer_obs\"\n)\nf_test_2 &lt;- NULL\nfor (v in c(\"y_obs\", \"y_a_obs\", \"h_obs\", \"c_obs\", \"i_obs\", \"reer_obs\")) {\n  f_test_tmp &lt;- fisher_test(\n    variables_rest = variables_rest,\n    variables_interest = v, print_res = F, contemp = F\n  )\n  f_test_2 &lt;- f_test_2 |&gt; bind_rows(f_test_tmp)\n}\n\nComplete model:\n---------------\ny_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_obs_1\n\nRestricted model:\n---------------\ny_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_obs_1\nComplete model:\n---------------\ny_a_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_a_obs_1\n\nRestricted model:\n---------------\ny_a_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_a_obs_1\nComplete model:\n---------------\nh_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + h_obs_1\n\nRestricted model:\n---------------\nh_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + h_obs_1\nComplete model:\n---------------\nc_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + c_obs_1\n\nRestricted model:\n---------------\nc_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + c_obs_1\nComplete model:\n---------------\ni_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + i_obs_1\n\nRestricted model:\n---------------\ni_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + i_obs_1\nComplete model:\n---------------\nreer_obs ~ 1 + smdi_obs_1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + reer_obs_1\n\nRestricted model:\n---------------\nreer_obs ~ 1 + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + reer_obs_1\n\n\n\n\n\n\n\n\n\n\n\nMultivariate, past + current\n\n\n\n\n\n\nvariables_rest &lt;- c(\n  \"smdi_obs\", \"wy_obs\", \"y_obs\", \"y_a_obs\", \"h_obs\", \"c_obs\", \"i_obs\", \"reer_obs\"\n)\nf_test_3 &lt;- NULL\nfor (v in c(\"y_obs\", \"y_a_obs\", \"h_obs\", \"c_obs\", \"i_obs\", \"reer_obs\")) {\n  f_test_tmp &lt;- fisher_test(\n    variables_rest = variables_rest,\n    variables_interest = v, print_res = F\n  )\n  f_test_3 &lt;- f_test_3 |&gt; bind_rows(f_test_tmp)\n}\n\nComplete model:\n---------------\ny_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_obs_1\n\nRestricted model:\n---------------\ny_obs ~ 1 + wy_obs + wy_obs_1 + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_obs_1\nComplete model:\n---------------\ny_a_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_a_obs_1\n\nRestricted model:\n---------------\ny_a_obs ~ 1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + y_a_obs_1\nComplete model:\n---------------\nh_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + h_obs_1\n\nRestricted model:\n---------------\nh_obs ~ 1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + h_obs_1\nComplete model:\n---------------\nc_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + c_obs_1\n\nRestricted model:\n---------------\nc_obs ~ 1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs_1 + i_obs_1 + reer_obs_1 + c_obs_1\nComplete model:\n---------------\ni_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs + c_obs_1 + i_obs_1 + reer_obs_1 + i_obs_1\n\nRestricted model:\n---------------\ni_obs ~ 1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs + c_obs_1 + i_obs_1 + reer_obs_1 + i_obs_1\nComplete model:\n---------------\nreer_obs ~ 1 + smdi_obs + smdi_obs_1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs + c_obs_1 + i_obs + i_obs_1 + reer_obs_1 + reer_obs_1\n\nRestricted model:\n---------------\nreer_obs ~ 1 + wy_obs + wy_obs_1 + y_obs + y_obs_1 + y_a_obs + y_a_obs_1 + h_obs + h_obs_1 + c_obs + c_obs_1 + i_obs + i_obs_1 + reer_obs_1 + reer_obs_1\n\n\n\n\n\nThe results:\n\n\nCodes to create the Table.\nf_test |&gt; \n  left_join(\n    f_test_2 |&gt; \n      rename(F_2 = F, `Pr(&gt;F)_2` = `Pr(&gt;F)`), \n    by = \"var\"\n  ) |&gt; \n  left_join(\n    f_test_3 |&gt; \n      rename(F_3 = F, `Pr(&gt;F)_3` = `Pr(&gt;F)`), \n      by = \"var\"\n  ) |&gt; \n  kableExtra::kbl(digits = 2) |&gt; \n  kableExtra::kable_paper(\"hover\") |&gt; \n  kableExtra::add_header_above(\n    c(\n      \" \" = 1, \n      \"bivariate\\n past only \" = 2, \n      \"multivariate\\n past only \" = 2, \n      \"multivariate\\n past + current\" = 2\n    )\n  )\n\n\n\n\nTable 8.1: Inclusion tests of the (lagged) weather variable (F-test for nested models).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbivariate\npast only\n\n\nmultivariate\npast only\n\n\nmultivariate\npast + current\n\n\n\nvar\nF\nPr(&gt;F)\nF_2\nPr(&gt;F)_2\nF_3\nPr(&gt;F)_3\n\n\n\n\ny_obs\n2.86\n0.06\n3.34\n0.00\n2.53\n0.01\n\n\ny_a_obs\n2.57\n0.08\n6.38\n0.00\n3.22\n0.00\n\n\nh_obs\n0.88\n0.42\n0.05\n1.00\n0.63\n0.80\n\n\nc_obs\n0.24\n0.78\n0.39\n0.92\n0.34\n0.98\n\n\ni_obs\n0.61\n0.54\n2.12\n0.04\n0.31\n0.99\n\n\nreer_obs\n2.21\n0.12\n1.52\n0.16\n1.61\n0.10",
    "crumbs": [
      "VAR",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>VAR</span>"
    ]
  },
  {
    "objectID": "var-estimation.html#sec-var-irfs",
    "href": "var-estimation.html#sec-var-irfs",
    "title": "8  VAR",
    "section": "8.4 IRFs",
    "text": "8.4 IRFs\nWe now turn to the impulse response functions. We set the number of lags for the impulse response analysis to 30.\n\nlast_lag &lt;- 30\n\nTo calculate confidence intervals, we perform Monte Carlo simulations, with 10,000 runs.\n\nruns &lt;- 10000\n\n\n\nThe irf_varest(), irf_internal(), and boot_internal() functions.\n# This set of functions is the replication of the code found in the function\n# irf.varest package vars to compute the IRFs.\n# But we correct it so that it works with restricted vars.\n\n#' irf_varest\nirf_varest &lt;- function(x,\n                       impulse = NULL,\n                       response = NULL,\n                       n.ahead = 10,\n                       ortho = TRUE,\n                       cumulative = FALSE,\n                       boot = TRUE,\n                       ci = 0.95,\n                       runs = 100,\n                       seed = NULL,\n                       restrict = NULL, ...) {\n  if (!(class(x) == \"varest\")) {\n    stop(\"\\nPlease provide an object of class 'varest', generated by 'VAR()'.\\n\")\n  }\n  y.names &lt;- colnames(x$y)\n  if (is.null(impulse)) {\n    impulse &lt;- y.names\n  } else {\n    impulse &lt;- as.vector(as.character(impulse))\n    if(any(!(impulse %in% y.names))) {\n      stop(\"\\nPlease provide variables names in impulse\\nthat are in the set of endogenous variables.\\n\")\n    }\n    impulse &lt;- subset(y.names, subset = y.names %in% impulse)\n  }\n  if (is.null(response)) {\n    response &lt;- y.names\n  } else {\n    response &lt;- as.vector(as.character(response))\n    if(any(!(response %in% y.names))){\n      stop(\"\\nPlease provide variables names in response\\nthat are in the set of endogenous variables.\\n\")\n    }\n    response &lt;- subset(y.names, subset = y.names %in% response)\n  }\n  ## Getting the irf\n  irs &lt;- irf_internal(\n    x = x,\n    impulse = impulse,\n    response = response,\n    y.names = y.names,\n    n.ahead = n.ahead,\n    ortho = ortho,\n    cumulative = cumulative\n  )\n  ## Bootstrapping\n  Lower &lt;- NULL\n  Upper &lt;- NULL\n  std_err &lt;- NULL\n  if (boot) {\n    ci &lt;- as.numeric(ci)\n    if((ci &lt;= 0)|(ci &gt;= 1)){\n      stop(\"\\nPlease provide a number between 0 and 1 for the confidence interval.\\n\")\n    }\n    ci &lt;- 1 - ci\n    BOOT &lt;- boot_internal(x = x, n.ahead = n.ahead, runs = runs, ortho = ortho, cumulative = cumulative, impulse = impulse, response = response, ci = ci, seed = seed, y.names = y.names, restrict = restrict)\n    Lower &lt;- BOOT$Lower\n    Upper &lt;- BOOT$Upper\n    std_err &lt;- BOOT$sd\n  }\n  result &lt;- list(\n    irf = irs,\n    Lower = Lower,\n    Upper = Upper,\n    sd = std_err,\n    response = response,\n    impulse = impulse,\n    ortho = ortho,\n    cumulative = cumulative,\n    runs = runs, ci = ci,\n    boot = boot,\n    model = class(x)\n  )\n  class(result) &lt;- \"varirf\"\n\n  return(result)\n}\n\n\n#' irf (internal)\nirf_internal &lt;- function(x,\n                         impulse,\n                         response,\n                         y.names,\n                         n.ahead,\n                         ortho,\n                         cumulative) {\n  if ((class(x) == \"varest\") || (class(x) == \"vec2var\")) {\n    if(ortho){\n      irf &lt;- Psi(x, nstep = n.ahead)\n    } else {\n      irf &lt;- Phi(x, nstep = n.ahead)\n    }\n  } else if ((class(x) == \"svarest\") || (class(x) == \"svecest\")) {\n    irf &lt;- Phi(x, nstep = n.ahead)\n  }\n  dimnames(irf) &lt;- list(y.names, y.names, NULL)\n  idx &lt;- length(impulse)\n  irs &lt;- list()\n  for (i in 1 : idx) {\n    irs[[i]] &lt;- matrix(\n      t(irf[response , impulse[i], 1 : (n.ahead + 1)]),\n      nrow = n.ahead+1\n    )\n    colnames(irs[[i]]) &lt;- response\n    if (cumulative) {\n      if(length(response) &gt; 1) irs[[i]] &lt;- apply(irs[[i]], 2, cumsum)\n      if(length(response) == 1){\n        tmp &lt;- matrix(cumsum(irs[[1]]))\n        colnames(tmp) &lt;- response\n        irs[[1]] &lt;- tmp\n      }\n    }\n  }\n  names(irs) &lt;- impulse\n  result &lt;- irs\n\n  return(result)\n}\n\n\n#' boot_internal\n#' Bootstrapping IRF for VAR and SVAR\n#'\nboot_internal &lt;- function(x,\n                          n.ahead,\n                          runs,\n                          ortho,\n                          cumulative,\n                          impulse,\n                          response,\n                          ci,\n                          seed,\n                          y.names,\n                          restrict = NULL) {\n  if (!(is.null(seed))) set.seed(abs(as.integer(seed)))\n  if (class(x) == \"varest\") {\n    VAR &lt;- eval.parent(x)\n  } else if (class(x) == \"svarest\") {\n    VAR &lt;- eval.parent(x$var)\n  } else {\n    stop(\"Bootstrap not implemented for this class.\\n\")\n  }\n  p &lt;- VAR$p\n  K &lt;- VAR$K\n  obs &lt;- VAR$obs\n  total &lt;- VAR$totobs\n  type &lt;- VAR$type\n  B &lt;- Bcoef(VAR)\n  BOOT &lt;- vector(\"list\", runs)\n  ysampled &lt;- matrix(0, nrow = total, ncol = K)\n  colnames(ysampled) &lt;- colnames(VAR$y)\n  Zdet &lt;- NULL\n  if (ncol(VAR$datamat) &gt; (K * (p+1))) {\n    Zdet &lt;- as.matrix(VAR$datamat[, (K * (p + 1) + 1):ncol(VAR$datamat)])\n  }\n  resorig &lt;- scale(resid(VAR), scale = FALSE)\n  B &lt;- Bcoef(VAR)\n  for (i in 1:runs) {\n    booted &lt;- sample(c(1 : obs), replace=TRUE)\n    resid &lt;- resorig[booted, ]\n    lasty &lt;- c(t(VAR$y[p : 1, ]))\n    ysampled[c(1 : p), ] &lt;- VAR$y[c(1 : p), ]\n    for(j in 1 : obs){\n      lasty &lt;- lasty[1 : (K * p)]\n      Z &lt;- c(lasty, Zdet[j, ])\n      ysampled[j + p, ] &lt;- B %*% Z + resid[j, ]\n      lasty &lt;- c(ysampled[j + p, ], lasty)\n    }\n\n    varboot &lt;- update(VAR, y = ysampled)\n    if (!is.null(restrict))\n      varboot &lt;- restrict(varboot, method = \"man\", resmat = restrict)\n    if (class(x) == \"svarest\") {\n      varboot &lt;- update(x, x = varboot)\n    }\n    BOOT[[i]] &lt;- irf_internal(\n      x = varboot,\n      n.ahead = n.ahead,\n      ortho = ortho,\n      cumulative = cumulative,\n      impulse = impulse,\n      response = response,\n      y.names = y.names\n    )\n  }\n  lower &lt;- ci / 2\n  upper &lt;- 1 - ci / 2\n  mat.l &lt;- matrix(NA, nrow = n.ahead + 1, ncol = length(response))\n  mat.u &lt;- matrix(NA, nrow = n.ahead + 1, ncol = length(response))\n  mat.sd &lt;- matrix(NA, nrow = n.ahead + 1, ncol = length(response))\n  Lower &lt;- list()\n  Upper &lt;- list()\n  std_err &lt;- list()\n  idx1 &lt;- length(impulse)\n  idx2 &lt;- length(response)\n  idx3 &lt;- n.ahead + 1\n  temp &lt;- rep(NA, runs)\n  for (j in 1 : idx1) {\n    for (m in 1 : idx2) {\n      for (l in 1 : idx3) {\n        for (i in 1 : runs) {\n          if (idx2 &gt; 1) {\n            temp[i] &lt;- BOOT[[i]][[j]][l, m]\n          } else {\n            temp[i] &lt;- matrix(BOOT[[i]][[j]])[l, m]\n          }\n        }\n        mat.l[l, m] &lt;- quantile(temp, lower, na.rm = TRUE)\n        mat.u[l, m] &lt;- quantile(temp, upper, na.rm = TRUE)\n        mat.sd[l, m] &lt;- sd(temp, na.rm = TRUE)\n      }\n    }\n    colnames(mat.l) &lt;- response\n    colnames(mat.u) &lt;- response\n    colnames(mat.sd) &lt;- response\n    Lower[[j]] &lt;- mat.l\n    Upper[[j]] &lt;- mat.u\n    std_err[[j]] &lt;- mat.sd\n  }\n  names(Lower) &lt;- impulse\n  names(Upper) &lt;- impulse\n  names(std_err) &lt;- impulse\n  result &lt;- list(Lower = Lower, Upper = Upper, sd = std_err)\n\n  return(result)\n}\n\n\nCompute the IRFs (pointwise estimations):\n\nirfs &lt;- irf(\n  svar_est,\n  n.ahead = last_lag,\n  boot = FALSE,\n  cumulative = FALSE,\n  ortho = TRUE,\n  seed = 1\n)\n\n\n8.4.1 Monte Carlo Simulations\nWe define a function, irfs_mc(), which draws multiple shocks from a Normal distribution for a given shocked variable and compute the response to the system for each of the shocks.\nThe simulated responses are summarized into 68% and 95% credible intervals: \\[\n  \\text{IRF}_{h}^{(s)} = \\varepsilon_s \\cdot \\text{IRF}_h,\n  \\quad \\varepsilon_s \\sim \\mathcal{N}(0,1)\n\\]\n\n#' Returns a tibble with the IRFs values for a shocked variable\n#' and the credible intervals as computed with Monte-Carlo simulations\n#'\n#' @param irfs IRFs obtained with `irf_varest()`\n#' @param var_shocked Name of the variable shocked in `irfs$irf`\n#' @param runs No. draws for the Monte-Carlo simulations (default: 10,000)\n#'\nirfs_mc &lt;- function(var_shocked,\n                    irfs,\n                    runs = 10000) {\n  sim &lt;- vector(\"list\", runs)\n  n_variables &lt;- ncol(irfs$irf[[var_shocked]])\n\n  for (i in 1:length(sim)) {\n    x &lt;- matrix(0, ncol = n_variables, nrow = nrow(irfs$irf[[var_shocked]]))\n    x[1,1] &lt;- sqrt(abs(rnorm(n = 1, mean = 0, sd = 1))) # Generate a random shock\n    sim[[i]] &lt;- x[1,1] * irfs$irf[[var_shocked]] |&gt; as_tibble()\n    sim[[i]]$horizon &lt;- 1:nrow(irfs$irf[[var_shocked]])\n  }\n\n  sim &lt;-\n    sim |&gt; bind_rows() |&gt;\n    gather(var_response, value, -horizon) |&gt;\n    group_by(horizon, var_response) |&gt;\n    summarise(\n      mean = mean(value),\n      sd = sd(value),\n      lower_95 = quantile(value, probs = .05),\n      upper_95 = quantile(value, probs = .95),\n      lower_68 = quantile(value, probs = .32),\n      upper_68 = quantile(value, probs = .68),\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(var_shocked = !!var_shocked) |&gt;\n    rename(value = mean)\n\n  sim\n}\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the function irfs_mc(), since irfs$irf[[var_shocked]] already gives the response to a one–standard deviation shock, multiplying by a random \\(\\mathcal{N}(0,1)\\) means that we are simulating IRFs for random shock magnitudes drawn in standard deviation units. So the simulated values are expressed in the same units as the variables themselves (e.g., percentage deviations, log-deviations, etc.), per one-standard deviation of the shocked variable.\nIn our model, since the macro variables are detrended and expressed as log-deviations from trend, the IRFs of the weather shock measure approximatively the percentage responses to a one standard-deviation drought shock.\n\n\nWe apply this function to perform 10,000 draws so as to get the 0.95% and 68% credible intervals for the IRFs to a shock to the weather variable.\n\n# This takes about 30sec to run\ndf_irfs &lt;-\n  # pbapply::pblapply(names(irfs$irf), irfs_mc, irfs = irfs) |&gt;\n  lapply(names(irfs$irf), irfs_mc, irfs = irfs) |&gt;\n  bind_rows()\n\nLet us have a look at the beginning of these IRFs. We begin with looking at the response of the rest-of-the-world GDP, which is supposed to be 0 because of all our restrictions.\n\ndf_irfs |&gt; filter(var_shocked == \"smdi_obs\", var_response == \"wy_obs\") |&gt; head()\n\n# A tibble: 6 × 9\n  horizon var_response value    sd lower_95 upper_95 lower_68 upper_68\n    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       1 wy_obs           0     0        0        0        0        0\n2       2 wy_obs           0     0        0        0        0        0\n3       3 wy_obs           0     0        0        0        0        0\n4       4 wy_obs           0     0        0        0        0        0\n5       5 wy_obs           0     0        0        0        0        0\n6       6 wy_obs           0     0        0        0        0        0\n# ℹ 1 more variable: var_shocked &lt;chr&gt;\n\n\nThe response of national GDP:\n\ndf_irfs |&gt; filter(var_shocked == \"smdi_obs\", var_response == \"y_obs\") |&gt; head()\n\n# A tibble: 6 × 9\n  horizon var_response   value     sd lower_95 upper_95 lower_68 upper_68\n    &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       1 y_obs        -0.0520 0.0220  -0.0882  -0.0161  -0.0629  -0.0406\n2       2 y_obs        -0.176  0.0745  -0.299   -0.0548  -0.213   -0.138 \n3       3 y_obs        -0.195  0.0823  -0.331   -0.0605  -0.236   -0.152 \n4       4 y_obs        -0.159  0.0673  -0.270   -0.0495  -0.193   -0.124 \n5       5 y_obs        -0.117  0.0493  -0.198   -0.0362  -0.141   -0.0911\n6       6 y_obs        -0.0818 0.0346  -0.139   -0.0254  -0.0990  -0.0640\n# ℹ 1 more variable: var_shocked &lt;chr&gt;\n\n\nThe response of hours worked:\n\ndf_irfs |&gt; filter(var_shocked == \"smdi_obs\", var_response == \"h_obs\") |&gt; head()\n\n# A tibble: 6 × 9\n  horizon var_response   value     sd lower_95 upper_95 lower_68 upper_68\n    &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       1 h_obs        -0.127  0.0538   -0.216  -0.0395   -0.154  -0.0994\n2       2 h_obs        -0.132  0.0556   -0.223  -0.0409   -0.159  -0.103 \n3       3 h_obs        -0.128  0.0540   -0.217  -0.0397   -0.155  -0.0999\n4       4 h_obs        -0.116  0.0490   -0.197  -0.0361   -0.140  -0.0907\n5       5 h_obs        -0.104  0.0439   -0.176  -0.0323   -0.126  -0.0811\n6       6 h_obs        -0.0931 0.0393   -0.158  -0.0289   -0.113  -0.0727\n# ℹ 1 more variable: var_shocked &lt;chr&gt;\n\n\n\n\nThe table corresp_names with variable names and different labels (long, short, formulas), and the table possibilites with all combinations of two variables.\ncorresp_names &lt;- tribble(\n  ~name_r, ~long_name, ~short_name, ~pm_name,\n  \"wy_obs\", \"Foreign Output\", \"$\\\\Delta log\\\\left(Y_t^*\\\\right)$\", \"hat(y)[t]^F\",\n  \"wp_obs\", \"Foreign CPI Inflation\", \"$\\\\pi_t^*$\", \"hat(pi)[t]^F\",\n  \"wr_obs\", \"Foreign Interest Rate\", \"r_t^*\", \"hat(r)[t]^F\",\n  \"wp_a_obs\", \"Foreign Ag. Price Infl.\", \"\\\\Delta log \\\\left(p_t^{A*}\\\\right)$\", \"p[t]^D\",\n  \"wy_a_obs\", \"Foreign Ag. Output\", \"$\\\\Delta log \\\\left(Y_t^{A*}\\\\right)$\", \"Y[t]^D\",\n  \"oil_obs\", \"Crude Oil Inflation\", \"$oil_t$\", \"hat(oil)[t]\",\n  \"y_obs\", \"Output\", \"$\\\\Delta log \\\\left(Y_t^d\\\\right)$\", \"hat(y)[t]\",\n  \"y_a_obs\", \"Ag. Output\", \"$\\\\Delta log \\\\left(X_t^A\\\\right)$\", \"hat(y)[t]^A\",\n  \"p_obs\", \"CPI Inflation\", \"$\\\\pi_t^C$\", \"hat(pi)[t]\",\n  \"ratio_p_obs\", \"Rel. Prices\", \"$\\\\pi_{x,t}^A / \\\\pi_t^C$\", \"hat(pi)[t] / hat(pi)[t]^A\",\n  \"p_a_obs\", \"Ag. Inflation\", \"$log \\\\left(\\\\pi_{x,t}^A\\\\right)$\", \"hat(pi)[t]^A\",\n  \"c_obs\", \"Consumption\", \"$log \\\\left(c_{t}\\\\right)$\", \"hat(c)[t]\",\n  \"h_obs\", \"Hours Worked\", \"\\\\Delta log \\\\left($h_t\\\\right)$\", \"hat(h)[t]\",\n  \"i_obs\", \"Investment\", \"$\\\\Delta log \\\\left(i_t\\\\right)$\", \"hat(i)[t]\",\n  \"q_obs\", \"Stock Prices\", \"q_t\", \"hat(q)[t]\",\n  \"im_obs\", \"Imports\", \"\\\\Delta log \\\\left(im_{t}\\\\right)$\", \"hat(im)[t]\",\n  \"x_obs\", \"Exports\", \"$\\\\Delta log \\\\left(x_{t}\\\\right)$\", \"hat(x)[t]\",\n  \"tb_obs\", \"Trade Balance\", \"$tb_{t}$\", \"hat(tb)[t]\",\n  \"ex_rate_obs\", \"Real Ex. Rate\", \"$rer_t$\", \"hat(e)[t]\",\n  \"reer_obs\", \"Real Eff. Ex. Rate\", \"$reer_$\", \"hat(e)[t]\",\n  \"r_obs\", \"Interest Rate\", \"$r_t$\", \"hat(r)[t]\",\n  \"smdi_obs\", \"Weather\", \"$\\\\varepsilon_{t}^{W}$\", \"hat(s)[t]\",\n  \"r_y_hp\", \"GDP Deviation From HP Filter Trend\", \"$y_t$\", \"y[t]\",\n  \"r_y_a_hp\", \"agricultural GDP Deviation From HP Filter Trend\", \"$y_t^A$\", \"y[t]^A\",\n  \"smdi\", \"Wheater\", \"$\\\\varepsilon_{t}^{W}$\", \"hat(s)[t]\",\n  \"y_w\", \"Foreign Output\", \"$\\\\Delta log\\\\left(Y_t^*\\\\right)$\", \"hat(y)[t]^F\",\n  \"y\", \"Output\", \"$\\\\Delta log \\\\left(Y_t^d\\\\right)$\", \"hat(y)[t]\"\n)\n# We put the variables in a specific order when creating `corresp_names`.\n# Let us make sure, by casting column `name_r` as a factor, that the order is\n# preserved.\nnoms_ordonnees &lt;- corresp_names$name_r\ncorresp_names &lt;-\n  corresp_names |&gt;\n  mutate(name_r = factor(name_r, levels = noms_ordonnees))\n\n# Listing all possible subsets of 2 variables\npossibilites &lt;- expand.grid(\n  Var1 = levels(corresp_names$name_r),\n  Var2 = levels(corresp_names$name_r)\n) |&gt;\n  arrange(Var1, Var2) |&gt;\n  mutate(title = str_c(Var1, \"-\", Var2)) |&gt;\n  as_tibble()\n\n\nWe define a small function, c_name(), to retrieve the label of a variable in the tibble corresp_names.\n\n\nThe c_name() function.\n#' Returns the label of a variable by looking it up in `corresp_names`\n#' @param x Variable name (column `name_r` in `corresp_names`)\n#' @param type Type of desired label (`\"pm\"`, `\"short\"`, `\"long\"`). See details.\n#'\n#' @returns A character vector with the label of the variable.\n#' @details\n#' The desired type corresponds to the prefix of the column names of\n#' `corresp_names`: `\"pm\"` for R formulas, `\"short\"` for LaTeX formulas, and\n#' `\"long\"` for a string describing the variable.\n#'\n#' @examples\n#' c_name(\"wy_obs\", \"pm\")\n#'\nc_name &lt;- function(x, type = c(\"pm\", \"short\", \"long\")){\n  type &lt;- match.arg(type)\n  type &lt;- type %&gt;% str_c(\"_name\")\n  ind &lt;- match(x, corresp_names$name_r)\n  corresp_names[ind, ] |&gt; pull(type)\n}\n\n\nWe define a function, get_df_plot() to reshape the IRFs results for later use in a ggplot graph.\n\n\nThe get_df_plot()\n#' Reshape IRF table\nget_df_plot &lt;- function(df_irfs) {\n\n  df_plot &lt;-\n    df_irfs |&gt;\n    mutate(\n      title = str_c(var_shocked, \"-\", var_response),\n      title2 = str_c(\n        c_name(var_shocked, type = \"pm\"), \" %-&gt;% \",\n        c_name(var_response, type = \"pm\")\n      ),\n      title = factor(title, levels = possibilites$title)\n    ) |&gt;\n    arrange(title) |&gt;\n    ungroup() |&gt;\n    rename(lower = lower_95, upper = upper_95) |&gt;\n    mutate(\n      lower = as.numeric(lower),\n      upper = as.numeric(upper),\n      var_shocked = factor(var_shocked, levels = corresp_names$name_r),\n      var_response = factor(var_response, levels = corresp_names$name_r)\n    ) |&gt;\n    arrange(var_shocked, var_response, horizon)\n\n  ordered_titles &lt;- df_plot |&gt; pull(\"title2\") |&gt; unique()\n\n  df_plot &lt;-\n    df_plot |&gt;\n    mutate(title2 = factor(title2, levels = ordered_titles))\n}\n\n\n\ndf_irfs_plot &lt;- get_df_plot(df_irfs)\n\nThe plot with all the IRFs is shown in Figure 8.1.\n\n\nCodes to create the Figure.\n# IRFs with all the shocked variables\n# **Beware**: this may take a lot of time to be displayed\n# And requires a very large screen\np &lt;- ggplot(\n  data = df_irfs_plot |&gt;\n    unite(values_95, lower, upper, sep = \"@\") |&gt;\n    unite(values_68, lower_68, upper_68, sep = \"@\") |&gt;\n    gather(band, value_bounds, values_95, values_68) |&gt;\n    separate(value_bounds, into = c(\"lower\", \"upper\"), sep = \"@\", convert = T),\n  mapping = aes(x = horizon, y = value)\n) +\n  geom_ribbon(\n    mapping = aes(\n      ymin = lower, ymax = upper,\n      fill = band, group = band), alpha = .5\n  ) +\n  geom_line(colour = \"blue\") +\n  geom_line(aes(x = horizon, y = lower, group = band, linetype = band)) +\n  geom_line(aes(x = horizon, y = upper, group = band, linetype = band)) +\n  facet_wrap(~ title2, scales=\"free_y\", labeller = label_parsed) +\n  geom_hline(yintercept = 0, col = \"black\") +\n  labs(x = NULL, y = NULL) +\n  scale_fill_manual(\n    name=\"C.I. Level\",\n    values = c(\"values_95\" = \"#1E90FF\", \"values_68\" = \"#005A9C\"),\n    labels = c(\"values_95\" = \"95%\", \"values_68\" = \"68%\")\n  ) +\n  scale_linetype_manual(\n    name=\"C.I. Level\",\n    values = c(\"values_95\" = \"dashed\", \"values_68\" = \"dotted\"),\n    labels = c(\"values_95\" = \"95%\", \"values_68\" = \"68%\")\n  ) +\n  theme(\n    strip.background = element_rect(fill=NA),\n    legend.position = \"bottom\",\n    strip.text.x = element_text(size = 25)\n  )\np\n\n\n\n\n\nFigure 8.1: VAR impulse response to a standard shock. Shocked variables are in rows.\n\n\n\n\n\n\n\n\nIt is more useful to create a function that will only display the IRFs for one equation.\n\n\nThe print_irf() function.\n#' Displays the IRFs for one equation\n#'\n#' @param df_plot (data.frame) the data containing the IRFs (obtained from get_df_plot())\n#' @param shocked variable shocked\n#' df_plot &lt;- df_irfs_plot ; shocked &lt;- \"smdi_obs\" ; ci &lt;- 95\nprint_irf &lt;- function(df_plot, shocked) {\n\n  df_tmp &lt;-\n    df_plot |&gt;\n    filter(var_shocked == !!shocked) |&gt;\n    mutate(title2 = c_name(var_response, type = \"pm\")) |&gt;\n    arrange(var_shocked, var_response, horizon)\n\n  ordered_titles &lt;- df_tmp |&gt; pull(\"title2\") |&gt; unique()\n\n  df_tmp &lt;-\n    df_tmp |&gt;\n    mutate(title2 = factor(title2, levels = ordered_titles))\n\n  ggplot(\n    data = df_tmp |&gt;\n      unite(values_95, lower, upper, sep = \"@\") |&gt;\n      unite(values_68, lower_68, upper_68, sep = \"@\") |&gt;\n      gather(band, value_bounds, values_95, values_68) |&gt;\n      separate(value_bounds, into = c(\"lower\", \"upper\"), sep = \"@\", convert = T),\n    mapping = aes(x = horizon, y = value)\n  ) +\n    geom_ribbon(\n      mapping = aes(ymin = lower, ymax = upper, fill = band, group = band),\n      alpha=.5\n    ) +\n    geom_line(colour = \"blue\") +\n    geom_line(aes(x = horizon, y = lower, group = band, linetype = band)) +\n    geom_line(aes(x = horizon, y = upper, group = band, linetype = band)) +\n    facet_wrap(~title2, scales=\"free\", labeller = label_parsed) +\n    geom_hline(yintercept = 0, col = \"black\") +\n    labs(x = NULL, y = NULL) +\n    scale_fill_manual(\n      name = \"C.I. Level\",\n      values = c(\"values_95\" = \"#1E90FF\", \"values_68\" = \"#005A9C\"),\n      labels = c(\"values_95\" = \"95%\", \"values_68\" = \"68%\")\n    ) +\n    scale_linetype_manual(\n      name=\"C.I. Level\",\n      values = c(\"values_95\" = \"dashed\", \"values_68\" = \"dotted\"),\n      labels = c(\"values_95\" = \"95%\", \"values_68\" = \"68%\")\n    ) +\n    theme(\n      strip.background = element_rect(fill = NA),\n      legend.position = \"bottom\",\n      strip.text.x = element_text(size = 25)\n    )\n}\n\n\nThe response of the system to a 1-sd shock to the weather variable is shown in Figure 8.2.\n\n\nCodes to create the Figure.\nprint_irf(shocked = \"smdi_obs\", df_plot = df_irfs_plot) +\n  xlim(c(0, 20))\n\n\n\n\n\nFigure 8.2: VAR impulse response to a standard drought shock. For macro variables expressed in log-deviations from their trend, the y-axis represents, approximatively, the percentage response to a one-standard deviation weather shock.\n\n\n\n\n\n\n\n\n\n# Export the IRFs\nreadr::write_csv(df_irfs, file = \"../data/var_irfs_ebook.csv\")",
    "crumbs": [
      "VAR",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>VAR</span>"
    ]
  },
  {
    "objectID": "dsge.html",
    "href": "dsge.html",
    "title": "9  DSGE",
    "section": "",
    "text": "9.1 Une Section\nIn Section 9.1, …\nCeci est un essai pour écrire du code dynare.",
    "crumbs": [
      "DSGE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DSGE</span>"
    ]
  },
  {
    "objectID": "dsge.html#sec-code",
    "href": "dsge.html#sec-code",
    "title": "9  DSGE",
    "section": "",
    "text": "%----------------------------------------------------------------\n% 0. Housekeeping (close all graphic windows)\n%----------------------------------------------------------------\n\nclose all;\n%----------------------------------------------------------------\n% 1. Defining variables\n%----------------------------------------------------------------\n\nvar c uc uA uN hu i y h v m u vh rer_obs tb vc\n    y_N k_N i_N h_N w_N r q_N p_N d\n    y_A k_A i_A h_A w_A r q_A p_A land x y_N_obs\n    gdp NFA \n    c_R c_N de\n    c_star m_star uc_star pc_A pc_N rer r_star n\n    phi varrho ca_y dyA dy dc dH welf\n    y_obs i_obs h_obs smdi_obs y_a_obs wy_obs c_obs\n    e_z e_h e_g e_i e_n e_s e_e;\nvarexo eta_z eta_h eta_g eta_i eta_n eta_s eta_c eta_e;\n\n\nparameters  beta delta_K alpha sigmaC sigmaH chi gy b chi_I iota varphi mu \n            gamma Tss\n            omega tau kappa_A Lss delta_L psi theta1 theta2 theta3 theta4 alpha_N mu_N alpha_A mu_A chi_b rho_c sigmaC_star b_star\n            sig_z sig_h sig_g sig_i sig_n sig_s sig_c sig_e\n            % shocks\n            rho_z rho_h rho_g rho_i rho_n rho_s rho_e ;",
    "crumbs": [
      "DSGE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DSGE</span>"
    ]
  },
  {
    "objectID": "dsge.html#une-autre-section",
    "href": "dsge.html#une-autre-section",
    "title": "9  DSGE",
    "section": "9.2 Une Autre Section",
    "text": "9.2 Une Autre Section\nPour numéroter des équation, on doit nécessairement commencer son label par #eq- et il ne faut pas ajouter de _, il faut mettre des -, comme pour Equation 9.1. \\[\nX \\in \\mathcal{X}\n\\tag{9.1}\\]",
    "crumbs": [
      "DSGE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DSGE</span>"
    ]
  },
  {
    "objectID": "dsge.html#une-section-avec-un-tableau-et-une-figure",
    "href": "dsge.html#une-section-avec-un-tableau-et-une-figure",
    "title": "9  DSGE",
    "section": "9.3 Une Section avec un Tableau et une Figure",
    "text": "9.3 Une Section avec un Tableau et une Figure\nIl en va de même pour des tableaux (Table 9.1), le label doit commencer par #tbl-.\n\n\n\nTable 9.1: Un titre.\n\n\n\n\n\nCol 1\nCol 2\nCol 3\n\n\n\n\nval 1\n\\(x_1\\)\n\\(y_1\\)\n\n\nval 2\n\\(x_2\\)\n\\(y_2\\)\n\n\n\n\n\n\nPour les figures (Figure 9.1), le label doit commencer par fig-.\n\n\n\n\n\nFigure 9.1: Prior distributions.",
    "crumbs": [
      "DSGE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>DSGE</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Campbell, Gaylon S., and John M. Norman. 1998.\n“Introduction.” In An Introduction to Environmental\nBiophysics, 1–13. Springer New York. https://doi.org/10.1007/978-1-4612-1626-1_1.\n\n\nDingman, S. Lawrence. 2015. Physical Hydrology. Waveland press.\n\n\nGallic, Ewen, and Gauthier Vermandel. 2020. “Weather\nShocks.” European Economic Review 124 (May): 103409. https://doi.org/10.1016/j.euroecorev.2020.103409.\n\n\nHamon, W. R. 1964. “Computation of Direct Runoff Amounts from\nStorm Rainfall” 63: 52–62.\n\n\nLutz, James A., Jan W. van Wagtendonk, and Jerry F. Franklin. 2010.\n“Climatic Water Deficit, Tree Species Ranges, and Climate Change\nin Yosemite National Park.” Journal of Biogeography 37\n(5): 936–50. https://doi.org/10.1111/j.1365-2699.2009.02268.x.\n\n\nNarasimhan, B., and R. Srinivasan. 2005. “Development and\nEvaluation of Soil Moisture Deficit Index (SMDI) and Evapotranspiration\nDeficit Index (ETDI) for Agricultural Drought Monitoring.”\nAgricultural and Forest Meteorology 133 (1–4): 69–88. https://doi.org/10.1016/j.agrformet.2005.07.012.\n\n\nVicente-Serrano, Sergio M., Santiago Beguería, and Juan I. López-Moreno.\n2010. “A Multiscalar Drought Index Sensitive to Global Warming:\nThe Standardized Precipitation Evapotranspiration Index.”\nJournal of Climate 23 (7): 1696–1718. https://doi.org/10.1175/2009jcli2909.1.",
    "crumbs": [
      "References"
    ]
  }
]